benchmark_id,categories,created_at,data_type,description,implementation_link,language,max_score,modality,multilingual,name,paper_link,parent_benchmark_id,updated_at,verified
aa-index,['general'],2025-07-28T00:00:00.000000+00:00,benchmark_definition,"No official academic documentation found for this benchmark. Extensive research through ArXiv, IEEE/ACL/NeurIPS papers, and university research sites yielded no peer-reviewed sources for an 'aa-index' benchmark. This entry requires verification from official academic sources.",,en,1.0,text,False,AA-Index,,,2025-07-28T00:00:00.000000+00:00,False
acebench,"['general', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"ACEBench is a comprehensive benchmark for evaluating Large Language Models' tool usage capabilities across three primary evaluation types: Normal (basic tool usage scenarios), Special (tool usage with ambiguous or incomplete instructions), and Agent (multi-agent interactions simulating real-world dialogues). The benchmark covers 4,538 APIs across 8 major domains and 68 sub-domains including technology, finance, entertainment, society, health, culture, and environment, supporting both English and Chinese languages.",https://github.com/ACEBench/ACEBench,en,1.0,text,False,ACEBench,https://arxiv.org/abs/2501.12851,,2025-09-30T00:00:00.000000+00:00,False
activitynet,"['vision', 'video']",2025-07-19T19:56:15.378371+00:00,benchmark_definition,"A large-scale video benchmark for human activity understanding. Provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. The benchmark covers a wide range of complex human activities that are of interest to people in their daily living and can be used to compare algorithms for three scenarios: untrimmed video classification, trimmed activity classification, and activity detection.",https://github.com/activitynet/ActivityNet,en,1.0,video,False,ActivityNet,https://openaccess.thecvf.com/content_cvpr_2015/html/Heilbron_ActivityNet_A_Large-Scale_2015_CVPR_paper.html,,2025-07-19T19:56:15.378371+00:00,False
agieval,"['reasoning', 'general', 'math']",2025-07-19T19:56:13.970928+00:00,benchmark_definition,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",https://github.com/ruixiangcui/AGIEval,en,1.0,text,False,AGIEval,https://arxiv.org/abs/2304.06364,,2025-07-19T19:56:13.970928+00:00,False
ai2-reasoning-challenge-(arc),"['reasoning', 'general']",2025-07-19T19:56:15.419158+00:00,benchmark_definition,"A dataset of 7,787 genuine grade-school level, multiple-choice science questions assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and Easy Set, where the Challenge Set contains only questions answered incorrectly by both retrieval-based and word co-occurrence algorithms. Covers multiple scientific domains including biology, physics, earth science, and chemistry, requiring scientific reasoning, causal understanding, and conceptual knowledge beyond simple fact retrieval. Includes a supporting corpus of over 14 million science sentences.",https://github.com/allenai/ARC-Solvers,en,1.0,text,False,AI2 Reasoning Challenge (ARC),https://arxiv.org/abs/1803.05457,,2025-07-19T19:56:15.419158+00:00,False
ai2d,"['vision', 'reasoning', 'multimodal']",2025-07-19T19:56:13.618926+00:00,benchmark_definition,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",https://allenai.org/data/diagrams,en,1.0,multimodal,False,AI2D,https://arxiv.org/abs/1603.07396,,2025-07-19T19:56:13.618926+00:00,False
aider-polyglot-edit,"['general', 'code']",2025-07-19T19:56:13.789839+00:00,benchmark_definition,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",https://github.com/Aider-AI/polyglot-benchmark,en,1.0,text,False,Aider-Polyglot Edit,,,2025-09-30T00:00:00.000000+00:00,False
aider-polyglot,"['general', 'code']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",https://github.com/Aider-AI/polyglot-benchmark,en,1.0,text,False,Aider-Polyglot,,,2025-09-30T00:00:00.000000+00:00,False
aider,"['reasoning', 'code']",2025-07-19T19:56:14.566857+00:00,benchmark_definition,"Aider is a comprehensive code editing benchmark based on 133 practice exercises from Exercism's Python repository, designed to evaluate AI models' ability to translate natural language coding requests into executable code that passes unit tests. The benchmark measures end-to-end code editing capabilities, including GPT's ability to edit existing code and format code changes for automated saving to local files. The Aider Polyglot variant extends this evaluation across 225 challenging exercises spanning C++, Go, Java, JavaScript, Python, and Rust, making it a standard benchmark for assessing multilingual code editing performance in AI research.",https://github.com/Aider-AI/aider,en,1.0,text,False,Aider,,,2025-07-19T19:56:14.566857+00:00,False
aime-2024,"['math', 'reasoning']",2025-07-19T19:56:11.941652+00:00,benchmark_definition,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",,en,1.0,text,False,AIME 2024,https://arxiv.org/html/2503.21380v2,,2025-09-30T00:00:00.000000+00:00,False
aime-2025,"['math', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",,en,1.0,text,False,AIME 2025,https://arxiv.org/abs/2503.21380,,2025-09-05T00:00:00.000000+00:00,False
aime,"['math', 'reasoning']",2025-07-19T19:56:14.057279+00:00,benchmark_definition,American Invitational Mathematics Examination (AIME) benchmark for evaluating mathematical reasoning capabilities of large language models. Contains 30 challenging mathematical problems from AIME 2024 competition that require multi-step reasoning and advanced mathematical insight. Each problem has an integer answer between 000-999.,,en,1.0,text,False,AIME,https://arxiv.org/html/2503.21380v2,,2025-07-19T19:56:14.057279+00:00,False
aitz-em,"['multimodal', 'reasoning']",2025-07-19T19:56:14.785085+00:00,benchmark_definition,"Android-In-The-Zoo (AitZ) benchmark for evaluating autonomous GUI agents on smartphones. Contains 18,643 screen-action pairs with chain-of-action-thought annotations spanning over 70 Android apps. Designed to connect perception (screen layouts and UI elements) with cognition (action decision-making) for natural language-triggered smartphone task completion.",,en,1.0,multimodal,False,AITZ_EM,https://arxiv.org/abs/2403.02713,,2025-07-19T19:56:14.785085+00:00,False
alignbench,"['general', 'language', 'math', 'reasoning', 'roleplay']",2025-07-19T19:56:14.542033+00:00,benchmark_definition,"AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",,en,1.0,text,True,AlignBench,https://arxiv.org/abs/2311.18743,,2025-07-19T19:56:14.542033+00:00,False
alpacaeval-2.0,"['general', 'creativity', 'reasoning']",2025-07-19T19:56:15.038178+00:00,benchmark_definition,"AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",,en,1.0,text,False,AlpacaEval 2.0,https://arxiv.org/abs/2404.04475,,2025-07-19T19:56:15.038178+00:00,False
amc-2022-23,"['math', 'reasoning']",2025-07-19T19:56:13.992903+00:00,benchmark_definition,"American Mathematics Competition problems from the 2022-23 academic year, consisting of multiple-choice mathematics competition problems designed for high school students. These problems require advanced mathematical reasoning, problem-solving strategies, and mathematical knowledge covering topics like algebra, geometry, number theory, and combinatorics. The benchmark is derived from the official AMC competitions sponsored by the Mathematical Association of America.",,en,1.0,text,False,AMC_2022_23,https://arxiv.org/abs/2103.03874,,2025-07-19T19:56:13.992903+00:00,False
android-control-high-em,"['multimodal', 'reasoning']",2025-07-19T19:56:14.792498+00:00,benchmark_definition,Android device control benchmark using high exact match evaluation metric for assessing agent performance on mobile interface tasks,,en,1.0,multimodal,False,Android Control High_EM,,,2025-07-19T19:56:14.792498+00:00,False
android-control-low-em,"['multimodal', 'reasoning']",2025-07-19T19:56:14.800337+00:00,benchmark_definition,Android control benchmark evaluating autonomous agents on mobile device interaction tasks with low exact match scoring criteria,,en,1.0,multimodal,False,Android Control Low_EM,,,2025-07-19T19:56:14.800337+00:00,False
androidworld-sr,"['general', 'multimodal', 'reasoning']",2025-07-19T19:56:14.808659+00:00,benchmark_definition,"AndroidWorld Success Rate (SR) benchmark - A dynamic benchmarking environment for autonomous agents operating on Android devices. Evaluates agents on 116 programmatic tasks across 20 real-world Android apps using multimodal inputs (screen screenshots, accessibility trees, and natural language instructions). Measures success rate of agents completing tasks like sending messages, creating calendar events, and navigating mobile interfaces. Published at ICLR 2025. Best current performance: 30.6% success rate (M3A agent) vs 80.0% human performance.",,en,1.0,multimodal,False,AndroidWorld_SR,https://arxiv.org/abs/2405.14573,,2025-07-19T19:56:14.808659+00:00,False
api-bank,['reasoning'],2025-07-19T19:56:14.374447+00:00,benchmark_definition,"A comprehensive benchmark for tool-augmented LLMs that evaluates API planning, retrieval, and calling capabilities. Contains 314 tool-use dialogues with 753 API calls across 73 API tools, designed to assess how effectively LLMs can utilize external tools and overcome obstacles in tool leveraging.",,en,1.0,text,False,API-Bank,https://arxiv.org/abs/2304.08244,,2025-07-19T19:56:14.374447+00:00,False
arc-agi-v2,"['reasoning', 'vision', 'spatial_reasoning']",2025-07-19T19:56:13.916360+00:00,benchmark_definition,"ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",,en,1.0,multimodal,False,ARC-AGI v2,https://arxiv.org/abs/2505.11831,,2025-07-19T19:56:13.916360+00:00,False
arc-agi,"['reasoning', 'vision', 'spatial_reasoning']",2025-07-19T19:56:15.187761+00:00,benchmark_definition,"The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a benchmark designed to test general intelligence and abstract reasoning capabilities through visual grid-based transformation tasks. Each task consists of 2-5 demonstration pairs showing input grids transformed into output grids according to underlying rules, with test-takers required to infer these rules and apply them to novel test inputs. The benchmark uses colored grids (up to 30x30) with 10 discrete colors/symbols, designed to measure human-like general fluid intelligence and skill-acquisition efficiency with minimal prior knowledge.",,en,1.0,image,False,ARC-AGI,https://arxiv.org/abs/1911.01547,,2025-07-19T19:56:15.187761+00:00,False
arc-c,"['reasoning', 'general']",2025-07-19T19:56:11.052939+00:00,benchmark_definition,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",,en,1.0,text,False,ARC-C,https://arxiv.org/abs/1803.05457,,2025-07-19T19:56:11.052939+00:00,False
arc-e,"['reasoning', 'general']",2025-07-19T19:56:13.192662+00:00,benchmark_definition,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",,en,1.0,text,False,ARC-E,https://arxiv.org/abs/1803.05457,,2025-07-19T19:56:13.192662+00:00,False
arc,"['reasoning', 'general']",2025-07-19T19:56:13.967150+00:00,benchmark_definition,"The Abstraction and Reasoning Corpus (ARC) is a benchmark designed to measure human-like general fluid intelligence through grid-based reasoning tasks. It consists of 800 tasks (400 training, 400 evaluation) where each task presents input-output grids that require understanding abstract patterns and transformations. Test-takers must produce exactly correct output grids for all test inputs in a task to solve it, with 3 trials allowed per test input. ARC aims to enable fair comparisons of general intelligence between AI systems and humans using priors designed to be as close as possible to innate human priors.",,en,1.0,multimodal,False,Arc,https://arxiv.org/abs/1911.01547,,2025-07-19T19:56:13.967150+00:00,False
arena-hard-v2,"['general', 'reasoning', 'creativity']",2025-08-03T22:06:11.411643+00:00,benchmark_definition,"Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",,en,1.0,text,False,Arena-Hard v2,https://arxiv.org/abs/2406.11939,,2025-08-03T22:06:11.411643+00:00,False
arena-hard,"['general', 'reasoning', 'creativity']",2025-07-19T19:56:14.079874+00:00,benchmark_definition,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",,en,1.0,text,False,Arena Hard,https://arxiv.org/abs/2406.11939,,2025-07-19T19:56:14.079874+00:00,False
attaq,['safety'],2025-07-19T19:56:15.079764+00:00,benchmark_definition,"AttaQ is a unique dataset containing adversarial examples in the form of questions designed to provoke harmful or inappropriate responses from large language models. The benchmark evaluates safety vulnerabilities by using specialized clustering techniques that analyze both the semantic similarity of input attacks and the harmfulness of model responses, facilitating targeted improvements to model safety mechanisms.",,en,1.0,text,False,AttaQ,https://arxiv.org/abs/2311.04124,,2025-07-19T19:56:15.079764+00:00,False
autologi,['reasoning'],2025-09-05T00:00:00.000000+00:00,benchmark_definition,"AutoLogi is an automated method for synthesizing open-ended logic puzzles to evaluate reasoning abilities of Large Language Models. The benchmark addresses limitations of existing multiple-choice reasoning evaluations by featuring program-based verification and controllable difficulty levels. It includes 1,575 English and 883 Chinese puzzles, enabling more reliable evaluation that better distinguishes models' reasoning capabilities across languages.",,en,1.0,text,True,AutoLogi,https://arxiv.org/abs/2502.16906,,2025-09-05T00:00:00.000000+00:00,False
bbh,"['reasoning', 'math', 'language']",2025-07-19T19:56:13.031859+00:00,benchmark_definition,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",,en,1.0,text,False,BBH,https://arxiv.org/abs/2210.09261,,2025-07-19T19:56:13.031859+00:00,False
bfcl-v2,"['general', 'reasoning']",2025-07-19T19:56:14.444045+00:00,benchmark_definition,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",,en,1.0,text,True,BFCL v2,https://arxiv.org/abs/2305.15334,,2025-07-19T19:56:14.444045+00:00,False
bfcl-v3-multiturn,"['general', 'reasoning']",2025-07-19T19:56:14.962161+00:00,benchmark_definition,"Berkeley Function Calling Leaderboard (BFCL) V3 MultiTurn benchmark that evaluates large language models' ability to handle multi-turn and multi-step function calling scenarios. The benchmark introduces complex interactions requiring models to manage sequential function calls, handle conversational context across multiple turns, and make dynamic decisions about when and how to use available functions. BFCL V3 uses state-based evaluation by verifying the actual state of API systems after function execution, providing more realistic assessment of function calling capabilities in agentic applications.",,en,1.0,text,False,BFCL_v3_MultiTurn,https://openreview.net/forum?id=2GmDdhBdDk,,2025-07-19T19:56:14.962161+00:00,False
bfcl-v3,"['general', 'reasoning']",2025-08-03T22:06:11.216985+00:00,benchmark_definition,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",,en,1.0,text,False,BFCL-v3,https://openreview.net/forum?id=2GmDdhBdDk,,2025-08-03T22:06:11.216985+00:00,False
bfcl,"['general', 'reasoning']",2025-07-19T19:56:12.763704+00:00,benchmark_definition,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",,en,1.0,text,False,BFCL,https://openreview.net/pdf?id=2GmDdhBdDk,,2025-07-19T19:56:12.763704+00:00,False
big-bench-extra-hard,"['reasoning', 'general', 'language']",2025-07-19T19:56:13.279517+00:00,benchmark_definition,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",,en,1.0,text,False,BIG-Bench Extra Hard,https://arxiv.org/abs/2502.19187,,2025-07-19T19:56:13.279517+00:00,False
big-bench-hard,"['reasoning', 'math', 'language']",2025-07-19T19:56:13.222809+00:00,benchmark_definition,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",,en,1.0,text,False,BIG-Bench Hard,https://arxiv.org/abs/2210.09261,,2025-07-19T19:56:13.222809+00:00,False
big-bench,"['reasoning', 'math', 'language']",2025-07-19T19:56:13.926457+00:00,benchmark_definition,"Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark consisting of 204+ tasks designed to probe large language models and extrapolate their future capabilities. It covers diverse domains including linguistics, mathematics, common-sense reasoning, biology, physics, social bias, software development, and more. The benchmark focuses on tasks believed to be beyond current language model capabilities and includes both English and non-English tasks across multiple languages.",,en,1.0,text,True,BIG-Bench,https://arxiv.org/abs/2206.04615,,2025-07-19T19:56:13.926457+00:00,False
bigcodebench-full,"['general', 'reasoning']",2025-07-19T19:56:14.508830+00:00,benchmark_definition,"A comprehensive benchmark that evaluates large language models' ability to solve complex, practical programming tasks via code generation. Contains 1,140 fine-grained tasks across 7 domains using function calls from 139 libraries. Challenges LLMs to invoke multiple function calls as tools and handle complex instructions for realistic software engineering and general-purpose reasoning tasks.",,en,1.0,text,False,BigCodeBench-Full,https://arxiv.org/abs/2406.15877,,2025-07-19T19:56:14.508830+00:00,False
bigcodebench-hard,"['general', 'reasoning']",2025-07-19T19:56:14.512684+00:00,benchmark_definition,"BigCodeBench-Hard is a subset of 148 challenging programming tasks from BigCodeBench, designed to evaluate large language models' ability to solve complex, real-world programming problems. These tasks require diverse function calls from multiple libraries across 7 domains including computation, networking, data analysis, and visualization. The benchmark tests compositional reasoning and the ability to implement complex instructions that span 139 libraries with an average of 2.8 libraries per task.",,en,1.0,text,False,BigCodeBench-Hard,https://arxiv.org/abs/2406.15877,,2025-07-19T19:56:14.512684+00:00,False
bigcodebench,"['general', 'reasoning']",2025-07-19T19:56:14.048433+00:00,benchmark_definition,"A benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. Evaluates code generation with diverse function calls and complex instructions, featuring two variants: Complete (code completion based on comprehensive docstrings) and Instruct (generating code from natural language instructions).",,en,1.0,text,False,BigCodeBench,https://arxiv.org/abs/2406.15877,,2025-07-19T19:56:14.048433+00:00,False
bird-sql-(dev),['reasoning'],2025-07-19T19:56:13.410905+00:00,benchmark_definition,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",,en,1.0,text,False,Bird-SQL (dev),https://arxiv.org/abs/2305.03111,,2025-07-19T19:56:13.410905+00:00,False
blink,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.326398+00:00,benchmark_definition,"BLINK: Multimodal Large Language Models Can See but Not Perceive. A benchmark for multimodal language models focusing on core visual perception abilities. Reformats 14 classic computer vision tasks into 3,807 multiple-choice questions paired with single or multiple images and visual prompting. Tasks include relative depth estimation, visual correspondence, forensics detection, multi-view reasoning, counting, object localization, and spatial reasoning that humans can solve 'within a blink'.",,en,1.0,multimodal,False,BLINK,https://arxiv.org/abs/2404.12390,,2025-07-19T19:56:14.326398+00:00,False
boolq,"['language', 'reasoning']",2025-07-19T19:56:13.117325+00:00,benchmark_definition,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",,en,1.0,text,False,BoolQ,https://arxiv.org/abs/1905.10044,,2025-07-19T19:56:13.117325+00:00,False
browsecomp-long-128k,"['reasoning', 'search']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"A challenging benchmark for evaluating web browsing agents' ability to persistently navigate the internet and find hard-to-locate, entangled information. Comprises 1,266 questions requiring strategic reasoning, creative search, and interpretation of retrieved content, with short and easily verifiable answers.",,en,1.0,text,False,BrowseComp Long Context 128k,https://arxiv.org/abs/2504.12516,browsecomp,2025-07-24T12:00:00.000000+00:00,False
browsecomp-long-256k,"['reasoning', 'search']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"BrowseComp is a benchmark for measuring the ability of agents to browse the web, comprising 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. The benchmark focuses on questions where answers are obscure, time-invariant, and well-supported by evidence scattered across the open web.",,en,1.0,text,False,BrowseComp Long Context 256k,https://arxiv.org/abs/2504.12516,browsecomp,2025-07-24T12:00:00.000000+00:00,False
browsecomp-zh,"['reasoning', 'search']",2025-09-15T00:00:00.000000+00:00,benchmark_definition,"A high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web, consisting of 289 multi-hop questions spanning 11 diverse domains including Film & TV, Technology, Medicine, and History. Questions are reverse-engineered from short, objective, and easily verifiable answers, requiring sophisticated reasoning and information reconciliation beyond basic retrieval. The benchmark addresses linguistic, infrastructural, and censorship-related complexities in Chinese web environments.",,zh,1.0,text,True,BrowseComp-zh,https://arxiv.org/abs/2504.19314,browsecomp,2025-09-15T00:00:00.000000+00:00,False
browsecomp,"['reasoning', 'search']",2025-07-28T00:00:00.000000+00:00,benchmark_definition,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",,en,1.0,text,False,BrowseComp,https://arxiv.org/abs/2504.12516,,2025-07-28T00:00:00.000000+00:00,False
c-eval,"['general', 'reasoning']",2025-07-19T19:56:11.917478+00:00,benchmark_definition,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",,en,1.0,text,True,C-Eval,https://arxiv.org/abs/2305.08322,,2025-07-19T19:56:11.917478+00:00,False
cbnsl,"['math', 'reasoning']",2025-07-19T19:56:12.590999+00:00,benchmark_definition,Curriculum Learning of Bayesian Network Structures (CBNSL) benchmark for evaluating algorithms that learn Bayesian network structures from data using curriculum learning techniques. The benchmark uses networks from the bnlearn repository and evaluates structure learning performance using BDeu scoring metrics.,,en,1.0,text,False,CBNSL,http://proceedings.mlr.press/v45/Zhao15a.pdf,,2025-07-19T19:56:12.590999+00:00,False
cc-ocr,"['vision', 'multimodal', 'text-to-image']",2025-07-19T19:56:14.652986+00:00,benchmark_definition,"A comprehensive OCR benchmark for evaluating Large Multimodal Models (LMMs) in literacy. Comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. Contains 39 subsets with 7,058 fully annotated images, 41% sourced from real applications. Tests capabilities including text grounding, multi-orientation text recognition, and detecting hallucination/repetition across diverse visual challenges.",https://github.com/AlibabaResearch/AdvancedLiterateMachinery,en,1.0,multimodal,True,CC-OCR,https://arxiv.org/abs/2412.02210,,2025-07-19T19:56:14.652986+00:00,False
cfeval,['code'],2025-09-15T00:00:00.000000+00:00,benchmark_definition,CFEval benchmark for evaluating code generation and problem-solving capabilities,,en,10000.0,text,False,CFEval,,,2025-09-15T00:00:00.000000+00:00,False
charadessta,"['video', 'language', 'multimodal']",2025-07-19T19:56:14.760027+00:00,benchmark_definition,"Charades-STA is a benchmark dataset for temporal activity localization via language queries, extending the Charades dataset with sentence temporal annotations. It contains 12,408 training and 3,720 testing segment-sentence pairs from videos with natural language descriptions and precise temporal boundaries for localizing activities based on language queries.",,en,1.0,multimodal,False,CharadesSTA,https://arxiv.org/abs/1705.02101,,2025-07-19T19:56:14.760027+00:00,False
chartqa,"['reasoning', 'vision', 'multimodal']",2025-07-19T19:56:12.783541+00:00,benchmark_definition,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",,en,1.0,multimodal,False,ChartQA,https://arxiv.org/abs/2203.10244,,2025-07-19T19:56:12.783541+00:00,False
charxiv-d,"['reasoning', 'vision', 'multimodal']",2025-07-19T19:56:15.325204+00:00,benchmark_definition,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",,en,1.0,multimodal,False,CharXiv-D,https://arxiv.org/abs/2406.18521,,2025-07-19T19:56:15.325204+00:00,False
charxiv-r,"['reasoning', 'vision', 'multimodal']",2025-07-19T19:56:15.191553+00:00,benchmark_definition,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",,en,1.0,multimodal,False,CharXiv-R,https://arxiv.org/abs/2406.18521,,2025-07-19T19:56:15.191553+00:00,False
chexpert-cxr,"['healthcare', 'vision']",2025-07-19T19:56:14.021015+00:00,benchmark_definition,"CheXpert is a large dataset of 224,316 chest radiographs from 65,240 patients for automated chest X-ray interpretation. The dataset includes uncertainty labels for 14 medical observations extracted from radiology reports. It serves as a benchmark for developing and evaluating automated chest radiograph interpretation models.",,en,1.0,image,False,CheXpert CXR,https://arxiv.org/abs/1901.07031,,2025-07-19T19:56:14.021015+00:00,False
cluewsc,"['language', 'reasoning']",2025-07-19T19:56:12.233189+00:00,benchmark_definition,"CLUEWSC2020 is the Chinese version of the Winograd Schema Challenge, part of the CLUE benchmark. It focuses on pronoun disambiguation and coreference resolution, requiring models to determine which noun a pronoun refers to in a sentence. The dataset contains 1,244 training samples and 304 development samples extracted from contemporary Chinese literature.",,en,1.0,text,True,CLUEWSC,https://arxiv.org/abs/2004.05986,,2025-07-19T19:56:12.233189+00:00,False
cmmlu,"['language', 'reasoning', 'general']",2025-07-19T19:56:14.941108+00:00,benchmark_definition,"CMMLU (Chinese Massive Multitask Language Understanding) is a comprehensive Chinese benchmark that evaluates the knowledge and reasoning capabilities of large language models across 67 different subject topics. The benchmark covers natural sciences, social sciences, engineering, and humanities with multiple-choice questions ranging from basic to advanced professional levels.",,en,1.0,text,True,CMMLU,https://arxiv.org/abs/2306.09212,,2025-07-19T19:56:14.941108+00:00,False
cnmo-2024,['math'],2025-09-05T00:00:00.000000+00:00,benchmark_definition,China Mathematical Olympiad 2024 - A challenging mathematics competition.,,en,1.0,text,False,CNMO 2024,,,2025-09-05T00:00:00.000000+00:00,False
codeforces,"['math', 'reasoning']",2025-07-19T19:56:14.624663+00:00,benchmark_definition,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",,en,3000.0,text,False,CodeForces,https://arxiv.org/abs/2501.01257,,2025-07-19T19:56:14.624663+00:00,False
codegolf-v2.2,['code'],2025-07-19T19:56:13.778275+00:00,benchmark_definition,Codegolf v2.2 benchmark,,en,1.0,text,False,Codegolf v2.2,,,2025-07-19T19:56:13.778275+00:00,False
collie,"['language', 'reasoning', 'writing']",2025-07-19T19:56:15.250323+00:00,benchmark_definition,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",,en,1.0,text,False,COLLIE,https://arxiv.org/abs/2307.08689,,2025-07-19T19:56:15.250323+00:00,False
common-voice-15,"['audio', 'speech-to-text', 'language']",2025-07-19T19:56:14.830793+00:00,benchmark_definition,"Common Voice is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Version 15.0 contains 28,750 recorded hours across 114 languages, consisting of crowdsourced voice recordings with corresponding transcriptions.",,en,100.0,audio,True,Common Voice 15,https://arxiv.org/abs/1912.06670,,2025-07-19T19:56:14.830793+00:00,False
commonsenseqa,"['reasoning', 'language']",2025-07-19T19:56:15.129679+00:00,benchmark_definition,"CommonSenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict correct answers. It contains 12,102 questions with one correct answer and four distractors, designed to test semantic reasoning and conceptual relationships. Questions are created based on ConceptNet concepts and require prior world knowledge for accurate reasoning.",,en,1.0,text,False,CommonSenseQA,https://arxiv.org/abs/1811.00937,,2025-07-19T19:56:15.129679+00:00,False
complexfuncbench,"['long_context', 'reasoning']",2025-07-19T19:56:15.336577+00:00,benchmark_definition,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",,en,1.0,text,False,ComplexFuncBench,https://arxiv.org/abs/2501.10132,,2025-07-19T19:56:15.336577+00:00,False
covost2-en-zh,"['audio', 'speech-to-text', 'language']",2025-07-19T19:56:14.825578+00:00,benchmark_definition,CoVoST 2 English-to-Chinese subset is part of the large-scale multilingual speech translation corpus derived from Common Voice. This subset focuses specifically on English to Chinese speech translation tasks within the broader CoVoST 2 dataset.,,en,100.0,audio,True,CoVoST2 en-zh,https://arxiv.org/abs/2007.10310,,2025-07-19T19:56:14.825578+00:00,False
covost2,"['audio', 'speech-to-text', 'language']",2025-07-19T19:56:13.958237+00:00,benchmark_definition,"CoVoST 2 is a large-scale multilingual speech translation corpus derived from Common Voice, covering translations from 21 languages into English and from English into 15 languages. The dataset contains 2,880 hours of speech with 78K speakers for speech translation research.",,en,1.0,audio,True,CoVoST2,https://arxiv.org/abs/2007.10310,,2025-07-19T19:56:13.958237+00:00,False
crag,"['reasoning', 'search']",2025-07-19T19:56:12.741280+00:00,benchmark_definition,"CRAG (Comprehensive RAG Benchmark) is a factual question answering benchmark consisting of 4,409 question-answer pairs across 5 domains (finance, sports, music, movie, open domain) and 8 question categories. The benchmark includes mock APIs to simulate web and Knowledge Graph search, designed to represent the diverse and dynamic nature of real-world QA tasks with temporal dynamism ranging from years to seconds. It evaluates retrieval-augmented generation systems for trustworthy question answering.",,en,1.0,text,False,CRAG,https://arxiv.org/abs/2406.04744,,2025-07-19T19:56:12.741280+00:00,False
creative-writing-v3,"['creativity', 'writing']",2025-08-03T22:06:11.157942+00:00,benchmark_definition,"EQ-Bench Creative Writing v3 is an LLM-judged creative writing benchmark that evaluates models across 32 writing prompts with 3 iterations per prompt. Uses a hybrid scoring system combining rubric assessment and Elo ratings through pairwise comparisons. Challenges models in areas like humor, romance, spatial awareness, and unique perspectives to assess emotional intelligence and creative writing capabilities.",,en,1.0,text,False,Creative Writing v3,https://arxiv.org/abs/2312.06281,,2025-08-03T22:06:11.157942+00:00,False
crperelation,"['healthcare', 'reasoning']",2025-07-19T19:56:14.834739+00:00,benchmark_definition,Clinical reasoning problems evaluation benchmark for assessing diagnostic reasoning and medical knowledge application capabilities.,,en,1.0,text,False,CRPErelation,,,2025-07-19T19:56:14.834739+00:00,False
crux-o,['reasoning'],2025-07-19T19:56:14.635245+00:00,benchmark_definition,"CRUXEval-O (output prediction) is part of the CRUXEval benchmark consisting of 800 Python functions (3-13 lines) designed to evaluate AI models' capabilities in code reasoning, understanding, and execution. The benchmark tests models' ability to predict correct function outputs given function code and inputs, focusing on short problems that a good human programmer should be able to solve in a minute.",,en,100.0,text,False,CRUX-O,https://arxiv.org/abs/2401.03065,,2025-07-19T19:56:14.635245+00:00,False
cruxeval-input-cot,['reasoning'],2025-07-19T19:56:14.551746+00:00,benchmark_definition,"CRUXEval input prediction task with Chain of Thought (CoT) prompting. Part of the CRUXEval benchmark for code reasoning, understanding, and execution evaluation. Given a Python function and its expected output, the task is to predict the appropriate input using chain-of-thought reasoning. Consists of 800 Python functions (3-13 lines) designed to evaluate code comprehension and reasoning capabilities.",,en,1.0,text,False,CRUXEval-Input-CoT,https://arxiv.org/abs/2401.03065,,2025-07-19T19:56:14.551746+00:00,False
cruxeval-o,['reasoning'],2025-07-19T19:56:15.146592+00:00,benchmark_definition,"CruxEval-O is the output prediction task of the CRUXEval benchmark, designed to evaluate code reasoning, understanding, and execution capabilities. It consists of 800 Python functions (3-13 lines) where models must predict the output given a function and input. The benchmark tests fundamental code execution reasoning abilities and goes beyond simple code generation to assess deeper understanding of program behavior.",,en,1.0,text,False,CruxEval-O,https://arxiv.org/abs/2401.03065,,2025-07-19T19:56:15.146592+00:00,False
cruxeval-output-cot,['reasoning'],2025-07-19T19:56:14.555432+00:00,benchmark_definition,"CRUXEval-O (output prediction) with Chain-of-Thought prompting. Part of the CRUXEval benchmark consisting of 800 Python functions (3-13 lines) designed to evaluate code reasoning, understanding, and execution capabilities. The output prediction task requires models to predict the output of a given Python function with specific inputs, evaluated using chain-of-thought reasoning methodology.",,en,1.0,text,False,CRUXEval-Output-CoT,https://arxiv.org/abs/2401.03065,,2025-07-19T19:56:14.555432+00:00,False
csimpleqa,"['general', 'language']",2025-07-19T19:56:11.931358+00:00,benchmark_definition,"Chinese SimpleQA is the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions. It contains 3,000 high-quality questions spanning 6 major topics with 99 diverse subtopics, designed to assess Chinese factual knowledge across humanities, science, engineering, culture, and society.",,en,1.0,text,True,CSimpleQA,https://arxiv.org/abs/2411.07140,,2025-07-19T19:56:11.931358+00:00,False
cybersecurity-ctfs,['safety'],2025-07-19T19:56:15.387055+00:00,benchmark_definition,"Cybersecurity Capture the Flag (CTF) benchmark for evaluating LLMs in offensive security challenges. Contains diverse cybersecurity tasks including cryptography, web exploitation, binary analysis, and forensics to assess AI capabilities in cybersecurity problem-solving.",,en,1.0,text,False,Cybersecurity CTFs,https://arxiv.org/abs/2406.05590,,2025-07-19T19:56:15.387055+00:00,False
dermmcqa,['healthcare'],2025-07-19T19:56:14.024498+00:00,benchmark_definition,Dermatology multiple choice question assessment benchmark for evaluating medical knowledge and diagnostic reasoning in dermatological conditions and treatments.,,en,1.0,text,False,DermMCQA,https://arxiv.org/abs/2309.06961,,2025-07-19T19:56:14.024498+00:00,False
docvqa,"['vision', 'multimodal']",2025-07-19T19:56:12.825214+00:00,benchmark_definition,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",,en,1.0,multimodal,False,DocVQA,https://arxiv.org/abs/2007.00398,,2025-07-19T19:56:12.825214+00:00,False
docvqatest,"['vision', 'multimodal']",2025-07-19T19:56:14.579372+00:00,benchmark_definition,"DocVQA is a Visual Question Answering benchmark on document images containing 50,000 questions defined on 12,000+ document images. The benchmark focuses on understanding document structure and content to answer questions about various document types including letters, memos, notes, and reports from the UCSF Industry Documents Library.",,en,1.0,multimodal,False,DocVQAtest,https://arxiv.org/abs/2007.00398,,2025-07-19T19:56:14.579372+00:00,False
drop,"['reasoning', 'math']",2025-07-19T19:56:12.981569+00:00,benchmark_definition,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",,en,1.0,text,False,DROP,https://arxiv.org/abs/1903.00161,,2025-07-19T19:56:12.981569+00:00,False
ds-arena-code,['reasoning'],2025-07-19T19:56:15.057744+00:00,benchmark_definition,"Data Science Arena Code benchmark for evaluating LLMs on realistic data science code generation tasks. Tests capabilities in complex data processing, analysis, and programming across popular Python libraries used in data science workflows.",,en,1.0,text,False,DS-Arena-Code,https://arxiv.org/abs/2505.15621,,2025-07-19T19:56:15.057744+00:00,False
ds-fim-eval,['general'],2025-07-19T19:56:15.053854+00:00,benchmark_definition,DeepSeek's internal Fill-in-the-Middle evaluation dataset for measuring code completion performance improvements in data science contexts,,en,1.0,text,False,DS-FIM-Eval,https://arxiv.org/abs/2406.11931,,2025-07-19T19:56:15.053854+00:00,False
eclektic,"['language', 'reasoning']",2025-07-19T19:56:13.561292+00:00,benchmark_definition,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",,en,1.0,text,True,ECLeKTic,https://arxiv.org/abs/2502.21228,,2025-07-19T19:56:13.561292+00:00,False
egoschema,"['vision', 'reasoning', 'long_context']",2025-07-19T19:56:12.915240+00:00,benchmark_definition,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",,en,1.0,video,False,EgoSchema,https://arxiv.org/abs/2308.09126,,2025-07-19T19:56:12.915240+00:00,False
erqa,"['vision', 'reasoning', 'spatial_reasoning']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"Embodied Reasoning Question Answering benchmark consisting of 400 multiple-choice visual questions across spatial reasoning, trajectory reasoning, action reasoning, state estimation, and multi-view reasoning for evaluating AI capabilities in physical world interactions",https://github.com/embodiedreasoning/ERQA,en,1.0,multimodal,False,ERQA,https://arxiv.org/abs/2503.20020,,2025-07-24T12:00:00.000000+00:00,False
evalplus,"['reasoning', 'code']",2025-07-19T19:56:11.793176+00:00,benchmark_definition,"A rigorous code synthesis evaluation framework that augments existing datasets with extensive test cases generated by LLM and mutation-based strategies to better assess functional correctness of generated code, including HumanEval+ with 80x more test cases",,en,100.0,text,False,EvalPlus,https://arxiv.org/abs/2305.01210,,2025-07-19T19:56:11.793176+00:00,False
facts-grounding,['reasoning'],2025-07-19T19:56:13.260285+00:00,benchmark_definition,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",,en,1.0,text,False,FACTS Grounding,https://arxiv.org/abs/2501.03200,,2025-07-19T19:56:13.260285+00:00,False
factscore,['reasoning'],2025-07-24T12:00:00.000000+00:00,benchmark_definition,"A fine-grained atomic evaluation metric for factual precision in long-form text generation that breaks generated text into atomic facts and computes the percentage supported by reliable knowledge sources, with automated assessment using retrieval and language models",,en,1.0,text,False,FActScore,https://arxiv.org/abs/2305.14251,,2025-07-24T12:00:00.000000+00:00,False
finqa,"['finance', 'math', 'reasoning']",2025-07-19T19:56:12.734486+00:00,benchmark_definition,"A large-scale dataset for numerical reasoning over financial data with question-answering pairs written by financial experts, featuring complex numerical reasoning and understanding of heterogeneous representations with annotated gold reasoning programs for full explainability",,en,1.0,text,False,FinQA,https://arxiv.org/abs/2109.00122,,2025-07-19T19:56:12.734486+00:00,False
flenqa,"['reasoning', 'long_context']",2025-07-19T19:56:14.277205+00:00,benchmark_definition,"Flexible Length Question Answering dataset for evaluating the impact of input length on reasoning performance of language models, featuring True/False questions embedded in contexts of varying lengths (250-3000 tokens) across three reasoning tasks: Monotone Relations, People In Rooms, and simplified Ruletaker",,en,1.0,text,False,FlenQA,https://arxiv.org/abs/2402.14848,,2025-07-19T19:56:14.277205+00:00,False
fleurs,"['language', 'speech-to-text']",2025-07-19T19:56:13.943695+00:00,benchmark_definition,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",,en,100.0,audio,True,FLEURS,https://arxiv.org/abs/2205.12446,,2025-07-19T19:56:13.943695+00:00,False
frames,"['reasoning', 'search']",2025-07-19T19:56:14.954436+00:00,benchmark_definition,"Factuality, Retrieval, And reasoning MEasurement Set - a unified evaluation dataset of 824 challenging multi-hop questions for testing retrieval-augmented generation systems across factuality, retrieval accuracy, and reasoning capabilities, requiring integration of 2-15 Wikipedia articles per question",,en,1.0,text,False,FRAMES,https://arxiv.org/abs/2409.12941,,2025-07-19T19:56:14.954436+00:00,False
french-mmlu,"['general', 'language', 'reasoning']",2025-07-19T19:56:15.134340+00:00,benchmark_definition,"French version of MMLU-Pro, a multilingual benchmark for evaluating language models' cross-lingual reasoning capabilities across 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health.",,en,1.0,text,True,French MMLU,https://arxiv.org/abs/2503.10497,,2025-07-19T19:56:15.134340+00:00,False
frontiermath,"['math', 'reasoning']",2025-07-19T19:56:15.179213+00:00,benchmark_definition,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",,en,1.0,text,False,FrontierMath,https://arxiv.org/abs/2411.04872,,2025-07-19T19:56:15.179213+00:00,False
functionalmath,"['math', 'reasoning']",2025-07-19T19:56:13.987516+00:00,benchmark_definition,"A functional variant of the MATH benchmark that tests language models' ability to generalize reasoning patterns across different problem instances, revealing the reasoning gap between static and functional performance.",,en,1.0,text,False,FunctionalMATH,https://arxiv.org/abs/2402.19450,,2025-07-19T19:56:13.987516+00:00,False
giantsteps-tempo,['audio'],2025-07-19T19:56:14.838584+00:00,benchmark_definition,"A dataset for tempo estimation in electronic dance music containing 664 2-minute audio previews from Beatport, annotated from user corrections for evaluating automatic tempo estimation algorithms.",,en,1.0,audio,False,GiantSteps Tempo,https://archives.ismir.net/ismir2015/paper/000246.pdf,,2025-07-19T19:56:14.838584+00:00,False
global-mmlu-lite,"['general', 'language', 'reasoning']",2025-07-19T19:56:13.534515+00:00,benchmark_definition,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,,en,1.0,text,True,Global-MMLU-Lite,https://arxiv.org/abs/2412.03304,,2025-07-19T19:56:13.534515+00:00,False
global-mmlu,"['general', 'language', 'reasoning']",2025-07-19T19:56:13.747524+00:00,benchmark_definition,"A comprehensive multilingual benchmark covering 42 languages that addresses cultural and linguistic biases in evaluation, with improved translation quality and culturally sensitive question subsets.",,en,1.0,text,True,Global-MMLU,https://arxiv.org/abs/2412.03304,,2025-07-19T19:56:13.747524+00:00,False
gorilla-benchmark-api-bench,"['reasoning', 'code']",2025-07-19T19:56:14.383584+00:00,benchmark_definition,"APIBench, a comprehensive dataset of over 11,000 instruction-API pairs from HuggingFace, TorchHub, and TensorHub APIs for evaluating language models' ability to generate accurate API calls.",,en,1.0,text,False,Gorilla Benchmark API Bench,https://arxiv.org/abs/2305.15334,,2025-07-19T19:56:14.383584+00:00,False
govreport,"['summarization', 'long_context']",2025-07-19T19:56:14.218809+00:00,benchmark_definition,"A long document summarization dataset consisting of reports from government research agencies including Congressional Research Service and U.S. Government Accountability Office, with significantly longer documents and summaries than other datasets.",,en,1.0,text,False,GovReport,https://arxiv.org/abs/2104.02112,,2025-07-19T19:56:14.218809+00:00,False
gpqa-biology,"['reasoning', 'general']",2025-07-19T19:56:15.391187+00:00,benchmark_definition,"Biology subset of GPQA, containing challenging multiple-choice questions written by domain experts in biology. These Google-proof questions require graduate-level knowledge and reasoning.",,en,1.0,text,False,GPQA Biology,https://arxiv.org/abs/2311.12022,,2025-07-19T19:56:15.391187+00:00,False
gpqa-chemistry,"['reasoning', 'chemistry']",2025-07-19T19:56:15.395806+00:00,benchmark_definition,"Chemistry subset of GPQA, containing challenging multiple-choice questions written by domain experts in chemistry. These Google-proof questions require graduate-level knowledge and reasoning.",,en,1.0,text,False,GPQA Chemistry,https://arxiv.org/abs/2311.12022,,2025-07-19T19:56:15.395806+00:00,False
gpqa-physics,"['reasoning', 'physics']",2025-07-19T19:56:15.400663+00:00,benchmark_definition,"Physics subset of GPQA, containing challenging multiple-choice questions written by domain experts in physics. These Google-proof questions require graduate-level knowledge and reasoning.",,en,1.0,text,False,GPQA Physics,https://arxiv.org/abs/2311.12022,,2025-07-19T19:56:15.400663+00:00,False
gpqa,"['reasoning', 'general']",2025-07-19T19:56:11.588605+00:00,benchmark_definition,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",,en,1.0,text,False,GPQA,https://arxiv.org/abs/2311.12022,,2025-07-19T19:56:11.588605+00:00,False
graphwalks-bfs->128k,"['reasoning', 'spatial_reasoning', 'long_context']",2025-07-19T19:56:15.295876+00:00,benchmark_definition,"A graph reasoning benchmark that evaluates language models' ability to perform breadth-first search (BFS) operations on graphs with context length over 128k tokens, testing long-context reasoning capabilities.",,en,1.0,text,False,Graphwalks BFS >128k,,,2025-07-19T19:56:15.295876+00:00,False
graphwalks-parents->128k,"['reasoning', 'spatial_reasoning', 'long_context']",2025-07-19T19:56:15.316836+00:00,benchmark_definition,"A graph reasoning benchmark that evaluates language models' ability to find parent nodes in graphs with context length over 128k tokens, testing long-context reasoning and graph structure understanding.",,en,1.0,text,False,Graphwalks parents >128k,,,2025-07-19T19:56:15.316836+00:00,False
groundui-1k,"['multimodal', 'vision']",2025-07-19T19:56:12.758595+00:00,benchmark_definition,"A subset of GroundUI-18K for UI grounding evaluation, where models must predict action coordinates on screenshots based on single-step instructions across web, desktop, and mobile platforms.",,en,1.0,multimodal,False,GroundUI-1K,https://arxiv.org/abs/2403.17918,,2025-07-19T19:56:12.758595+00:00,False
gsm-8k-(cot),"['math', 'reasoning']",2025-07-19T19:56:14.360381+00:00,benchmark_definition,"Grade School Math 8K with Chain-of-Thought prompting, featuring 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",,en,1.0,text,False,GSM-8K (CoT),https://arxiv.org/abs/2110.14168,,2025-07-19T19:56:14.360381+00:00,False
gsm8k-chat,"['math', 'reasoning']",2025-07-19T19:56:15.101578+00:00,benchmark_definition,"Grade School Math 8K adapted for chat format evaluation, featuring 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",,en,1.0,text,False,GSM8K Chat,https://arxiv.org/abs/2110.14168,,2025-07-19T19:56:15.101578+00:00,False
gsm8k,"['math', 'reasoning']",2025-07-19T19:56:11.397385+00:00,benchmark_definition,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",,en,1.0,text,False,GSM8k,https://arxiv.org/abs/2110.14168,,2025-07-19T19:56:11.397385+00:00,False
hallusion-bench,"['vision', 'reasoning']",2025-07-19T19:56:14.689507+00:00,benchmark_definition,"A comprehensive benchmark designed to evaluate image-context reasoning in large visual-language models (LVLMs) by challenging models with 346 images and 1,129 carefully crafted questions to assess language hallucination and visual illusion",,en,1.0,multimodal,False,Hallusion Bench,https://arxiv.org/abs/2310.14566,,2025-07-19T19:56:14.689507+00:00,False
healthbench-hard,['healthcare'],2025-08-05T19:56:13.424873+00:00,benchmark_definition,"A challenging variation of HealthBench that evaluates large language models' performance and safety in healthcare through 5,000 multi-turn conversations with particularly rigorous evaluation criteria validated by 262 physicians from 60 countries",,en,1.0,text,False,HealthBench Hard,https://arxiv.org/abs/2505.08775,,2025-08-05T19:56:13.424873+00:00,False
healthbench,['healthcare'],2025-08-05T19:56:13.424873+00:00,benchmark_definition,"An open-source benchmark for measuring performance and safety of large language models in healthcare, consisting of 5,000 multi-turn conversations evaluated by 262 physicians using 48,562 unique rubric criteria across health contexts and behavioral dimensions",,en,1.0,text,False,HealthBench,https://arxiv.org/abs/2505.08775,,2025-08-05T19:56:13.424873+00:00,False
hellaswag,['reasoning'],2025-07-19T19:56:11.145630+00:00,benchmark_definition,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",,en,1.0,text,False,HellaSwag,https://arxiv.org/abs/1905.07830,,2025-07-19T19:56:11.145630+00:00,False
hiddenmath,"['math', 'reasoning']",2025-07-19T19:56:13.424873+00:00,benchmark_definition,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,,en,1.0,text,False,HiddenMath,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,2025-07-19T19:56:13.424873+00:00,False
hle,"['reasoning', 'math']",2025-07-28T00:00:00.000000+00:00,benchmark_definition,"Humanity's Last Exam (HLE) is a multi-modal academic benchmark with 2,500 questions across mathematics, humanities, and natural sciences, designed to test LLM capabilities at the frontier of human knowledge with unambiguous, verifiable solutions",,en,1.0,multimodal,False,HLE,https://arxiv.org/abs/2501.14249,,2025-07-28T00:00:00.000000+00:00,False
hmmt-2025,['math'],2025-09-05T00:00:00.000000+00:00,benchmark_definition,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",,en,1.0,text,False,HMMT 2025,http://web.mit.edu/HMMT/www/,,2025-09-05T00:00:00.000000+00:00,False
hmmt25,['math'],2025-07-19T19:56:15.061281+00:00,benchmark_definition,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",,en,1.0,text,False,HMMT25,http://web.mit.edu/HMMT/www/,,2025-07-19T19:56:15.061281+00:00,False
humaneval+,['reasoning'],2025-07-19T19:56:14.062352+00:00,benchmark_definition,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",,en,1.0,text,False,HumanEval+,https://arxiv.org/abs/2305.01210,,2025-07-19T19:56:14.062352+00:00,False
humaneval-average,['reasoning'],2025-07-19T19:56:15.171175+00:00,benchmark_definition,"A variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",,en,1.0,text,False,HumanEval-Average,https://arxiv.org/abs/2107.03374,,2025-07-19T19:56:15.171175+00:00,False
humaneval-er,['reasoning'],2025-07-19T19:56:12.704744+00:00,benchmark_definition,"A variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",,en,1.0,text,False,HumanEval-ER,https://arxiv.org/abs/2107.03374,,2025-07-19T19:56:12.704744+00:00,False
humaneval-mul,['reasoning'],2025-07-19T19:56:15.032472+00:00,benchmark_definition,"A multilingual variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",,en,1.0,text,True,HumanEval-Mul,https://arxiv.org/abs/2107.03374,,2025-07-19T19:56:15.032472+00:00,False
humaneval-plus,"['reasoning', 'code']",2025-08-03T22:06:10.921756+00:00,benchmark_definition,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",,en,1.0,text,False,HumanEval Plus,https://arxiv.org/abs/2305.01210,,2025-08-03T22:06:10.921756+00:00,False
humaneval,"['reasoning', 'code']",2025-07-19T19:56:12.595263+00:00,benchmark_definition,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",,en,1.0,text,False,HumanEval,https://arxiv.org/abs/2107.03374,,2025-07-19T19:56:12.595263+00:00,False
humanevalfim-average,['general'],2025-07-19T19:56:15.160562+00:00,benchmark_definition,"Average evaluation of HumanEval Fill-in-the-Middle benchmark variants (single-line, multi-line, random-span) for assessing code infilling capabilities of language models",,en,1.0,text,False,HumanEvalFIM-Average,https://arxiv.org/abs/2207.14255,,2025-07-19T19:56:15.160562+00:00,False
humanity's-last-exam,['general'],2025-07-19T19:56:12.507693+00:00,benchmark_definition,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",,en,1.0,multimodal,False,Humanity's Last Exam,https://arxiv.org/abs/2501.14249,,2025-07-19T19:56:12.507693+00:00,False
if,['general'],2025-08-03T22:06:11.089394+00:00,benchmark_definition,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",,en,1.0,text,False,IF,https://arxiv.org/abs/2311.07911,,2025-08-03T22:06:11.089394+00:00,False
ifeval,['general'],2025-07-19T19:56:12.241350+00:00,benchmark_definition,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",,en,1.0,text,False,IFEval,https://arxiv.org/abs/2311.07911,,2025-07-19T19:56:12.241350+00:00,False
include,['general'],2025-07-19T19:56:13.724387+00:00,benchmark_definition,Include benchmark - specific documentation not found in official sources,,en,1.0,text,False,Include,,,2025-07-19T19:56:13.724387+00:00,False
infinitebench-en.mc,['long_context'],2025-07-19T19:56:14.461508+00:00,benchmark_definition,InfiniteBench English Multiple Choice variant - first LLM benchmark featuring average data length surpassing 100K tokens for evaluating long-context capabilities with 12 tasks spanning diverse domains,,en,1.0,text,False,InfiniteBench/En.MC,https://arxiv.org/abs/2402.13718,,2025-07-19T19:56:14.461508+00:00,False
infinitebench-en.qa,['long_context'],2025-07-19T19:56:14.457927+00:00,benchmark_definition,InfiniteBench English Question Answering variant - first LLM benchmark featuring average data length surpassing 100K tokens for evaluating long-context capabilities with 12 tasks spanning diverse domains,,en,1.0,text,False,InfiniteBench/En.QA,https://arxiv.org/abs/2402.13718,,2025-07-19T19:56:14.457927+00:00,False
infographicsqa,"['vision', 'multimodal']",2025-07-19T19:56:14.417669+00:00,benchmark_definition,"InfographicVQA dataset with 5,485 infographic images and over 30,000 questions requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",,en,1.0,multimodal,False,InfographicsQA,https://arxiv.org/abs/2104.12756,,2025-07-19T19:56:14.417669+00:00,False
infovqa,"['vision', 'multimodal']",2025-07-19T19:56:13.601294+00:00,benchmark_definition,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",,en,1.0,multimodal,False,InfoVQA,https://arxiv.org/abs/2104.12756,,2025-07-19T19:56:13.601294+00:00,False
infovqatest,"['vision', 'multimodal']",2025-07-19T19:56:14.583939+00:00,benchmark_definition,"InfoVQA test set with infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",,en,1.0,multimodal,False,InfoVQAtest,https://arxiv.org/abs/2104.12756,,2025-07-19T19:56:14.583939+00:00,False
instruct-humaneval,['general'],2025-07-19T19:56:15.105488+00:00,benchmark_definition,Instruction-based variant of HumanEval benchmark for evaluating large language models' code generation capabilities with functional correctness using pass@k metric on programming problems,,en,1.0,text,False,Instruct HumanEval,https://arxiv.org/abs/2107.03374,,2025-07-19T19:56:15.105488+00:00,False
intergps,"['math', 'spatial_reasoning']",2025-07-19T19:56:14.259321+00:00,benchmark_definition,"Interpretable Geometry Problem Solver (Inter-GPS) with Geometry3K dataset of 3,002 geometry problems with dense annotation in formal language using theorem knowledge and symbolic reasoning",,en,1.0,text,False,InterGPS,https://arxiv.org/abs/2105.04165,,2025-07-19T19:56:14.259321+00:00,False
internal-api-instruction-following-(hard),['general'],2025-07-19T19:56:15.222560+00:00,benchmark_definition,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,,en,1.0,text,False,Internal API instruction following (hard),,,2025-07-19T19:56:15.222560+00:00,False
lbpp-(v2),['reasoning'],2025-07-19T19:56:14.053535+00:00,benchmark_definition,"LBPP (v2) benchmark - specific documentation not found in official sources, possibly related to language-based planning problems",,en,1.0,text,False,LBPP (v2),https://arxiv.org/abs/2206.10498,,2025-07-19T19:56:14.053535+00:00,False
livebench-20241125,"['math', 'reasoning', 'general']",2025-08-03T22:06:11.046321+00:00,benchmark_definition,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",,en,1.0,text,False,LiveBench 20241125,https://arxiv.org/abs/2406.19314,,2025-08-03T22:06:11.046321+00:00,False
livebench,"['math', 'reasoning', 'general']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",,en,1.0,text,False,LiveBench,https://arxiv.org/abs/2406.19314,,2025-09-05T00:00:00.000000+00:00,False
livecodebench(01-09),"['reasoning', 'general']",2025-07-19T19:56:15.049594+00:00,benchmark_definition,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",,en,1.0,text,False,LiveCodeBench(01-09),https://arxiv.org/abs/2403.07974,,2025-07-19T19:56:15.049594+00:00,False
livecodebench-v5-24.12-25.2,"['reasoning', 'general']",2025-07-19T19:56:12.066180+00:00,benchmark_definition,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",,en,1.0,text,False,LiveCodeBench v5 24.12-25.2,https://arxiv.org/abs/2403.07974,,2025-07-19T19:56:12.066180+00:00,False
livecodebench-v5,"['reasoning', 'general']",2025-07-19T19:56:13.759330+00:00,benchmark_definition,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",,en,1.0,text,False,LiveCodeBench v5,https://arxiv.org/abs/2403.07974,,2025-07-19T19:56:13.759330+00:00,False
livecodebench-v6,"['reasoning', 'general']",2025-07-19T19:56:11.785682+00:00,benchmark_definition,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",,en,1.0,text,False,LiveCodeBench v6,https://arxiv.org/abs/2403.07974,,2025-07-19T19:56:11.785682+00:00,False
livecodebench,"['reasoning', 'general', 'code']",2025-07-19T19:56:13.292229+00:00,benchmark_definition,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",,en,1.0,text,False,LiveCodeBench,https://arxiv.org/abs/2403.07974,,2025-07-19T19:56:13.292229+00:00,False
longbench-v2,"['long_context', 'reasoning', 'general']",2025-07-19T19:56:15.029281+00:00,benchmark_definition,"LongBench v2 is a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. It consists of 503 challenging multiple-choice questions with contexts ranging from 8k to 2M words across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.",,en,1.0,text,True,LongBench v2,https://arxiv.org/abs/2412.15204,,2025-07-19T19:56:15.029281+00:00,False
longfact-concepts,"['general', 'reasoning']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"LongFact is a benchmark for evaluating long-form factuality in large language models. It comprises 2,280 fact-seeking prompts spanning 38 topics, designed to test a model's ability to generate accurate, long-form responses. The benchmark uses SAFE (Search-Augmented Factuality Evaluator) to evaluate factual accuracy.",,en,1.0,text,False,LongFact Concepts,https://arxiv.org/abs/2403.18802,,2025-07-24T12:00:00.000000+00:00,False
longfact-objects,"['general', 'reasoning']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"LongFact is a benchmark for evaluating long-form factuality in large language models. It comprises 2,280 fact-seeking prompts spanning 38 topics, designed to test a model's ability to generate accurate, long-form responses. The benchmark uses SAFE (Search-Augmented Factuality Evaluator) to evaluate factual accuracy.",,en,1.0,text,False,LongFact Objects,https://arxiv.org/abs/2403.18802,,2025-07-24T12:00:00.000000+00:00,False
longvideobench,"['vision', 'long_context', 'multimodal']",2025-07-19T19:56:14.730349+00:00,benchmark_definition,"LongVideoBench is a question-answering benchmark featuring video-language interleaved inputs up to an hour long. It includes 3,763 varying-length web-collected videos with subtitles across diverse themes and 6,678 human-annotated multiple-choice questions in 17 fine-grained categories for comprehensive evaluation of long-term multimodal understanding.",,en,1.0,multimodal,False,LongVideoBench,https://arxiv.org/abs/2407.15754,,2025-07-19T19:56:14.730349+00:00,False
lsat,"['reasoning', 'legal', 'general']",2025-07-19T19:56:15.409871+00:00,benchmark_definition,"LSAT (Law School Admission Test) benchmark evaluating complex reasoning capabilities across three challenging tasks: analytical reasoning, logical reasoning, and reading comprehension. The LSAT measures skills considered essential for success in law school including critical thinking, reading comprehension of complex texts, and analysis of arguments.",,en,1.0,text,False,LSAT,https://arxiv.org/abs/2108.00648,,2025-07-19T19:56:15.409871+00:00,False
lvbench,"['vision', 'multimodal', 'long_context']",2025-07-19T19:56:12.724041+00:00,benchmark_definition,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",,en,1.0,multimodal,False,LVBench,https://arxiv.org/abs/2406.08035,,2025-07-19T19:56:12.724041+00:00,False
math-(cot),"['math', 'reasoning']",2025-07-19T19:56:14.366159+00:00,benchmark_definition,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects. This variant uses Chain-of-Thought prompting to encourage step-by-step reasoning.",,en,1.0,text,False,MATH (CoT),https://arxiv.org/abs/2103.03874,,2025-07-19T19:56:14.366159+00:00,False
math-500,"['math', 'reasoning']",2025-07-19T19:56:12.027850+00:00,benchmark_definition,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",,en,1.0,text,False,MATH-500,https://arxiv.org/abs/2103.03874,,2025-07-19T19:56:12.027850+00:00,False
math,"['math', 'reasoning']",2025-07-19T19:56:11.804258+00:00,benchmark_definition,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",,en,1.0,text,False,MATH,https://arxiv.org/abs/2103.03874,,2025-07-19T19:56:11.804258+00:00,False
mathvision,"['math', 'vision', 'multimodal']",2025-07-19T19:56:14.695583+00:00,benchmark_definition,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",,en,1.0,multimodal,False,MathVision,https://arxiv.org/abs/2402.14804,,2025-07-19T19:56:14.695583+00:00,False
mathvista-mini,"['math', 'vision', 'multimodal']",2025-07-19T19:56:13.654470+00:00,benchmark_definition,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",,en,1.0,multimodal,False,MathVista-Mini,https://arxiv.org/abs/2310.02255,,2025-07-19T19:56:13.654470+00:00,False
mathvista,"['math', 'vision', 'multimodal']",2025-07-19T19:56:12.069611+00:00,benchmark_definition,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",,en,1.0,multimodal,False,MathVista,https://arxiv.org/abs/2310.02255,,2025-07-19T19:56:12.069611+00:00,False
mbpp+,"['reasoning', 'general']",2025-07-19T19:56:14.501855+00:00,benchmark_definition,"MBPP+ is an enhanced version of MBPP (Mostly Basic Python Problems) with significantly more test cases (35x) for more rigorous evaluation. MBPP is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality.",,en,1.0,text,False,MBPP+,https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:14.501855+00:00,False
mbpp-++-base-version,"['reasoning', 'general']",2025-07-19T19:56:14.341560+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality. This is an enhanced version with additional test cases.",,en,1.0,text,False,MBPP ++ base version,https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:14.341560+00:00,False
mbpp-evalplus-(base),"['reasoning', 'general']",2025-07-19T19:56:14.421722+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. EvalPlus extends MBPP with significantly more test cases (35x) for more rigorous evaluation of LLM-synthesized code, providing high-quality and precise evaluation.",,en,1.0,text,False,MBPP EvalPlus (base),https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:14.421722+00:00,False
mbpp-evalplus,"['reasoning', 'general']",2025-07-19T19:56:14.425667+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. EvalPlus extends MBPP with significantly more test cases (35x) for more rigorous evaluation of LLM-synthesized code, providing high-quality and precise evaluation.",,en,1.0,text,False,MBPP EvalPlus,https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:14.425667+00:00,False
mbpp-pass@1,"['reasoning', 'general']",2025-07-19T19:56:15.138778+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases. This variant uses pass@1 evaluation metric measuring the percentage of problems solved correctly on the first attempt.",,en,1.0,text,False,MBPP pass@1,https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:15.138778+00:00,False
mbpp-plus,"['reasoning', 'code']",2025-08-03T22:06:11.143382+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality. This is an enhanced version with additional test cases for more rigorous evaluation.",,en,1.0,text,False,MBPP Plus,https://arxiv.org/abs/2108.07732,,2025-08-03T22:06:11.143382+00:00,False
mbpp,"['reasoning', 'general']",2025-07-19T19:56:13.453370+00:00,benchmark_definition,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",,en,100.0,text,False,MBPP,https://arxiv.org/abs/2108.07732,,2025-07-19T19:56:13.453370+00:00,False
medxpertqa,"['healthcare', 'reasoning', 'multimodal']",2025-07-19T19:56:14.040381+00:00,benchmark_definition,"A comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning, featuring 4,460 questions spanning 17 specialties and 11 body systems. Includes both text-only and multimodal subsets with expert-level exam questions incorporating diverse medical images and rich clinical information.",,en,1.0,multimodal,False,MedXpertQA,https://arxiv.org/abs/2501.18362,,2025-07-19T19:56:14.040381+00:00,False
mega-mlqa,"['language', 'reasoning']",2025-07-19T19:56:14.187404+00:00,benchmark_definition,"MLQA as part of the MEGA (Multilingual Evaluation of Generative AI) benchmark suite. A multi-way aligned extractive QA evaluation benchmark for cross-lingual question answering across 7 languages (English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese) with over 12K QA instances in English and 5K in each other language.",,en,1.0,text,True,MEGA MLQA,https://arxiv.org/abs/2303.12528,,2025-07-19T19:56:14.187404+00:00,False
mega-tydi-qa,"['language', 'reasoning']",2025-07-19T19:56:14.192871+00:00,benchmark_definition,"TyDi QA as part of the MEGA benchmark suite. A question answering dataset covering 11 typologically diverse languages (Arabic, Bengali, English, Finnish, Indonesian, Japanese, Korean, Russian, Swahili, Telugu, and Thai) with 204K question-answer pairs. Features realistic information-seeking questions written by people who want to know the answer but don't know it yet.",,en,1.0,text,True,MEGA TyDi QA,https://arxiv.org/abs/2003.05002,,2025-07-19T19:56:14.192871+00:00,False
mega-udpos,['language'],2025-07-19T19:56:14.198318+00:00,benchmark_definition,"Universal Dependencies POS tagging as part of the MEGA benchmark suite. A multilingual part-of-speech tagging dataset based on Universal Dependencies treebanks, utilizing the universal POS tag set of 17 tags across 38 diverse languages from different language families. Used for evaluating multilingual POS tagging systems.",,en,1.0,text,True,MEGA UDPOS,https://arxiv.org/abs/2004.10643,,2025-07-19T19:56:14.198318+00:00,False
mega-xcopa,"['reasoning', 'language']",2025-07-19T19:56:14.205296+00:00,benchmark_definition,"XCOPA (Cross-lingual Choice of Plausible Alternatives) as part of the MEGA benchmark suite. A typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, including resource-poor languages like Eastern Apurmac Quechua and Haitian Creole. Requires models to select which choice is the effect or cause of a given premise.",,en,1.0,text,True,MEGA XCOPA,https://arxiv.org/abs/2005.00333,,2025-07-19T19:56:14.205296+00:00,False
mega-xstorycloze,"['reasoning', 'language']",2025-07-19T19:56:14.212479+00:00,benchmark_definition,"XStoryCloze as part of the MEGA benchmark suite. A cross-lingual story completion task that consists of professionally translated versions of the English StoryCloze dataset to 10 non-English languages. Requires models to predict the correct ending for a given four-sentence story, evaluating commonsense reasoning and narrative understanding.",,en,1.0,text,True,MEGA XStoryCloze,https://arxiv.org/abs/2303.12528,,2025-07-19T19:56:14.212479+00:00,False
meld,"['multimodal', 'psychology']",2025-07-19T19:56:14.842977+00:00,benchmark_definition,"MELD (Multimodal EmotionLines Dataset) is a multimodal multi-party dataset for emotion recognition in conversations. Contains approximately 13,000 utterances from 1,433 dialogues extracted from the TV series Friends. Each utterance is annotated with emotion (Anger, Disgust, Sadness, Joy, Neutral, Surprise, Fear) and sentiment labels across audio, visual, and textual modalities.",,en,1.0,multimodal,False,Meld,https://arxiv.org/abs/1810.02508,,2025-07-19T19:56:14.842977+00:00,False
mgsm,"['math', 'reasoning']",2025-07-19T19:56:13.669061+00:00,benchmark_definition,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",,en,1.0,text,True,MGSM,https://arxiv.org/abs/2210.03057,,2025-07-19T19:56:13.669061+00:00,False
mimic-cxr,"['healthcare', 'vision', 'multimodal']",2025-07-19T19:56:14.017221+00:00,benchmark_definition,"MIMIC-CXR is a large publicly available dataset of chest radiographs with free-text radiology reports. Contains 377,110 images corresponding to 227,835 radiographic studies from 65,379 patients at Beth Israel Deaconess Medical Center. The dataset is de-identified and widely used for medical imaging research, automated report generation, and medical AI development.",,en,1.0,multimodal,False,MIMIC CXR,https://arxiv.org/abs/1901.07042,,2025-07-19T19:56:14.017221+00:00,False
mlvu-m,['general'],2025-07-19T19:56:14.931298+00:00,benchmark_definition,MLVU-M benchmark,,en,1.0,text,False,MLVU-M,,,2025-07-19T19:56:14.931298+00:00,False
mlvu,"['video', 'multimodal', 'long_context']",2025-07-19T19:56:14.755571+00:00,benchmark_definition,"A comprehensive benchmark for multi-task long video understanding that evaluates multimodal large language models on videos ranging from 3 minutes to 2 hours across 9 distinct tasks including reasoning, captioning, recognition, and summarization.",,en,1.0,multimodal,False,MLVU,https://arxiv.org/abs/2406.04264,,2025-07-19T19:56:14.755571+00:00,False
mm-if-eval,"['multimodal', 'reasoning']",2025-07-19T19:56:15.142939+00:00,benchmark_definition,"A challenging multimodal instruction-following benchmark that includes both compose-level constraints for output responses and perception-level constraints tied to input images, with comprehensive evaluation pipeline.",,en,1.0,multimodal,False,MM IF-Eval,https://arxiv.org/abs/2504.07957,,2025-07-19T19:56:15.142939+00:00,False
mm-mind2web,"['multimodal', 'frontend_development', 'reasoning']",2025-07-19T19:56:12.753488+00:00,benchmark_definition,"A multimodal web navigation benchmark comprising 2,000 open-ended tasks spanning 137 websites across 31 domains. Each task includes HTML documents paired with webpage screenshots, action sequences, and complex web interactions.",,en,1.0,multimodal,False,MM-Mind2Web,https://arxiv.org/abs/2306.06070,,2025-07-19T19:56:12.753488+00:00,False
mm-mt-bench,"['multimodal', 'communication']",2025-07-19T19:56:14.880812+00:00,benchmark_definition,A multi-turn LLM-as-a-judge evaluation benchmark for testing multimodal instruction-tuned models' ability to follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner.,,en,100.0,multimodal,False,MM-MT-Bench,,,2025-07-19T19:56:14.880812+00:00,False
mmau-music,"['audio', 'multimodal', 'reasoning']",2025-07-19T19:56:14.851711+00:00,benchmark_definition,A subset of the MMAU benchmark focused specifically on music understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across music audio clips.,,en,1.0,multimodal,False,MMAU Music,https://arxiv.org/abs/2410.19168,,2025-07-19T19:56:14.851711+00:00,False
mmau-sound,"['audio', 'multimodal', 'reasoning']",2025-07-19T19:56:14.859503+00:00,benchmark_definition,A subset of the MMAU benchmark focused specifically on environmental sound understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across environmental sound clips.,,en,1.0,multimodal,False,MMAU Sound,https://arxiv.org/abs/2410.19168,,2025-07-19T19:56:14.859503+00:00,False
mmau-speech,"['audio', 'multimodal', 'reasoning', 'speech-to-text']",2025-07-19T19:56:14.863540+00:00,benchmark_definition,A subset of the MMAU benchmark focused specifically on speech understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across speech audio clips.,,en,1.0,multimodal,False,MMAU Speech,https://arxiv.org/abs/2410.19168,,2025-07-19T19:56:14.863540+00:00,False
mmau,"['audio', 'multimodal', 'reasoning']",2025-07-19T19:56:14.846435+00:00,benchmark_definition,"A massive multi-task audio understanding and reasoning benchmark comprising 10,000 carefully curated audio clips paired with human-annotated natural language questions spanning speech, environmental sounds, and music. Requires expert-level knowledge and complex reasoning across 27 distinct skills.",,en,1.0,multimodal,False,MMAU,https://arxiv.org/abs/2410.19168,,2025-07-19T19:56:14.846435+00:00,False
mmbench-test,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.607904+00:00,benchmark_definition,"Test set of MMBench, a bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",,en,1.0,multimodal,False,MMBench_test,https://arxiv.org/abs/2307.06281,,2025-07-19T19:56:14.607904+00:00,False
mmbench-v1.1,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.868950+00:00,benchmark_definition,"Version 1.1 of MMBench, an improved bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",,en,1.0,multimodal,True,MMBench-V1.1,https://arxiv.org/abs/2307.06281,,2025-07-19T19:56:14.868950+00:00,False
mmbench-video,"['video', 'multimodal', 'reasoning']",2025-07-19T19:56:14.738914+00:00,benchmark_definition,"A long-form multi-shot benchmark for holistic video understanding that incorporates approximately 600 web videos from YouTube spanning 16 major categories, with each video ranging from 30 seconds to 6 minutes. Includes roughly 2,000 original question-answer pairs covering 26 fine-grained capabilities.",,en,1.0,multimodal,False,MMBench-Video,https://arxiv.org/abs/2406.14515,,2025-07-19T19:56:14.738914+00:00,False
mmbench,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.235585+00:00,benchmark_definition,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",,en,1.0,multimodal,True,MMBench,https://arxiv.org/abs/2307.06281,,2025-07-19T19:56:14.235585+00:00,False
mme-realworld,"['vision', 'multimodal', 'general']",2025-07-19T19:56:14.877676+00:00,benchmark_definition,"A comprehensive evaluation benchmark for Multimodal Large Language Models featuring over 13,366 high-resolution images and 29,429 question-answer pairs across 43 subtasks and 5 real-world scenarios. The largest manually annotated multimodal benchmark to date, designed to test MLLMs on challenging high-resolution real-world scenarios.",,en,1.0,multimodal,False,MME-RealWorld,https://arxiv.org/abs/2408.13257,,2025-07-19T19:56:14.877676+00:00,False
mme,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:15.022505+00:00,benchmark_definition,A comprehensive evaluation benchmark for Multimodal Large Language Models measuring both perception and cognition abilities across 14 subtasks. Features manually designed instruction-answer pairs to avoid data leakage and provides systematic quantitative assessment of MLLM capabilities.,,en,1.0,multimodal,False,MME,https://arxiv.org/abs/2306.13394,,2025-07-19T19:56:15.022505+00:00,False
mmlu-(cot),"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:14.330830+00:00,benchmark_definition,"Chain-of-Thought variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses chain-of-thought prompting to elicit step-by-step reasoning.",,en,1.0,text,False,MMLU (CoT),https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:14.330830+00:00,False
mmlu-base,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:14.562710+00:00,benchmark_definition,"Base version of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. Designed to comprehensively measure the breadth and depth of a model's academic and professional understanding.",,en,1.0,text,False,MMLU-Base,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:14.562710+00:00,False
mmlu-chat,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:15.095600+00:00,benchmark_definition,"Chat-format variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses conversational prompting format for model evaluation.",,en,1.0,text,False,MMLU Chat,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:15.095600+00:00,False
mmlu-french,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:15.175211+00:00,benchmark_definition,"French language variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This multilingual version tests model performance in French.",,fr,1.0,text,True,MMLU French,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:15.175211+00:00,False
mmlu-pro,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:11.408351+00:00,benchmark_definition,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",,en,1.0,text,False,MMLU-Pro,https://arxiv.org/abs/2406.01574,,2025-07-19T19:56:11.408351+00:00,False
mmlu-prox,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:13.738623+00:00,benchmark_definition,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,,en,1.0,text,False,MMLU-ProX,https://arxiv.org/abs/2406.01574,,2025-07-19T19:56:13.738623+00:00,False
mmlu-redux-2.0,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:11.518552+00:00,benchmark_definition,"A curated version of the MMLU benchmark featuring manually re-annotated 5,700 questions across 57 subjects to identify and correct errors in the original dataset. Addresses the 6.49% error rate found in MMLU and provides more reliable evaluation metrics for language models.",,en,1.0,text,False,MMLU-redux-2.0,https://arxiv.org/abs/2406.04127,,2025-07-19T19:56:11.518552+00:00,False
mmlu-redux,"['language', 'reasoning', 'math', 'general']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,,en,1.0,text,False,MMLU-Redux,https://arxiv.org/abs/2406.04127,,2025-09-05T00:00:00.000000+00:00,False
mmlu-stem,"['math', 'reasoning', 'physics', 'chemistry']",2025-07-19T19:56:14.495405+00:00,benchmark_definition,"STEM-focused subset of the Massive Multitask Language Understanding benchmark, evaluating language models on science, technology, engineering, and mathematics topics including physics, chemistry, mathematics, and other technical subjects.",,en,1.0,text,False,MMLU-STEM,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:14.495405+00:00,False
mmlu,"['general', 'reasoning', 'language', 'math']",2025-07-19T19:56:11.200416+00:00,benchmark_definition,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",,en,1.0,text,False,MMLU,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:11.200416+00:00,False
mmmlu,"['language', 'reasoning', 'math', 'general']",2025-07-19T19:56:14.144789+00:00,benchmark_definition,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",,en,1.0,text,True,MMMLU,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:14.144789+00:00,False
mmmu-(val),"['vision', 'multimodal', 'reasoning', 'general']",2025-07-19T19:56:13.593262+00:00,benchmark_definition,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",,en,1.0,multimodal,False,MMMU (val),https://arxiv.org/abs/2311.16502,,2025-07-19T19:56:13.593262+00:00,False
mmmu-(validation),"['vision', 'multimodal', 'reasoning', 'general']",2025-07-19T19:56:15.118197+00:00,benchmark_definition,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",,en,1.0,multimodal,False,MMMU (validation),https://arxiv.org/abs/2311.16502,,2025-07-19T19:56:15.118197+00:00,False
mmmu-pro,"['vision', 'multimodal', 'reasoning', 'general']",2025-07-19T19:56:14.282252+00:00,benchmark_definition,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",,en,1.0,multimodal,False,MMMU-Pro,https://arxiv.org/abs/2409.02813,,2025-07-19T19:56:14.282252+00:00,False
mmmu,"['multimodal', 'reasoning', 'general']",2025-07-19T19:56:12.130105+00:00,benchmark_definition,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",,en,1.0,multimodal,False,MMMU,https://arxiv.org/abs/2311.16502,,2025-07-19T19:56:12.130105+00:00,False
mmmuval,"['vision', 'general', 'reasoning', 'multimodal']",2025-07-19T19:56:14.575948+00:00,benchmark_definition,"Validation set for MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning) benchmark, designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning across Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering.",,en,1.0,multimodal,False,MMMUval,https://arxiv.org/abs/2311.16502,,2025-07-19T19:56:14.575948+00:00,False
mmstar,"['vision', 'multimodal', 'reasoning', 'general']",2025-07-19T19:56:14.660584+00:00,benchmark_definition,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",,en,1.0,multimodal,False,MMStar,https://arxiv.org/abs/2403.20330,,2025-07-19T19:56:14.660584+00:00,False
mmt-bench,"['vision', 'multimodal', 'reasoning', 'general']",2025-07-19T19:56:14.674184+00:00,benchmark_definition,"MMT-Bench is a comprehensive multimodal benchmark for evaluating Large Vision-Language Models towards multitask AGI. It comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",,en,1.0,multimodal,False,MMT-Bench,https://arxiv.org/abs/2404.16006,,2025-07-19T19:56:14.674184+00:00,False
mmvet,"['vision', 'multimodal', 'reasoning', 'general', 'spatial_reasoning', 'math']",2025-07-19T19:56:14.684742+00:00,benchmark_definition,"MM-Vet is an evaluation benchmark that examines large multimodal models on complicated multimodal tasks requiring integrated capabilities. It assesses six core vision-language capabilities: recognition, knowledge, spatial awareness, language generation, OCR, and math through questions that require one or more of these capabilities.",,en,1.0,multimodal,False,MMVet,https://arxiv.org/abs/2308.02490,,2025-07-19T19:56:14.684742+00:00,False
mmvetgpt4turbo,"['vision', 'multimodal', 'reasoning', 'general', 'spatial_reasoning', 'math']",2025-07-19T19:56:14.611567+00:00,benchmark_definition,"MM-Vet evaluation using GPT-4 Turbo for scoring. This variant of MM-Vet examines large multimodal models on complicated multimodal tasks requiring integrated capabilities across six core vision-language abilities: recognition, knowledge, spatial awareness, language generation, OCR, and math.",,en,1.0,multimodal,False,MMVetGPT4Turbo,https://arxiv.org/abs/2308.02490,,2025-07-19T19:56:14.611567+00:00,False
mobileminiwob++-sr,"['multimodal', 'frontend_development']",2025-07-19T19:56:14.816755+00:00,benchmark_definition,"MobileMiniWob++ SR (Success Rate) is an adaptation of the MiniWob++ web interaction benchmark for mobile Android environments within AndroidWorld. It comprises 92 web interaction tasks adapted for touch-based mobile interfaces, evaluating agents' ability to navigate and interact with web applications on mobile devices.",,en,1.0,multimodal,False,MobileMiniWob++_SR,https://arxiv.org/abs/2405.14573,,2025-07-19T19:56:14.816755+00:00,False
mrcr-1m-(pointwise),"['long_context', 'reasoning', 'general']",2025-07-19T19:56:13.912789+00:00,benchmark_definition,"MRCR 1M (pointwise) is a variant of the Multi-Round Coreference Resolution benchmark that uses pointwise evaluation for ultra-long contexts (~1M tokens). This version evaluates each response independently rather than comparatively, testing models' absolute performance on long-context reasoning tasks.",,en,1.0,text,False,MRCR 1M (pointwise),https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:13.912789+00:00,False
mrcr-1m,"['long_context', 'reasoning', 'general']",2025-07-19T19:56:13.954336+00:00,benchmark_definition,MRCR 1M is a variant of the Multi-Round Coreference Resolution benchmark designed for testing extremely long context capabilities with approximately 1 million tokens. It evaluates models' ability to maintain reasoning and attention across ultra-long conversations.,,en,1.0,text,False,MRCR 1M,https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:13.954336+00:00,False
mrcr-v2-(8-needle),"['long_context', 'reasoning', 'general']",2025-07-19T19:56:14.010914+00:00,benchmark_definition,MRCR v2 (8-needle) is a variant of the Multi-Round Coreference Resolution benchmark that includes 8 needle items to retrieve from long contexts. This tests models' ability to simultaneously track and reason about multiple pieces of information across extended conversations.,,en,1.0,text,False,MRCR v2 (8-needle),https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:14.010914+00:00,False
mrcr-v2,"['long_context', 'reasoning', 'general']",2025-07-19T19:56:13.963241+00:00,benchmark_definition,MRCR v2 (Multi-Round Coreference Resolution version 2) is an enhanced version of the synthetic long-context reasoning task. It extends the original MRCR framework with improved evaluation criteria and additional complexity for testing models' ability to maintain attention and reasoning across extended contexts.,,en,1.0,text,False,MRCR v2,https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:13.963241+00:00,False
mrcr,"['long_context', 'reasoning', 'general']",2025-07-19T19:56:13.887445+00:00,benchmark_definition,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,,en,1.0,text,False,MRCR,https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:13.887445+00:00,False
mt-bench,"['communication', 'reasoning', 'general', 'roleplay']",2025-07-19T19:56:14.516415+00:00,benchmark_definition,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",,en,100.0,text,False,MT-Bench,https://arxiv.org/abs/2306.05685,,2025-07-19T19:56:14.516415+00:00,False
mtvqa,"['vision', 'multimodal', 'text-to-image']",2025-07-19T19:56:14.587333+00:00,benchmark_definition,"MTVQA (Multilingual Text-Centric Visual Question Answering) is the first benchmark featuring high-quality human expert annotations across 9 diverse languages, consisting of 6,778 question-answer pairs across 2,116 images. It addresses visual-textual misalignment problems in multilingual text-centric VQA.",,en,1.0,multimodal,True,MTVQA,https://arxiv.org/abs/2405.11985,,2025-07-19T19:56:14.587333+00:00,False
muirbench,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.888428+00:00,benchmark_definition,"A comprehensive benchmark for robust multi-image understanding capabilities of multimodal LLMs. Consists of 12 diverse multi-image tasks involving 10 categories of multi-image relations (e.g., multiview, temporal relations, narrative, complementary). Comprises 11,264 images and 2,600 multiple-choice questions created in a pairwise manner, where each standard instance is paired with an unanswerable variant for reliable assessment.",,en,1.0,multimodal,False,MuirBench,https://arxiv.org/abs/2406.09411,,2025-07-19T19:56:14.888428+00:00,False
multi-if,"['reasoning', 'communication', 'language']",2025-07-19T19:56:14.638787+00:00,benchmark_definition,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",,en,1.0,text,True,Multi-IF,https://arxiv.org/abs/2410.15553,,2025-07-19T19:56:14.638787+00:00,False
multi-swe-bench,"['reasoning', 'code']",2025-09-15T00:00:00.000000+00:00,benchmark_definition,"A multilingual benchmark for issue resolving that evaluates Large Language Models' ability to resolve software issues across diverse programming ecosystems. Covers 7 programming languages (Java, TypeScript, JavaScript, Go, Rust, C, and C++) with 1,632 high-quality instances carefully annotated by 68 expert annotators. Addresses limitations of existing benchmarks that focus almost exclusively on Python.",,en,1.0,text,True,Multi-SWE-Bench,https://arxiv.org/abs/2504.02605,,2025-09-15T00:00:00.000000+00:00,False
multichallenge-(o3-mini-grader),"['reasoning', 'language']",2025-07-19T19:56:15.235758+00:00,benchmark_definition,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",,en,1.0,text,False,MultiChallenge (o3-mini grader),https://arxiv.org/abs/2501.17399,,2025-07-19T19:56:15.235758+00:00,False
multichallenge,"['communication', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",,en,1.0,text,False,Multi-Challenge,https://arxiv.org/abs/2501.17399,,2025-09-05T00:00:00.000000+00:00,False
multilf,['general'],2025-07-19T19:56:14.628191+00:00,benchmark_definition,MultiLF benchmark,,en,1.0,text,False,MultiLF,,,2025-07-19T19:56:14.628191+00:00,False
multilingual-mgsm-(cot),"['math', 'reasoning']",2025-07-19T19:56:14.402248+00:00,benchmark_definition,Multilingual Grade School Math (MGSM) benchmark evaluates language models' chain-of-thought reasoning abilities across ten typologically diverse languages. Contains 250 grade-school math problems manually translated from GSM8K dataset into languages including Bengali and Swahili.,,en,1.0,text,True,Multilingual MGSM (CoT),https://arxiv.org/abs/2210.03057,,2025-07-19T19:56:14.402248+00:00,False
multilingual-mmlu,"['general', 'reasoning', 'language']",2025-07-19T19:56:14.139086+00:00,benchmark_definition,"MMLU-ProX is a comprehensive multilingual benchmark covering 29 typologically diverse languages, building upon MMLU-Pro. Each language version consists of 11,829 identical questions enabling direct cross-linguistic comparisons. The benchmark evaluates large language models' reasoning capabilities across linguistic and cultural boundaries through challenging, reasoning-focused questions with 10 answer choices.",,en,1.0,text,True,Multilingual MMLU,https://arxiv.org/abs/2503.10497,,2025-07-19T19:56:14.139086+00:00,False
multipl-e-humaneval,"['language', 'general']",2025-07-19T19:56:14.345081+00:00,benchmark_definition,"MultiPL-E is a scalable and extensible approach to benchmarking neural code generation that translates unit test-driven code generation benchmarks across multiple programming languages. It extends the HumanEval benchmark to 18 additional programming languages, enabling evaluation of code generation models across diverse programming paradigms and providing insights into how models generalize programming knowledge across language boundaries.",,en,1.0,text,True,Multipl-E HumanEval,https://arxiv.org/abs/2208.08227,,2025-07-19T19:56:14.345081+00:00,False
multipl-e-mbpp,"['general', 'reasoning']",2025-07-19T19:56:14.353635+00:00,benchmark_definition,"MultiPL-E extends the Mostly Basic Python Problems (MBPP) benchmark to 18+ programming languages for evaluating multilingual code generation capabilities. MBPP contains 974 crowd-sourced programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality. Each problem includes a task description, code solution, and automated test cases.",,en,1.0,text,True,Multipl-E MBPP,https://arxiv.org/abs/2208.08227,,2025-07-19T19:56:14.353635+00:00,False
multipl-e,"['general', 'language']",2025-07-19T19:56:12.311919+00:00,benchmark_definition,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",,en,1.0,text,True,MultiPL-E,https://arxiv.org/abs/2208.08227,,2025-07-19T19:56:12.311919+00:00,False
musiccaps,"['audio', 'multimodal']",2025-07-19T19:56:14.892085+00:00,benchmark_definition,"MusicCaps is a dataset composed of 5,521 music examples, each labeled with an English aspect list and a free text caption written by musicians. The dataset contains 10-second music clips from AudioSet paired with rich textual descriptions that capture sonic qualities and musical elements like genre, mood, tempo, instrumentation, and rhythm. Created to support research in music-text understanding and generation tasks.",,en,1.0,multimodal,False,MusicCaps,https://arxiv.org/abs/2301.11325,,2025-07-19T19:56:14.892085+00:00,False
musr,['reasoning'],2025-07-19T19:56:12.708705+00:00,benchmark_definition,"MuSR (Multistep Soft Reasoning) is a benchmark for evaluating language models on multistep soft reasoning tasks specified in natural language narratives. Created through a neurosymbolic synthetic-to-natural generation algorithm, it generates complex reasoning scenarios like murder mysteries roughly 1000 words in length that challenge current LLMs including GPT-4. The benchmark tests chain-of-thought reasoning capabilities across domains involving commonsense reasoning about physical and social situations.",,en,1.0,text,False,MuSR,https://arxiv.org/abs/2310.16049,,2025-07-19T19:56:12.708705+00:00,False
mvbench,"['vision', 'video', 'multimodal', 'spatial_reasoning', 'reasoning']",2025-07-19T19:56:14.615534+00:00,benchmark_definition,"A comprehensive multi-modal video understanding benchmark covering 20 challenging video tasks that require temporal understanding beyond single-frame analysis. Tasks span from perception to cognition, including action recognition, temporal reasoning, spatial reasoning, object interaction, scene transition, and counterfactual inference. Uses a novel static-to-dynamic method to systematically generate video tasks from existing annotations.",,en,1.0,multimodal,False,MVBench,https://arxiv.org/abs/2311.17005,,2025-07-19T19:56:14.615534+00:00,False
natural-questions,"['reasoning', 'general', 'search']",2025-07-19T19:56:13.178778+00:00,benchmark_definition,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",,en,1.0,text,False,Natural Questions,https://arxiv.org/abs/1901.08634,,2025-07-19T19:56:13.178778+00:00,False
natural2code,"['reasoning', 'general']",2025-07-19T19:56:13.518784+00:00,benchmark_definition,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",,en,1.0,text,False,Natural2Code,https://arxiv.org/abs/2405.04520,,2025-07-19T19:56:13.518784+00:00,False
nexus,['general'],2025-07-19T19:56:14.391550+00:00,benchmark_definition,NexusRaven benchmark for evaluating function calling capabilities of large language models in zero-shot scenarios across cybersecurity tools and API interactions,,en,1.0,text,False,Nexus,https://openreview.net/pdf?id=5lcPe6DqfI,,2025-07-19T19:56:14.391550+00:00,False
nih-multi-needle,['long_context'],2025-07-19T19:56:14.465778+00:00,benchmark_definition,Multi-needle in a haystack benchmark for evaluating long-context comprehension capabilities of language models by testing retrieval of multiple target pieces of information from extended documents,,en,1.0,text,False,NIH/Multi-needle,https://arxiv.org/abs/2406.11230,,2025-07-19T19:56:14.465778+00:00,False
nmos,['general'],2025-07-19T19:56:14.895373+00:00,benchmark_definition,NMOS evaluation benchmark for assessing model performance on specialized tasks,,en,100.0,text,False,NMOS,,,2025-07-19T19:56:14.895373+00:00,False
nq,"['reasoning', 'general']",2025-07-19T19:56:15.088246+00:00,benchmark_definition,"Natural Questions (NQ) benchmark containing real user questions issued to Google search with answers found from Wikipedia, designed for training and evaluation of automatic question answering systems",,en,1.0,text,False,NQ,https://aclanthology.org/Q19-1026/,,2025-07-19T19:56:15.088246+00:00,False
ocrbench-v2-(en),"['vision', 'image-to-text']",2025-07-19T19:56:14.926330+00:00,benchmark_definition,OCRBench v2 English subset: Enhanced benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with English text content,,en,1.0,multimodal,False,OCRBench-V2 (en),https://arxiv.org/abs/2501.00321,,2025-07-19T19:56:14.926330+00:00,False
ocrbench-v2-(zh),"['vision', 'image-to-text']",2025-07-19T19:56:14.944963+00:00,benchmark_definition,OCRBench v2 Chinese subset: Enhanced benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with Chinese text content,,zh,1.0,multimodal,False,OCRBench-V2 (zh),https://arxiv.org/abs/2501.00321,,2025-07-19T19:56:14.944963+00:00,False
ocrbench-v2,"['vision', 'image-to-text']",2025-07-19T19:56:14.898625+00:00,benchmark_definition,"OCRBench v2: Enhanced large-scale bilingual benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with 10,000 human-verified question-answering pairs across 8 core OCR capabilities",,en,1.0,multimodal,True,OCRBench_V2,https://arxiv.org/abs/2501.00321,,2025-07-19T19:56:14.898625+00:00,False
ocrbench,"['vision', 'image-to-text']",2025-07-19T19:56:14.304601+00:00,benchmark_definition,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",,en,1.0,multimodal,False,OCRBench,https://arxiv.org/abs/2305.07895,,2025-07-19T19:56:14.304601+00:00,False
odinw,['vision'],2025-07-19T19:56:14.902703+00:00,benchmark_definition,Object Detection in the Wild (ODinW) benchmark for evaluating object detection models' task-level transfer ability across diverse real-world datasets in terms of prediction accuracy and adaptation efficiency,,en,1.0,image,False,ODinW,https://arxiv.org/abs/2112.03857,,2025-07-19T19:56:14.902703+00:00,False
ojbench,['reasoning'],2025-09-05T00:00:00.000000+00:00,benchmark_definition,"OJBench is a competition-level code benchmark designed to assess the competitive-level code reasoning abilities of large language models. It comprises 232 programming competition problems from NOI and ICPC, categorized into Easy, Medium, and Hard difficulty levels. The benchmark evaluates models' ability to solve complex competitive programming challenges using Python and C++.",,en,1.0,text,False,OJBench,https://arxiv.org/abs/2506.16395,,2025-09-05T00:00:00.000000+00:00,False
olympiadbench,"['math', 'reasoning', 'physics', 'multimodal']",2025-07-19T19:56:14.821916+00:00,benchmark_definition,"A challenging benchmark for promoting AGI with Olympiad-level bilingual multimodal scientific problems. Comprises 8,476 math and physics problems from international and Chinese Olympiads and the Chinese college entrance exam, featuring expert-level annotations for step-by-step reasoning. Includes both text-only and multimodal problems in English and Chinese.",,en,1.0,multimodal,True,OlympiadBench,https://arxiv.org/abs/2402.14008,,2025-07-19T19:56:14.821916+00:00,False
omnibench-music,"['multimodal', 'audio']",2025-07-19T19:56:14.911093+00:00,benchmark_definition,"Music component of OmniBench, a comprehensive benchmark for evaluating omni-language models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. The music category includes various compositions and performances that require integrated understanding across text, image, and audio modalities.",,en,1.0,multimodal,False,OmniBench Music,https://arxiv.org/abs/2409.15272,,2025-07-19T19:56:14.911093+00:00,False
omnibench,"['multimodal', 'reasoning']",2025-07-19T19:56:14.906402+00:00,benchmark_definition,"A novel multimodal benchmark designed to evaluate large language models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. Comprises 1,142 question-answer pairs covering 8 task categories from basic perception to complex inference, with a unique constraint that accurate responses require integrated understanding of all three modalities.",,en,1.0,multimodal,False,OmniBench,https://arxiv.org/abs/2409.15272,,2025-07-19T19:56:14.906402+00:00,False
omnimath,"['math', 'reasoning']",2025-07-19T19:56:14.271468+00:00,benchmark_definition,"A Universal Olympiad Level Mathematic Benchmark for Large Language Models containing 4,428 competition-level problems with rigorous human annotation, categorized into over 33 sub-domains and spanning more than 10 distinct difficulty levels",,en,1.0,text,False,OmniMath,https://arxiv.org/abs/2410.07985,,2025-07-19T19:56:14.271468+00:00,False
open-rewrite,"['language', 'writing']",2025-07-19T19:56:14.435616+00:00,benchmark_definition,"OpenRewriteEval is a benchmark for evaluating open-ended rewriting of long-form texts, covering a wide variety of rewriting types expressed through natural language instructions including formality, expansion, conciseness, paraphrasing, and tone and style transfer.",,en,1.0,text,False,Open-rewrite,https://arxiv.org/abs/2305.15685,,2025-07-19T19:56:14.435616+00:00,False
openai-mmlu,"['general', 'reasoning', 'math', 'legal', 'healthcare', 'finance', 'physics', 'chemistry', 'economics', 'psychology']",2025-07-19T19:56:14.043675+00:00,benchmark_definition,"MMLU (Massive Multitask Language Understanding) is a comprehensive benchmark that measures a text model's multitask accuracy across 57 diverse academic and professional subjects. The test covers elementary mathematics, US history, computer science, law, morality, business ethics, clinical knowledge, and many other domains spanning STEM, humanities, social sciences, and professional fields. To attain high accuracy, models must possess extensive world knowledge and problem-solving ability.",,en,1.0,text,False,OpenAI MMLU,https://arxiv.org/abs/2009.03300,,2025-07-19T19:56:14.043675+00:00,False
openai-mrcr:-2-needle-256k,"['long_context', 'reasoning']",2025-07-24T12:00:00.000000+00:00,benchmark_definition,"Multi-Round Co-reference Resolution (MRCR) benchmark that tests long-context reasoning by evaluating a model's ability to distinguish between similar outputs, reason about ordering, and reproduce specific content from multi-turn conversations containing multiple writing requests on overlapping topics at 256k tokens.",,en,1.0,text,False,OpenAI-MRCR: 2 needle 256k,https://arxiv.org/abs/2409.12640,,2025-07-24T12:00:00.000000+00:00,False
openai-mrcr:-2-needle-128k,"['long_context', 'reasoning']",2025-07-19T19:56:15.266878+00:00,benchmark_definition,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",,en,1.0,text,False,OpenAI-MRCR: 2 needle 128k,https://arxiv.org/abs/2403.05530,,2025-07-19T19:56:15.266878+00:00,False
openai-mrcr:-2-needle-1m,"['long_context', 'reasoning']",2025-07-19T19:56:15.280285+00:00,benchmark_definition,"Multi-Round Co-reference Resolution benchmark that tests an LLM's ability to distinguish between multiple similar needles hidden in long conversations. Models must reproduce specific instances of content (e.g., 'Return the 2nd poem about tapirs') from multi-turn synthetic conversations, requiring reasoning about context, ordering, and subtle differences between similar outputs.",,en,1.0,text,False,OpenAI-MRCR: 2 needle 1M,https://arxiv.org/abs/2409.12640,,2025-07-19T19:56:15.280285+00:00,False
openbookqa,"['reasoning', 'general']",2025-07-19T19:56:14.129348+00:00,benchmark_definition,"OpenBookQA is a question-answering dataset modeled after open book exams for assessing human understanding. It contains 5,957 multiple-choice elementary-level science questions that probe understanding of 1,326 core science facts and their application to novel situations, requiring combination of open book facts with broad common knowledge through multi-hop reasoning.",,en,1.0,text,False,OpenBookQA,https://arxiv.org/abs/1809.02789,,2025-07-19T19:56:14.129348+00:00,False
osworld-extended,"['general', 'reasoning', 'multimodal']",2025-07-19T19:56:15.113488+00:00,benchmark_definition,"OSWorld is a scalable, real computer environment benchmark for evaluating multimodal agents on open-ended tasks across Ubuntu, Windows, and macOS. It comprises 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows. The benchmark evaluates agents' ability to interact with computer interfaces using screenshots and actions in realistic computing environments.",,en,1.0,multimodal,False,OSWorld Extended,https://arxiv.org/abs/2404.07972,,2025-07-19T19:56:15.113488+00:00,False
osworld-screenshot-only,"['multimodal', 'vision', 'general']",2025-07-19T19:56:15.109647+00:00,benchmark_definition,"OSWorld Screenshot-only: A variant of the OSWorld benchmark that evaluates multimodal AI agents using only screenshot observations to complete open-ended computer tasks across real operating systems (Ubuntu, Windows, macOS). Tests agents' ability to perform complex workflows involving web apps, desktop applications, file I/O, and multi-application tasks through visual interface understanding and GUI grounding.",,en,1.0,multimodal,False,OSWorld Screenshot-only,https://arxiv.org/abs/2404.07972,,2025-07-19T19:56:15.109647+00:00,False
osworld,"['multimodal', 'general', 'vision']",2025-07-19T19:56:14.935426+00:00,benchmark_definition,"OSWorld: The first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across Ubuntu, Windows, and macOS with 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows",,en,1.0,multimodal,False,OSWorld,https://arxiv.org/abs/2404.07972,,2025-07-19T19:56:14.935426+00:00,False
pathmcqa,"['healthcare', 'vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.036453+00:00,benchmark_definition,"PathMMU is a massive multimodal expert-level benchmark for understanding and reasoning in pathology, containing 33,428 multimodal multi-choice questions and 24,067 images validated by seven pathologists. It evaluates Large Multimodal Models (LMMs) performance on pathology tasks, with the top-performing model GPT-4V achieving only 49.8% zero-shot performance compared to 71.8% for human pathologists.",,en,1.0,multimodal,False,PathMCQA,https://arxiv.org/abs/2401.16355,,2025-07-19T19:56:14.036453+00:00,False
perceptiontest,"['video', 'multimodal', 'reasoning', 'physics', 'spatial_reasoning']",2025-07-19T19:56:14.708910+00:00,benchmark_definition,"A novel multimodal video benchmark designed to evaluate perception and reasoning skills of pre-trained models across video, audio, and text modalities. Contains 11.6k real-world videos (average 23 seconds) filmed by participants worldwide, densely annotated with six types of labels. Focuses on skills (Memory, Abstraction, Physics, Semantics) and reasoning types (descriptive, explanatory, predictive, counterfactual). Shows significant performance gap between human baseline (91.4%) and state-of-the-art video QA models (46.2%).",,en,1.0,multimodal,False,PerceptionTest,https://arxiv.org/abs/2305.13786,,2025-07-19T19:56:14.708910+00:00,False
phibench,"['reasoning', 'math', 'general']",2025-07-19T19:56:14.121593+00:00,benchmark_definition,"PhiBench is an internal benchmark designed to evaluate diverse skills and reasoning abilities of language models, covering a wide range of tasks including coding (debugging, extending incomplete code, explaining code snippets) and mathematics (identifying proof errors, generating related problems). Created by Microsoft's research team to address limitations of standard academic benchmarks and guide the development of the Phi-4 model.",,en,1.0,text,False,PhiBench,https://arxiv.org/abs/2412.08905,,2025-07-19T19:56:14.121593+00:00,False
physicsfinals,"['physics', 'math', 'reasoning']",2025-07-19T19:56:13.981919+00:00,benchmark_definition,"PHYSICS is a comprehensive benchmark for university-level physics problem solving, containing 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. Even advanced models like o3-mini achieve only 59.9% accuracy.",,en,1.0,text,False,PhysicsFinals,https://arxiv.org/abs/2503.21821,,2025-07-19T19:56:13.981919+00:00,False
piqa,"['reasoning', 'physics', 'general']",2025-07-19T19:56:13.133817+00:00,benchmark_definition,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",,en,1.0,text,False,PIQA,https://arxiv.org/abs/1911.11641,,2025-07-19T19:56:13.133817+00:00,False
pointgrounding,"['vision', 'spatial_reasoning', 'multimodal']",2025-07-19T19:56:14.914897+00:00,benchmark_definition,"PointArena is a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. It includes Point-Bench, a curated dataset of ~1,000 pointing tasks across five categories: Spatial (positional references), Affordance (functional part identification), Counting (attribute-based grouping), Steerable (relative pointing), and Reasoning (open-ended visual inference). The benchmark evaluates language-guided pointing capabilities in vision-language models.",,en,1.0,multimodal,False,PointGrounding,https://arxiv.org/abs/2505.09990,,2025-07-19T19:56:14.914897+00:00,False
polymath-en,"['math', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"PolyMath is a multilingual mathematical reasoning benchmark covering 18 languages and 4 difficulty levels from easy to hard, ensuring difficulty comprehensiveness, language diversity, and high-quality translation. The benchmark evaluates mathematical reasoning capabilities of large language models across diverse linguistic contexts, making it a highly discriminative multilingual mathematical benchmark.",,en,1.0,text,True,PolyMath-en,https://arxiv.org/abs/2504.18428,,2025-09-05T00:00:00.000000+00:00,False
polymath,"['math', 'reasoning', 'spatial_reasoning', 'multimodal', 'vision']",2025-08-03T22:06:11.108063+00:00,benchmark_definition,"Polymath is a challenging multi-modal mathematical reasoning benchmark designed to evaluate the general cognitive reasoning abilities of Multi-modal Large Language Models (MLLMs). The benchmark comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning.",,en,1.0,multimodal,False,PolyMATH,https://arxiv.org/abs/2410.14702,,2025-08-03T22:06:11.108063+00:00,False
pope,"['vision', 'safety', 'multimodal']",2025-07-19T19:56:14.264312+00:00,benchmark_definition,"Polling-based Object Probing Evaluation (POPE) is a benchmark for evaluating object hallucination in Large Vision-Language Models (LVLMs). POPE addresses the problem where LVLMs generate objects inconsistent with target images by using a polling-based query method that asks yes/no questions about object presence in images, providing more stable and flexible evaluation of object hallucination.",,en,1.0,multimodal,False,POPE,https://arxiv.org/abs/2305.10355,,2025-07-19T19:56:14.264312+00:00,False
popqa,"['general', 'reasoning']",2025-07-19T19:56:15.072897+00:00,benchmark_definition,"PopQA is an entity-centric open-domain question-answering dataset consisting of 14,000 QA pairs designed to evaluate language models' ability to memorize and recall factual knowledge across entities with varying popularity levels. The dataset probes both parametric memory (stored in model parameters) and non-parametric memory effectiveness, with questions covering 16 diverse relationship types from Wikidata converted to natural language using templates. Created by sampling knowledge triples from Wikidata and converting them to natural language questions, focusing on long-tail entities to understand LMs' strengths and limitations in memorizing factual knowledge.",,en,1.0,text,False,PopQA,https://arxiv.org/abs/2212.10511,,2025-07-19T19:56:15.072897+00:00,False
qasper,"['reasoning', 'long_context']",2025-07-19T19:56:14.166932+00:00,benchmark_definition,"QASPER is a dataset of 5,049 information-seeking questions and answers anchored in 1,585 NLP research papers. Questions are written by NLP practitioners who read only titles and abstracts, while answers require understanding the full paper text and provide supporting evidence. The dataset challenges models with complex reasoning across document sections for academic document question answering. Each question seeks information present in the full text and is answered by a separate set of NLP practitioners who also provide supporting evidence to answers.",,en,1.0,text,False,Qasper,https://arxiv.org/abs/2105.03011,,2025-07-19T19:56:14.166932+00:00,False
qmsum,"['summarization', 'long_context']",2025-07-19T19:56:14.223595+00:00,benchmark_definition,"QMSum is a benchmark for query-based multi-domain meeting summarization consisting of 1,808 query-summary pairs over 232 meetings across academic, product, and committee domains. The dataset enables models to select and summarize relevant spans of meetings in response to specific queries. Published at NAACL 2021, QMSum presents significant challenges in long meeting summarization where models must identify and summarize relevant content based on user queries.",,en,1.0,text,False,QMSum,https://arxiv.org/abs/2104.05938,,2025-07-19T19:56:14.223595+00:00,False
realworldqa,"['vision', 'spatial_reasoning']",2025-07-19T19:56:14.595271+00:00,benchmark_definition,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",,en,1.0,multimodal,False,RealWorldQA,,,2025-07-19T19:56:14.595271+00:00,False
repobench,"['reasoning', 'code']",2025-07-19T19:56:15.152588+00:00,benchmark_definition,"RepoBench is a benchmark for evaluating repository-level code auto-completion systems through three interconnected tasks: RepoBench-R (retrieval of relevant code snippets across files), RepoBench-C (code completion with cross-file and in-file context), and RepoBench-P (pipeline combining retrieval and prediction). Supports Python and Java programming languages and addresses the gap in evaluating real-world, multi-file programming scenarios by providing a more complete comparison of performance in auto-completion systems.",,en,1.0,text,False,RepoBench,https://arxiv.org/abs/2306.03091,,2025-07-19T19:56:15.152588+00:00,False
repoqa,"['long_context', 'reasoning', 'code']",2025-07-19T19:56:14.180278+00:00,benchmark_definition,"RepoQA is a benchmark for evaluating long-context code understanding capabilities of Large Language Models through the Searching Needle Function (SNF) task, where LLMs must locate specific functions in code repositories using natural language descriptions. The benchmark contains 500 code search tasks spanning 50 repositories across 5 modern programming languages (Python, Java, TypeScript, C++, and Rust), tested on 26 general and code-specific LLMs to assess their ability to comprehend and navigate code repositories.",,en,1.0,text,True,RepoQA,https://arxiv.org/abs/2406.06025,,2025-07-19T19:56:14.180278+00:00,False
ruler,"['long_context', 'reasoning']",2025-07-19T19:56:14.175181+00:00,benchmark_definition,"RULER (What's the Real Context Size of Your Long-Context Language Models?) is a synthetic benchmark designed to comprehensively evaluate the long-context capabilities of language models. It expands on needle-in-a-haystack (NIAH) testing by introducing new task categories including multi-hop tracing and aggregation tasks. The benchmark provides flexible configurations for customized sequence length and task complexity, evaluating 17 long-context language models across 13 representative tasks to reveal that despite models claiming 32K+ token context sizes, only half maintain satisfactory performance at 32K length.",,en,1.0,text,False,RULER,https://arxiv.org/abs/2404.06654,,2025-07-19T19:56:14.175181+00:00,False
sat-math,"['math', 'reasoning']",2025-07-19T19:56:15.414463+00:00,benchmark_definition,"SAT Math benchmark from AGIEval containing standardized mathematics questions from the College Board SAT examination, designed to evaluate mathematical reasoning capabilities of foundation models using human-centric assessment methods.",,en,1.0,text,False,SAT Math,https://arxiv.org/abs/2304.06364,,2025-07-19T19:56:15.414463+00:00,False
scale-multichallenge,"['reasoning', 'communication', 'general']",2025-07-19T19:56:15.205789+00:00,benchmark_definition,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark developed by Scale AI that evaluates large language models on four challenging conversation categories: instruction retention, inference memory of user information, reliable versioned editing, and self-coherence. Each challenge requires accurate instruction-following, context allocation, and in-context reasoning. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge.",,en,1.0,text,False,Scale MultiChallenge,https://arxiv.org/abs/2501.17399,,2025-07-19T19:56:15.205789+00:00,False
scicode,"['reasoning', 'math', 'physics', 'chemistry', 'code']",2025-07-28T00:00:00.000000+00:00,benchmark_definition,"SciCode is a research coding benchmark curated by scientists that challenges language models to code solutions for scientific problems. It contains 338 subproblems decomposed from 80 challenging main problems across 16 natural science sub-fields including mathematics, physics, chemistry, biology, and materials science. Problems require knowledge recall, reasoning, and code synthesis skills.",,en,1.0,text,False,SciCode,https://arxiv.org/abs/2407.13168,,2025-07-28T00:00:00.000000+00:00,False
scienceqa-visual,"['vision', 'reasoning', 'multimodal']",2025-07-19T19:56:14.300722+00:00,benchmark_definition,"ScienceQA Visual is a multimodal science question answering benchmark consisting of 21,208 multiple-choice questions from elementary and high school science curricula. The dataset covers 3 subjects (natural science, language science, social science), 26 topics, 127 categories, and 379 skills. 48.7% of questions include image context requiring multimodal reasoning. Questions are annotated with lectures (83.9%) and explanations (90.5%) to support chain-of-thought reasoning for science question answering.",,en,1.0,multimodal,False,ScienceQA Visual,https://arxiv.org/abs/2209.09513,,2025-07-19T19:56:14.300722+00:00,False
scienceqa,"['reasoning', 'math', 'multimodal']",2025-07-19T19:56:14.255251+00:00,benchmark_definition,"ScienceQA is the first large-scale multimodal science question answering benchmark with 21,208 multiple-choice questions covering 3 subjects (natural science, language science, social science), 26 topics, 127 categories, and 379 skills. The benchmark includes both text and image modalities, featuring detailed explanations and Chain-of-Thought reasoning to diagnose multi-hop reasoning ability.",,en,1.0,multimodal,False,ScienceQA,https://arxiv.org/abs/2209.09513,,2025-07-19T19:56:14.255251+00:00,False
screenspot-pro,"['vision', 'multimodal', 'spatial_reasoning']",2025-07-19T19:56:14.776671+00:00,benchmark_definition,"ScreenSpot-Pro is a novel GUI grounding benchmark designed to rigorously evaluate the grounding capabilities of multimodal large language models (MLLMs) in professional high-resolution computing environments. The benchmark comprises 1,581 instructions across 23 applications spanning 5 industries and 3 operating systems, featuring authentic high-resolution images from professional domains with expert annotations. Unlike previous benchmarks that focus on cropped screenshots in consumer applications, ScreenSpot-Pro addresses the complexity and diversity of real-world professional software scenarios, revealing significant performance gaps in current MLLM GUI perception capabilities.",,en,1.0,multimodal,False,ScreenSpot Pro,https://arxiv.org/abs/2504.07981,,2025-07-19T19:56:14.776671+00:00,False
screenspot,"['vision', 'multimodal', 'spatial_reasoning']",2025-07-19T19:56:14.766976+00:00,benchmark_definition,"ScreenSpot is the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. The dataset comprises over 1,200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element types (text and icon/widget), designed to evaluate visual GUI agents' ability to accurately locate screen elements based on natural language instructions.",,en,1.0,multimodal,False,ScreenSpot,https://arxiv.org/abs/2401.10935,,2025-07-19T19:56:14.766976+00:00,False
simpleqa,"['general', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",,en,1.0,text,False,SimpleQA,https://arxiv.org/abs/2411.04368,,2025-09-05T00:00:00.000000+00:00,False
slakevqa,"['vision', 'healthcare', 'multimodal', 'reasoning']",2025-07-19T19:56:14.027646+00:00,benchmark_definition,"A semantically-labeled knowledge-enhanced dataset for medical visual question answering. Contains 642 radiology images (CT scans, MRI scans, X-rays) covering five body parts and 14,028 bilingual English-Chinese question-answer pairs annotated by experienced physicians. Features comprehensive semantic labels and a structural medical knowledge base with both vision-only and knowledge-based questions requiring external medical knowledge reasoning.",,en,1.0,multimodal,True,SlakeVQA,https://arxiv.org/abs/2102.09542,,2025-07-19T19:56:14.027646+00:00,False
social-iqa,"['reasoning', 'psychology']",2025-07-19T19:56:13.155825+00:00,benchmark_definition,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",,en,1.0,text,False,Social IQa,https://arxiv.org/abs/1904.09728,,2025-07-19T19:56:13.155825+00:00,False
spider,"['language', 'reasoning']",2025-07-19T19:56:15.156791+00:00,benchmark_definition,"A large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. Contains 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. Requires models to generalize to both new SQL queries and new database schemas, making it distinct from previous semantic parsing tasks that use single databases.",,en,1.0,text,False,Spider,https://arxiv.org/abs/1809.08887,,2025-07-19T19:56:15.156791+00:00,False
squality,"['summarization', 'long_context', 'language']",2025-07-19T19:56:12.712415+00:00,benchmark_definition,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",,en,1.0,text,False,SQuALITY,https://arxiv.org/abs/2205.11465,,2025-07-19T19:56:12.712415+00:00,False
stem,"['math', 'reasoning', 'multimodal']",2025-07-19T19:56:14.559354+00:00,benchmark_definition,"A comprehensive multimodal benchmark dataset with 448 skills and 1,073,146 questions spanning all STEM subjects (Science, Technology, Engineering, Mathematics), designed to test neural models' vision-language STEM skills based on K-12 curriculum. Unlike existing datasets that focus on expert-level ability, this dataset includes fundamental skills designed around educational standards.",,en,1.0,multimodal,False,STEM,https://arxiv.org/abs/2402.17205,,2025-07-19T19:56:14.559354+00:00,False
summscreenfd,"['summarization', 'long_context']",2025-07-19T19:56:14.229354+00:00,benchmark_definition,"SummScreenFD is the ForeverDreaming subset of the SummScreen dataset for abstractive screenplay summarization, comprising pairs of TV series transcripts and human-written recaps from 88 different shows. The dataset provides a challenging testbed for abstractive summarization where plot details are often expressed indirectly in character dialogues and scattered across the entirety of the transcript, requiring models to find and integrate these details to form succinct plot descriptions.",,en,1.0,text,False,SummScreenFD,https://arxiv.org/abs/2104.07091,,2025-07-19T19:56:14.229354+00:00,False
superglue,"['general', 'language', 'reasoning']",2025-07-19T19:56:15.382590+00:00,benchmark_definition,"SuperGLUE is a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. It includes 8 primary tasks: BoolQ (Boolean Questions), CB (CommitmentBank), COPA (Choice of Plausible Alternatives), MultiRC (Multi-Sentence Reading Comprehension), ReCoRD (Reading Comprehension with Commonsense Reasoning), RTE (Recognizing Textual Entailment), WiC (Word-in-Context), and WSC (Winograd Schema Challenge). The benchmark evaluates diverse language understanding capabilities including reading comprehension, commonsense reasoning, causal reasoning, coreference resolution, textual entailment, and word sense disambiguation across multiple domains.",,en,1.0,text,False,SuperGLUE,https://arxiv.org/abs/1905.00537,,2025-07-19T19:56:15.382590+00:00,False
supergpqa,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",,en,1.0,text,False,SuperGPQA,https://arxiv.org/abs/2502.14739,,2025-09-05T00:00:00.000000+00:00,False
swe-bench-multilingual,"['reasoning', 'code']",2025-07-19T19:56:12.340903+00:00,benchmark_definition,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",,en,1.0,text,True,SWE-bench Multilingual,https://arxiv.org/abs/2504.02605,,2025-07-19T19:56:12.340903+00:00,False
swe-bench-verified-(agentic-coding),"['reasoning', 'code']",2025-07-19T19:56:12.331440+00:00,benchmark_definition,"SWE-bench Verified is a human-filtered subset of 500 software engineering problems drawn from real GitHub issues across 12 popular Python repositories. Given a codebase and an issue description, language models are tasked with generating patches that resolve the described problems. This benchmark evaluates AI's real-world agentic coding skills by requiring models to navigate complex codebases, understand software engineering problems, and coordinate changes across multiple functions, classes, and files to fix well-defined issues with clear descriptions.",,en,1.0,text,False,SWE-bench Verified (Agentic Coding),https://arxiv.org/abs/2310.06770,,2025-07-19T19:56:12.331440+00:00,False
swe-bench-verified-(agentless),"['general', 'reasoning']",2025-07-19T19:56:12.328122+00:00,benchmark_definition,"A human-validated subset of SWE-bench that evaluates language models' ability to resolve real-world GitHub issues using an agentless approach. The benchmark tests models on software engineering problems requiring understanding and coordinating changes across multiple functions, classes, and files simultaneously.",,en,1.0,text,False,SWE-bench Verified (Agentless),https://arxiv.org/abs/2407.01489,,2025-07-19T19:56:12.328122+00:00,False
swe-bench-verified-(multiple-attempts),['reasoning'],2025-07-19T19:56:12.336780+00:00,benchmark_definition,"SWE-bench Verified is a human-validated subset of 500 test samples from the original SWE-bench dataset that evaluates AI systems' ability to automatically resolve real GitHub issues in Python repositories. Given a codebase and issue description, models must edit the code to successfully resolve the problem, requiring understanding and coordination of changes across multiple functions, classes, and files. The Verified version provides more reliable evaluation through manual validation of test samples.",,en,1.0,text,False,SWE-bench Verified (Multiple Attempts),https://arxiv.org/abs/2310.06770,,2025-07-19T19:56:12.336780+00:00,False
swe-bench-verified,"['reasoning', 'frontend_development', 'code']",2025-07-19T19:56:13.812805+00:00,benchmark_definition,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",,en,1.0,text,False,SWE-Bench Verified,https://arxiv.org/abs/2310.06770,,2025-07-19T19:56:13.812805+00:00,False
swe-dev,['frontend_development'],2025-09-15T00:00:00.000000+00:00,benchmark_definition,"SWE-bench development split consisting of 225 software engineering problems drawn from real GitHub issues across 12 popular Python repositories. Language models are given a codebase along with a description of an issue to be resolved and must edit the codebase to address the issue, often requiring understanding and coordinating changes across multiple functions, classes, and files.",,en,1.0,text,False,SWE-Dev,https://arxiv.org/abs/2310.06770,,2025-09-15T00:00:00.000000+00:00,False
swe-lancer-(ic-diamond-subset),"['reasoning', 'code']",2025-07-19T19:56:15.359574+00:00,benchmark_definition,"SWE-Lancer (IC-Diamond subset) is a benchmark of real-world freelance software engineering tasks from Upwork, ranging from $50 bug fixes to $32,000 feature implementations. It evaluates AI models on independent engineering tasks using end-to-end tests triple-verified by experienced software engineers, and includes managerial tasks where models choose between technical implementation proposals.",,en,1.0,text,False,SWE-Lancer (IC-Diamond subset),https://arxiv.org/abs/2502.12115,,2025-07-19T19:56:15.359574+00:00,False
swe-lancer,"['reasoning', 'code']",2025-07-19T19:56:15.352660+00:00,benchmark_definition,"A benchmark for evaluating large language models on real-world freelance software engineering tasks from Upwork. Contains over 1,400 tasks valued at $1 million USD total, ranging from $50 bug fixes to $32,000 feature implementations. Includes both independent engineering tasks graded via end-to-end tests and managerial tasks assessed against original engineering managers' choices.",,en,1.0,text,False,SWE-Lancer,https://arxiv.org/abs/2502.12115,,2025-07-19T19:56:15.352660+00:00,False
tau-bench-airline,"['reasoning', 'communication']",2025-07-19T19:56:14.993213+00:00,benchmark_definition,"Part of -bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",,en,1.0,text,False,TAU-bench Airline,https://arxiv.org/abs/2406.12045,,2025-07-19T19:56:14.993213+00:00,False
tau-bench-retail,"['reasoning', 'communication']",2025-07-19T19:56:14.965635+00:00,benchmark_definition,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",,en,1.0,text,False,TAU-bench Retail,https://arxiv.org/abs/2406.12045,,2025-07-19T19:56:14.965635+00:00,False
tau-bench,"['general', 'reasoning']",2025-07-19T19:56:15.219001+00:00,benchmark_definition,-bench: A benchmark for tool-agent-user interaction in real-world domains. Tests language agents' ability to interact with users and follow domain-specific rules through dynamic conversations using API tools and policy guidelines across retail and airline domains. Evaluates consistency and reliability of agent behavior over multiple trials.,,en,1.0,text,False,Tau-bench,https://arxiv.org/abs/2406.12045,,2025-07-19T19:56:15.219001+00:00,False
tau2-airline,"['reasoning', 'communication']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",,en,1.0,text,False,Tau2 Airline,https://arxiv.org/abs/2506.07982,,2025-09-05T00:00:00.000000+00:00,False
tau2-retail,"['communication', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",,en,1.0,text,False,Tau2 Retail,https://arxiv.org/abs/2506.07982,,2025-09-05T00:00:00.000000+00:00,False
tau2-telecom,"['communication', 'reasoning']",2025-09-05T00:00:00.000000+00:00,benchmark_definition,"-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",,en,1.0,text,False,Tau2 Telecom,https://arxiv.org/abs/2506.07982,,2025-09-05T00:00:00.000000+00:00,False
tempcompass,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.748364+00:00,benchmark_definition,"TempCompass is a comprehensive benchmark for evaluating temporal perception capabilities of Video Large Language Models (Video LLMs). It constructs conflicting videos that share identical static content but differ in specific temporal aspects to prevent models from exploiting single-frame bias. The benchmark evaluates multiple temporal aspects including action, motion, speed, temporal order, and attribute changes across diverse task formats including multi-choice QA, yes/no QA, caption matching, and caption generation.",,en,1.0,multimodal,False,TempCompass,https://arxiv.org/abs/2403.00476,,2025-07-19T19:56:14.748364+00:00,False
terminal-bench,"['reasoning', 'code']",2025-07-28T00:00:00.000000+00:00,benchmark_definition,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",,en,1.0,text,False,Terminal-Bench,,,2025-07-28T00:00:00.000000+00:00,False
terminus,"['reasoning', 'code']",2025-07-19T19:56:12.355994+00:00,benchmark_definition,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments, evaluating how well agents can handle real-world, end-to-end tasks autonomously. The benchmark includes tasks spanning coding, system administration, security, data science, model training, file operations, version control, and web development. Terminus is the neutral test-bed agent designed to work with Terminal-Bench, operating purely through tmux sessions without dedicated tools.",,en,1.0,text,False,Terminus,https://github.com/laude-institute/terminal-bench,,2025-07-19T19:56:12.355994+00:00,False
textvqa,"['vision', 'multimodal', 'image-to-text']",2025-07-19T19:56:12.875287+00:00,benchmark_definition,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",,en,1.0,multimodal,False,TextVQA,https://arxiv.org/abs/1904.08920,,2025-07-19T19:56:12.875287+00:00,False
theoremqa,"['math', 'reasoning', 'physics', 'finance']",2025-07-19T19:56:14.479157+00:00,benchmark_definition,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",,en,1.0,text,False,TheoremQA,https://arxiv.org/abs/2305.12524,,2025-07-19T19:56:14.479157+00:00,False
tldr9+-(test),"['summarization', 'language']",2025-07-19T19:56:14.439927+00:00,benchmark_definition,"A large-scale summarization dataset containing over 9 million training instances extracted from Reddit, designed for extreme summarization (generating one-sentence summaries with high compression and abstraction). More than twice larger than previously proposed datasets.",,en,1.0,text,False,TLDR9+ (test),https://arxiv.org/abs/2110.01159,,2025-07-19T19:56:14.439927+00:00,False
translation-enset1-comet22,['language'],2025-07-19T19:56:12.959436+00:00,benchmark_definition,COMET-22 is an ensemble machine translation evaluation metric combining a COMET estimator model trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It demonstrates improved correlations compared to state-of-the-art metrics and increased robustness to critical errors.,,en,1.0,text,True,Translation enSet1 COMET22,https://aclanthology.org/2022.wmt-1.52/,,2025-07-19T19:56:12.959436+00:00,False
translation-enset1-spbleu,['language'],2025-07-19T19:56:12.936891+00:00,benchmark_definition,"Translation evaluation using spBLEU (SentencePiece BLEU), a BLEU metric computed over text tokenized with a language-agnostic SentencePiece subword model. Introduced in the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",,en,1.0,text,True,Translation enSet1 spBleu,https://arxiv.org/abs/2106.03193,,2025-07-19T19:56:12.936891+00:00,False
translation-set1en-comet22,['language'],2025-07-19T19:56:12.974744+00:00,benchmark_definition,COMET-22 is a neural machine translation evaluation metric that uses an ensemble of two models: a COMET estimator trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It provides improved correlations with human judgments and increased robustness to critical errors compared to previous metrics.,,en,1.0,text,True,Translation Set1en COMET22,https://aclanthology.org/2022.wmt-1.52/,,2025-07-19T19:56:12.974744+00:00,False
translation-set1en-spbleu,['language'],2025-07-19T19:56:12.967240+00:00,benchmark_definition,"spBLEU (SentencePiece BLEU) evaluation metric for machine translation quality assessment, using language-agnostic SentencePiece tokenization with BLEU scoring. Part of the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",,en,1.0,text,True,Translation Set1en spBleu,https://arxiv.org/abs/2106.03193,,2025-07-19T19:56:12.967240+00:00,False
triviaqa,"['general', 'reasoning']",2025-07-19T19:56:11.563587+00:00,benchmark_definition,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",,en,1.0,text,False,TriviaQA,https://arxiv.org/abs/1705.03551,,2025-07-19T19:56:11.563587+00:00,False
truthfulqa,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",2025-07-19T19:56:11.339268+00:00,benchmark_definition,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",,en,1.0,text,False,TruthfulQA,https://arxiv.org/abs/2109.07958,,2025-07-19T19:56:11.339268+00:00,False
tydiqa,"['language', 'reasoning']",2025-07-19T19:56:14.470500+00:00,benchmark_definition,A multilingual question answering benchmark covering 11 typologically diverse languages with 204K question-answer pairs. Questions are written by people seeking genuine information and data is collected directly in each language without translation to test model generalization across diverse linguistic structures.,,en,1.0,text,True,TydiQA,https://arxiv.org/abs/2003.05002,,2025-07-19T19:56:14.470500+00:00,False
uniform-bar-exam,"['legal', 'reasoning']",2025-07-19T19:56:15.404860+00:00,benchmark_definition,"The Uniform Bar Examination (UBE) benchmark evaluates language models on the complete bar exam including multiple-choice Multistate Bar Examination (MBE), open-ended Multistate Essay Exam (MEE), and Multistate Performance Test (MPT) components. Used to assess legal reasoning capabilities across seven subject areas including Evidence, Torts, Constitutional Law, Contracts, Criminal Law and Procedure, Real Property, and Civil Procedure.",,en,1.0,text,False,Uniform Bar Exam,https://royalsocietypublishing.org/doi/10.1098/rsta.2023.0254,,2025-07-19T19:56:15.404860+00:00,False
usamo25,"['math', 'reasoning']",2025-07-19T19:56:15.067604+00:00,benchmark_definition,"The 2025 United States of America Mathematical Olympiad (USAMO) benchmark consists of six challenging mathematical problems requiring rigorous proof-based reasoning. USAMO is the most prestigious high school mathematics competition in the United States, serving as the final round of the American Mathematics Competitions series. This benchmark evaluates models on mathematical problem-solving capabilities beyond simple numerical computation, focusing on formal mathematical reasoning and proof generation.",,en,1.0,text,False,USAMO25,https://arxiv.org/abs/2503.21934,,2025-07-19T19:56:15.067604+00:00,False
vatex,"['multimodal', 'video', 'language']",2025-07-19T19:56:12.909879+00:00,benchmark_definition,"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. Contains over 41,250 videos and 825,000 captions in both English and Chinese, with over 206,000 English-Chinese parallel translation pairs. Supports multilingual video captioning and video-guided machine translation tasks.",,en,1.0,multimodal,True,VATEX,https://arxiv.org/abs/1904.03493,,2025-07-19T19:56:12.909879+00:00,False
vcr-en-easy,"['vision', 'reasoning']",2025-07-19T19:56:14.592175+00:00,benchmark_definition,"Visual Commonsense Reasoning (VCR) benchmark that tests higher-order cognition and commonsense reasoning beyond simple object recognition. Models must answer challenging questions about images and provide rationales justifying their answers. The benchmark measures the ability to infer people's actions, goals, and mental states from visual context.",,en,1.0,multimodal,False,VCR_en_easy,https://arxiv.org/abs/1811.10830,,2025-07-19T19:56:14.592175+00:00,False
vibe-eval,"['multimodal', 'vision', 'general']",2025-07-19T19:56:13.871369+00:00,benchmark_definition,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",,en,1.0,multimodal,False,Vibe-Eval,https://arxiv.org/abs/2405.02287,,2025-07-19T19:56:13.871369+00:00,False
"video-mme-(long,-no-subtitles)","['vision', 'multimodal', 'video']",2025-07-19T19:56:15.374053+00:00,benchmark_definition,"Video-MME is the first-ever comprehensive evaluation benchmark for Multi-modal Large Language Models (MLLMs) in video analysis. This variant focuses on long-term videos (30min-60min) without subtitle inputs, testing robust contextual dynamics across 6 primary visual domains with 30 subfields including knowledge, film & television, sports competition, life record, and multilingual content.",,en,1.0,multimodal,True,"Video-MME (long, no subtitles)",https://arxiv.org/abs/2405.21075,,2025-07-19T19:56:15.374053+00:00,False
video-mme,"['multimodal', 'vision', 'reasoning']",2025-07-19T19:56:13.901883+00:00,benchmark_definition,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",,en,1.0,multimodal,True,Video-MME,https://arxiv.org/abs/2405.21075,,2025-07-19T19:56:13.901883+00:00,False
video-mmew-sub,"['multimodal', 'reasoning', 'vision']",2025-08-03T22:06:11.276310+00:00,benchmark_definition,"Video-MME is the first comprehensive evaluation benchmark for multi-modal large language models in video analysis. It consists of 900 videos (254 hours total) across 6 domains and 30 sub-categories, with 2,700 high-quality multiple-choice questions. The benchmark evaluates MLLMs on diverse video types of varying durations (11 seconds to 1 hour) with multi-modal inputs including video frames, subtitles, and audio to assess perception, reasoning, and temporal understanding capabilities.",,en,1.0,multimodal,True,Video-MMEw sub,https://arxiv.org/abs/2405.21075,,2025-08-03T22:06:11.276310+00:00,False
videomme-w-o-sub.,"['multimodal', 'video', 'vision']",2025-07-19T19:56:14.715184+00:00,benchmark_definition,"Video-MME is a comprehensive evaluation benchmark for multi-modal large language models in video analysis. It features 900 videos across 6 primary visual domains with 30 subfields, ranging from 11 seconds to 1 hour in duration, with 2,700 question-answer pairs. The benchmark evaluates MLLMs' capabilities in processing sequential visual data and multi-modal content including video frames, subtitles, and audio.",,en,1.0,multimodal,False,VideoMME w/o sub.,https://arxiv.org/abs/2405.21075,,2025-07-19T19:56:14.715184+00:00,False
videomme-w-sub.,"['vision', 'multimodal', 'video']",2025-07-19T19:56:14.723259+00:00,benchmark_definition,"The first-ever comprehensive evaluation benchmark of Multi-modal LLMs in Video analysis. Features 900 videos (254 hours) with 2,700 question-answer pairs covering 6 primary visual domains and 30 subfields. Evaluates temporal understanding across short (11 seconds) to long (1 hour) videos with multi-modal inputs including video frames, subtitles, and audio.",,en,1.0,multimodal,False,VideoMME w sub.,https://arxiv.org/abs/2405.21075,,2025-07-19T19:56:14.723259+00:00,False
videommmu,"['multimodal', 'vision', 'reasoning']",2025-07-19T19:56:14.007381+00:00,benchmark_definition,"Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",,en,1.0,multimodal,False,VideoMMMU,https://arxiv.org/abs/2501.13826,,2025-07-19T19:56:14.007381+00:00,False
visualwebbench,"['vision', 'multimodal', 'frontend_development']",2025-07-19T19:56:12.747583+00:00,benchmark_definition,"A multimodal benchmark designed to assess the capabilities of multimodal large language models (MLLMs) across web page understanding and grounding tasks. Comprises 7 tasks (captioning, webpage QA, heading OCR, element OCR, element grounding, action prediction, and action grounding) with 1.5K human-curated instances from 139 real websites across 87 sub-domains.",,en,1.0,multimodal,False,VisualWebBench,https://arxiv.org/abs/2404.05955,,2025-07-19T19:56:12.747583+00:00,False
vocalsound,['audio'],2025-07-19T19:56:14.919198+00:00,benchmark_definition,"A dataset for improving human vocal sounds recognition, containing over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Used for audio event classification and recognition of human non-speech vocalizations.",,en,1.0,audio,False,VocalSound,https://arxiv.org/abs/2205.03433,,2025-07-19T19:56:14.919198+00:00,False
voicebench-avg,"['general', 'reasoning', 'safety', 'communication']",2025-07-19T19:56:14.922519+00:00,benchmark_definition,"VoiceBench is the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants, evaluating capabilities including general knowledge, instruction-following, reasoning, and safety using both synthetic and real spoken instruction data with diverse speaker characteristics and environmental conditions.",,en,1.0,multimodal,False,VoiceBench Avg,https://arxiv.org/abs/2410.17196,,2025-07-19T19:56:14.922519+00:00,False
vqa-rad,"['vision', 'healthcare', 'multimodal']",2025-07-19T19:56:14.031802+00:00,benchmark_definition,"VQA-RAD (Visual Question Answering in Radiology) is the first manually constructed dataset of medical visual question answering containing 3,515 clinically generated visual questions and answers about radiology images. The dataset includes questions created by clinical trainees on 315 radiology images from MedPix covering head, chest, and abdominal scans, designed to support AI development for medical image analysis and improve patient care.",,en,1.0,multimodal,False,VQA-Rad,https://doi.org/10.1038/sdata.2018.251,,2025-07-19T19:56:14.031802+00:00,False
vqav2-(test),"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.430940+00:00,benchmark_definition,"VQA v2.0 (Visual Question Answering v2.0) is a balanced dataset designed to counter language priors in visual question answering. It consists of complementary image pairs where the same question yields different answers, forcing models to rely on visual understanding rather than language bias. The dataset contains 1,105,904 questions across 204,721 COCO images, requiring understanding of vision, language, and commonsense knowledge.",,en,1.0,multimodal,False,VQAv2 (test),https://arxiv.org/abs/1612.00837,,2025-07-19T19:56:14.430940+00:00,False
vqav2-(val),"['vision', 'multimodal', 'language', 'reasoning']",2025-07-19T19:56:13.647852+00:00,benchmark_definition,"VQAv2 is a balanced Visual Question Answering dataset containing open-ended questions about images that require understanding of vision, language, and commonsense knowledge to answer. VQAv2 addresses bias issues from the original VQA dataset by collecting complementary images such that every question is associated with similar images that result in different answers, forcing models to actually understand visual content rather than relying on language priors.",,en,1.0,multimodal,False,VQAv2 (val),https://arxiv.org/abs/1612.00837,,2025-07-19T19:56:13.647852+00:00,False
vqav2,"['vision', 'multimodal', 'reasoning']",2025-07-19T19:56:14.410411+00:00,benchmark_definition,"VQAv2 is a balanced Visual Question Answering dataset that addresses language bias by providing complementary images for each question, forcing models to rely on visual understanding rather than language priors. It contains approximately twice the number of image-question pairs compared to the original VQA dataset.",,en,1.0,multimodal,False,VQAv2,https://arxiv.org/abs/1612.00837,,2025-07-19T19:56:14.410411+00:00,False
wild-bench,"['general', 'reasoning']",2025-07-19T19:56:15.122112+00:00,benchmark_definition,"WildBench is an automated evaluation framework that benchmarks large language models using 1,024 challenging, real-world tasks selected from over one million human-chatbot conversation logs. It introduces two evaluation metrics (WB-Reward and WB-Score) that achieve high correlation with human preferences and uses task-specific checklists for systematic evaluation.",,en,1.0,text,False,Wild Bench,https://arxiv.org/abs/2406.04770,,2025-07-19T19:56:15.122112+00:00,False
winogrande,"['reasoning', 'language']",2025-07-19T19:56:11.370408+00:00,benchmark_definition,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",,en,1.0,text,False,Winogrande,https://arxiv.org/abs/1907.10641,,2025-07-19T19:56:11.370408+00:00,False
wmt23,['language'],2025-07-19T19:56:13.934606+00:00,benchmark_definition,"The Eighth Conference on Machine Translation (WMT23) benchmark evaluating machine translation systems across 8 language pairs (14 translation directions) including general, biomedical, literary, and low-resource language translation tasks. Features specialized shared tasks for quality estimation, metrics evaluation, sign language translation, and discourse-level literary translation with professional human assessment.",,en,1.0,text,True,WMT23,https://aclanthology.org/2023.wmt-1.1/,,2025-07-19T19:56:13.934606+00:00,False
wmt24++,['language'],2025-07-19T19:56:13.576712+00:00,benchmark_definition,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",,en,1.0,text,True,WMT24++,https://arxiv.org/abs/2502.12404,,2025-07-19T19:56:13.576712+00:00,False
writingbench,"['writing', 'creativity', 'communication']",2025-08-03T22:06:11.074130+00:00,benchmark_definition,"A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",,en,1.0,text,True,WritingBench,https://arxiv.org/abs/2503.05244,,2025-08-03T22:06:11.074130+00:00,False
xlsum-english,"['summarization', 'language']",2025-07-19T19:56:15.092213+00:00,benchmark_definition,"Large-scale multilingual abstractive summarization dataset comprising 1 million professionally annotated article-summary pairs from BBC, covering 44 languages. XL-Sum is highly abstractive, concise, and of high quality, designed to encourage research on multilingual abstractive summarization tasks.",,en,1.0,text,True,XLSum English,https://arxiv.org/abs/2106.13822,,2025-07-19T19:56:15.092213+00:00,False
xstest,['safety'],2025-07-19T19:56:13.998594+00:00,benchmark_definition,"XSTest is a test suite designed to identify exaggerated safety behaviours in large language models. It comprises 450 prompts: 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models should refuse. The benchmark systematically evaluates whether models refuse to respond to clearly safe prompts due to overly cautious safety mechanisms.",,en,1.0,text,False,XSTest,https://arxiv.org/abs/2308.01263,,2025-07-19T19:56:13.998594+00:00,False
zebralogic,['reasoning'],2025-09-05T00:00:00.000000+00:00,benchmark_definition,"ZebraLogic is an evaluation framework for assessing large language models' logical reasoning capabilities through logic grid puzzles derived from constraint satisfaction problems (CSPs). The benchmark consists of 1,000 programmatically generated puzzles with controllable and quantifiable complexity, revealing a 'curse of complexity' where model accuracy declines significantly as problem complexity grows.",,en,1.0,text,False,ZebraLogic,https://arxiv.org/abs/2502.01100,,2025-09-05T00:00:00.000000+00:00,False
