allow_commercial,analysis_method,announcement_date,available_in_zeroeval,benchmark_id,benchmark_name,categories_model,country,created_at,data_type,description_x,fine_tuned_from_model_id,implementation_link,is_self_reported,knowledge_cutoff,language_model,license_id_x,max_score_model,modality_model,model_benchmark_id,model_family_id,model_id,multilingual_model,multimodal_x,name_x,normalized_score,organization_country,organization_description,organization_id_x,organization_name_x,organization_website,paper_link,param_count_x,parent_benchmark_id,provider_id,release_date_x,score,self_reported_source_link,source_api_ref,source_paper,source_playground,source_repo_link,source_scorecard_blog_link,source_weights_link,training_tokens,updated_at,verification_date,verification_hardware,verification_notes,verification_provider_id,verified,verified_by_llmstats,website,param_count_disclosed_x,param_count_clean_x,training_tokens_disclosed_x,training_tokens_clean_x,score_clean,normalized_score_clean,license_type_x,release_year_x,release_month_x,release_year_month_x,cost_per_million_io_tokens,model_size_category_x,performance_category,name_y,organization_id_y,organization_name_y,param_count_y,param_count_clean_y,param_count_disclosed_y,training_tokens_clean_y,training_tokens_disclosed_y,multimodal_y,license_id_y,license_type_y,release_date_y,release_year_y,release_month_y,release_year_month_y,model_size_category_y,description_y,name,categories_benchmark,modality_benchmark,multilingual_benchmark,max_score_benchmark,language_benchmark,description,performance_ratio,is_general,is_code,is_math,is_reasoning,is_language,is_multimodal,is_safety,is_long_context,is_roleplay,is_agents,is_factuality,is_vision,is_multimodal_model,is_large_model,is_open_source,is_recent_model,is_2025_model,performance_tier,model_family
,3-shot F1 Score,,,drop,DROP,,,2025-07-19T19:56:13.017079+00:00,benchmark_result,,,,True,,,,,,958.0,,claude-3-5-haiku-20241022,,,,0.831,,,anthropic,,,,,,,,0.831,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:13.017079+00:00,,,,,,False,,False,0.0,False,0.0,0.831,0.831,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.831,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.725835+00:00,benchmark_result,,,,True,,,,,,331.0,,claude-3-5-haiku-20241022,,,,0.416,,,anthropic,,,,,,,,0.416,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:11.725835+00:00,,,,,,False,,False,0.0,False,0.0,0.416,0.416,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.416,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.671817+00:00,benchmark_result,,,,True,,,,,,801.0,,claude-3-5-haiku-20241022,,,,0.881,,,anthropic,,,,,,,,0.881,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:12.671817+00:00,,,,,,False,,False,0.0,False,0.0,0.881,0.881,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.881,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.885732+00:00,benchmark_result,,,,True,,,,,,417.0,,claude-3-5-haiku-20241022,,,,0.694,,,anthropic,,,,,,,,0.694,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:11.885732+00:00,,,,,,False,,False,0.0,False,0.0,0.694,0.694,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.694,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,0-shot CoT,,,mgsm,MGSM,,,2025-07-19T19:56:13.705114+00:00,benchmark_result,,,,True,,,,,,1292.0,,claude-3-5-haiku-20241022,,,,0.856,,,anthropic,,,,,,,,0.856,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:13.705114+00:00,,,,,,False,,False,0.0,False,0.0,0.856,0.856,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.856,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.499754+00:00,benchmark_result,,,,True,,,,,,210.0,,claude-3-5-haiku-20241022,,,,0.65,,,anthropic,,,,,,,,0.65,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:11.499754+00:00,,,,,,False,,False,0.0,False,0.0,0.65,0.65,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.65,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,standard,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.836974+00:00,benchmark_result,,,,True,,,,,,1347.0,,claude-3-5-haiku-20241022,,,,0.406,,,anthropic,,,,,,,,0.406,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:13.836974+00:00,,,,,,False,,False,0.0,False,0.0,0.406,0.406,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.406,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,standard,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:14.997081+00:00,benchmark_result,,,,True,,,,,,1771.0,,claude-3-5-haiku-20241022,,,,0.228,,,anthropic,,,,,,,,0.228,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:14.997081+00:00,,,,,,False,,False,0.0,False,0.0,0.228,0.228,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.228,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,standard,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.970473+00:00,benchmark_result,,,,True,,,,,,1757.0,,claude-3-5-haiku-20241022,,,,0.51,,,anthropic,,,,,,,,0.51,https://www.anthropic.com/news/claude-3-5-haiku,,,,,,,,2025-07-19T19:56:14.970473+00:00,,,,,,False,,False,0.0,False,0.0,0.51,0.51,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Haiku is Anthropic's fastest model, delivering advanced coding, tool use, and reasoning capabilities at an accessible price. It excels at user-facing products, specialized sub-agent tasks, and generating personalized experiences from large data volumes. The model is particularly well-suited for code completions, interactive chatbots, data extraction, and real-time content moderation.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.51,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,3-shot CoT,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.259482+00:00,benchmark_result,,,,True,,,,,,1086.0,,claude-3-5-sonnet-20240620,,,,0.931,,,anthropic,,,,,,,,0.931,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:13.259482+00:00,,,,,,False,,False,0.0,False,0.0,0.931,0.931,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.931,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,3-shot F1 Score,,,drop,DROP,,,2025-07-19T19:56:13.021997+00:00,benchmark_result,,,,True,,,,,,961.0,,claude-3-5-sonnet-20240620,,,,0.871,,,anthropic,,,,,,,,0.871,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:13.021997+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.871,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.733246+00:00,benchmark_result,,,,True,,,,,,336.0,,claude-3-5-sonnet-20240620,,,,0.594,,,anthropic,,,,,,,,0.594,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:11.733246+00:00,,,,,,False,,False,0.0,False,0.0,0.594,0.594,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.594,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.107479+00:00,benchmark_result,,,,True,,,,,,1010.0,,claude-3-5-sonnet-20240620,,,,0.964,,,anthropic,,,,,,,,0.964,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:13.107479+00:00,,,,,,False,,False,0.0,False,0.0,0.964,0.964,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.964,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.676235+00:00,benchmark_result,,,,True,,,,,,804.0,,claude-3-5-sonnet-20240620,,,,0.92,,,anthropic,,,,,,,,0.92,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:12.676235+00:00,,,,,,False,,False,0.0,False,0.0,0.92,0.92,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.92,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.891344+00:00,benchmark_result,,,,True,,,,,,420.0,,claude-3-5-sonnet-20240620,,,,0.711,,,anthropic,,,,,,,,0.711,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:11.891344+00:00,,,,,,False,,False,0.0,False,0.0,0.711,0.711,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.711,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT,,,mgsm,MGSM,,,2025-07-19T19:56:13.710814+00:00,benchmark_result,,,,True,,,,,,1295.0,,claude-3-5-sonnet-20240620,,,,0.916,,,anthropic,,,,,,,,0.916,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:13.710814+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,5-shot CoT,,,mmlu,MMLU,,,2025-07-19T19:56:11.300996+00:00,benchmark_result,,,,True,,,,,,107.0,,claude-3-5-sonnet-20240620,,,,0.904,,,anthropic,,,,,,,,0.904,https://www.anthropic.com/news/claude-3-5-sonnet,,,,,,,,2025-07-19T19:56:11.300996+00:00,,,,,,False,,False,0.0,False,0.0,0.904,0.904,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.904,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,5-shot,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.503274+00:00,benchmark_result,,,,True,,,,,,212.0,,claude-3-5-sonnet-20240620,,,,0.761,,,anthropic,,,,,,,,0.761,https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro,,,,,,,,2025-07-19T19:56:11.503274+00:00,,,,,,False,,False,0.0,False,0.0,0.761,0.761,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-06-21,2024.0,6.0,2024-06,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model. It excels in graduate-level reasoning, undergraduate-level knowledge, and coding proficiency, with improved understanding of nuance, humor, and complex instructions.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.761,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,test,,,ai2d,AI2D,,,2025-07-19T19:56:13.643744+00:00,benchmark_result,,,,True,,,,,,1260.0,,claude-3-5-sonnet-20241022,,,,0.947,,,anthropic,,,,,,,,0.947,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:13.643744+00:00,,,,,,False,,False,0.0,False,0.0,0.947,0.947,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.947,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),claude
,3-shot CoT,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.256021+00:00,benchmark_result,,,,True,,,,,,1084.0,,claude-3-5-sonnet-20241022,,,,0.931,,,anthropic,,,,,,,,0.931,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:13.256021+00:00,,,,,,False,,False,0.0,False,0.0,0.931,0.931,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.931,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,"test, relaxed accuracy",,,chartqa,ChartQA,,,2025-07-19T19:56:12.819413+00:00,benchmark_result,,,,True,,,,,,872.0,,claude-3-5-sonnet-20241022,,,,0.908,,,anthropic,,,,,,,,0.908,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:12.819413+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.908,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),claude
,"test, ANLS score",,,docvqa,DocVQA,,,2025-07-19T19:56:12.867423+00:00,benchmark_result,,,,True,,,,,,897.0,,claude-3-5-sonnet-20241022,,,,0.952,,,anthropic,,,,,,,,0.952,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:12.867423+00:00,,,,,,False,,False,0.0,False,0.0,0.952,0.952,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.952,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),claude
,3-shot F1 Score,,,drop,DROP,,,2025-07-19T19:56:13.018623+00:00,benchmark_result,,,,True,,,,,,959.0,,claude-3-5-sonnet-20241022,,,,0.871,,,anthropic,,,,,,,,0.871,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:13.018623+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.871,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Maj@32 5-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.730271+00:00,benchmark_result,,,,True,,,,,,334.0,,claude-3-5-sonnet-20241022,,,,0.672,,,anthropic,,,,,,,,0.672,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:11.730271+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.672,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.104248+00:00,benchmark_result,,,,True,,,,,,1008.0,,claude-3-5-sonnet-20241022,,,,0.964,,,anthropic,,,,,,,,0.964,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:13.104248+00:00,,,,,,False,,False,0.0,False,0.0,0.964,0.964,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.964,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.673295+00:00,benchmark_result,,,,True,,,,,,802.0,,claude-3-5-sonnet-20241022,,,,0.937,,,anthropic,,,,,,,,0.937,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:12.673295+00:00,,,,,,False,,False,0.0,False,0.0,0.937,0.937,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.937,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,standard,,,math,MATH,,,2025-07-19T19:56:11.887521+00:00,benchmark_result,,,,True,,,,,,418.0,,claude-3-5-sonnet-20241022,,,,0.783,,,anthropic,,,,,,,,0.783,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:11.887521+00:00,,,,,,False,,False,0.0,False,0.0,0.783,0.783,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.783,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,testmini,,,mathvista,MathVista,,,2025-07-19T19:56:12.108158+00:00,benchmark_result,,,,True,,,,,,535.0,,claude-3-5-sonnet-20241022,,,,0.677,,,anthropic,,,,,,,,0.677,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:12.108158+00:00,,,,,,False,,False,0.0,False,0.0,0.677,0.677,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.677,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),claude
,0-shot CoT,,,mgsm,MGSM,,,2025-07-19T19:56:13.707042+00:00,benchmark_result,,,,True,,,,,,1293.0,,claude-3-5-sonnet-20241022,,,,0.916,,,anthropic,,,,,,,,0.916,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:13.707042+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,5-shot CoT,,,mmlu,MMLU,,,2025-07-19T19:56:11.298011+00:00,benchmark_result,,,,True,,,,,,105.0,,claude-3-5-sonnet-20241022,,,,0.904,,,anthropic,,,,,,,,0.904,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:11.298011+00:00,,,,,,False,,False,0.0,False,0.0,0.904,0.904,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.904,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,5-shot,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.501331+00:00,benchmark_result,,,,True,,,,,,211.0,,claude-3-5-sonnet-20241022,,,,0.776,,,anthropic,,,,,,,,0.776,https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro,,,,,,,,2025-07-19T19:56:11.501331+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.776,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,validation,,,mmmu,MMMU,,,2025-07-19T19:56:12.201491+00:00,benchmark_result,,,,True,,,,,,584.0,,claude-3-5-sonnet-20241022,,,,0.683,,,anthropic,,,,,,,,0.683,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,,,,,,,2025-07-19T19:56:12.201491+00:00,,,,,,False,,False,0.0,False,0.0,0.683,0.683,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.683,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,standard,,,osworld-extended,OSWorld Extended,,,2025-07-19T19:56:15.117020+00:00,benchmark_result,,,,True,,,,,,1814.0,,claude-3-5-sonnet-20241022,,,,0.22,,,anthropic,,,,,,,,0.22,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:15.117020+00:00,,,,,,False,,False,0.0,False,0.0,0.22,0.22,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",OSWorld Extended,"['general', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"OSWorld is a scalable, real computer environment benchmark for evaluating multimodal agents on open-ended tasks across Ubuntu, Windows, and macOS. It comprises 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows. The benchmark evaluates agents' ability to interact with computer interfaces using screenshots and actions in realistic computing environments.",0.22,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,standard,,,osworld-screenshot-only,OSWorld Screenshot-only,,,2025-07-19T19:56:15.112291+00:00,benchmark_result,,,,True,,,,,,1813.0,,claude-3-5-sonnet-20241022,,,,0.149,,,anthropic,,,,,,,,0.149,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:15.112291+00:00,,,,,,False,,False,0.0,False,0.0,0.149,0.149,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",OSWorld Screenshot-only,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"OSWorld Screenshot-only: A variant of the OSWorld benchmark that evaluates multimodal AI agents using only screenshot observations to complete open-ended computer tasks across real operating systems (Ubuntu, Windows, macOS). Tests agents' ability to perform complex workflows involving web apps, desktop applications, file I/O, and multi-application tasks through visual interface understanding and GUI grounding.",0.149,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),claude
,standard,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.842061+00:00,benchmark_result,,,,True,,,,,,1350.0,,claude-3-5-sonnet-20241022,,,,0.49,,,anthropic,,,,,,,,0.49,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:13.842061+00:00,,,,,,False,,False,0.0,False,0.0,0.49,0.49,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.49,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,standard,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.003886+00:00,benchmark_result,,,,True,,,,,,1774.0,,claude-3-5-sonnet-20241022,,,,0.46,,,anthropic,,,,,,,,0.46,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:15.003886+00:00,,,,,,False,,False,0.0,False,0.0,0.46,0.46,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.46,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,standard,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.975456+00:00,benchmark_result,,,,True,,,,,,1760.0,,claude-3-5-sonnet-20241022,,,,0.692,,,anthropic,,,,,,,,0.692,https://www.anthropic.com/news/3-5-models-and-computer-use,,,,,,,,2025-07-19T19:56:14.975456+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3.5 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-10-22,2024.0,10.0,2024-10,Undisclosed,"Claude 3.5 Sonnet is a powerful AI model with industry-leading software engineering skills. It excels in coding, planning, and problem-solving, with significant improvements in agentic coding and tool use tasks. The model includes computer use capabilities in public beta, allowing it to interact with computer interfaces like a human user.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.692,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.007831+00:00,benchmark_result,,,,True,,,,,,478.0,,claude-3-7-sonnet-20250219,,,,0.8,,,anthropic,,,,,,,,0.8,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:12.007831+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.8,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"Parallel test-time compute (footnotes 4, 5)",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.464908+00:00,benchmark_result,,,,True,,,,,,700.0,,claude-3-7-sonnet-20250219,,,,0.548,,,anthropic,,,,,,,,0.548,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.464908+00:00,,,,,,False,,False,0.0,False,0.0,0.548,0.548,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.548,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.727330+00:00,benchmark_result,,,,True,,,,,,332.0,,claude-3-7-sonnet-20250219,,,,0.848,,,anthropic,,,,,,,,0.848,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:11.727330+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.848,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,,,,ifeval,IFEval,,,2025-07-19T19:56:12.294010+00:00,benchmark_result,,,,True,,,,,,629.0,,claude-3-7-sonnet-20250219,,,,0.932,,,anthropic,,,,,,,,0.932,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:12.294010+00:00,,,,,,False,,False,0.0,False,0.0,0.932,0.932,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.932,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,,,,math-500,MATH-500,,,2025-07-19T19:56:12.063685+00:00,benchmark_result,,,,True,,,,,,512.0,,claude-3-7-sonnet-20250219,,,,0.962,,,anthropic,,,,,,,,0.962,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:12.063685+00:00,,,,,,False,,False,0.0,False,0.0,0.962,0.962,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.962,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,Average over 14 non-English languages (footnote 3),,,mmmlu,MMMLU,,,2025-07-19T19:56:14.152773+00:00,benchmark_result,,,,True,,,,,,1478.0,,claude-3-7-sonnet-20250219,,,,0.861,,,anthropic,,,,,,,,0.861,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:14.152773+00:00,,,,,,False,,False,0.0,False,0.0,0.861,0.861,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.861,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,validation,,,mmmu,MMMU,,,2025-07-19T19:56:12.197283+00:00,benchmark_result,,,,True,,,,,,582.0,,claude-3-7-sonnet-20250219,,,,0.75,,,anthropic,,,,,,,,0.75,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:12.197283+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.75,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,With multiple parallel attempts and advanced scaffolding,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.838599+00:00,benchmark_result,,,,True,,,,,,1348.0,,claude-3-7-sonnet-20250219,,,,0.703,,,anthropic,,,,,,,,0.703,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:13.838599+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.703,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,With prompt addendum to better utilize planning,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:14.999875+00:00,benchmark_result,,,,True,,,,,,1772.0,,claude-3-7-sonnet-20250219,,,,0.584,,,anthropic,,,,,,,,0.584,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:14.999875+00:00,,,,,,False,,False,0.0,False,0.0,0.584,0.584,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.584,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,With prompt addendum to better utilize planning,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.971988+00:00,benchmark_result,,,,True,,,,,,1758.0,,claude-3-7-sonnet-20250219,,,,0.812,,,anthropic,,,,,,,,0.812,https://www.anthropic.com/news/claude-3-7-sonnet,,,,,,,,2025-07-19T19:56:14.971988+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.812,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"Parallel test-time compute, Claude Code agent framework (footnotes 2, 5)",,,terminal-bench,Terminal-bench,,,2025-07-19T19:56:12.350298+00:00,benchmark_result,,,,True,,,,,,653.0,,claude-3-7-sonnet-20250219,,,,0.352,,,anthropic,,,,,,,,0.352,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.350298+00:00,,,,,,False,,False,0.0,False,0.0,0.352,0.352,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3.7 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-24,2025.0,2.0,2025-02,Undisclosed,"The most intelligent Claude model and the first hybrid reasoning model on the market. Claude 3.7 Sonnet can produce near-instant responses or extended, step-by-step thinking that is made visible to the user. Shows particularly strong improvements in coding and front-end web development.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.352,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.137830+00:00,benchmark_result,,,,True,,,,,,27.0,,claude-3-haiku-20240307,,,,0.892,,,anthropic,,,,,,,,0.892,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:11.137830+00:00,,,,,,False,,False,0.0,False,0.0,0.892,0.892,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.892,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,3-shot CoT,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.257814+00:00,benchmark_result,,,,True,,,,,,1085.0,,claude-3-haiku-20240307,,,,0.737,,,anthropic,,,,,,,,0.737,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:13.257814+00:00,,,,,,False,,False,0.0,False,0.0,0.737,0.737,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.737,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"3-shot, F1 score",,,drop,DROP,,,2025-07-19T19:56:13.020609+00:00,benchmark_result,,,,True,,,,,,960.0,,claude-3-haiku-20240307,,,,0.784,,,anthropic,,,,,,,,0.784,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:13.020609+00:00,,,,,,False,,False,0.0,False,0.0,0.784,0.784,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.784,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.731729+00:00,benchmark_result,,,,True,,,,,,335.0,,claude-3-haiku-20240307,,,,0.333,,,anthropic,,,,,,,,0.333,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:11.731729+00:00,,,,,,False,,False,0.0,False,0.0,0.333,0.333,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.333,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.105970+00:00,benchmark_result,,,,True,,,,,,1009.0,,claude-3-haiku-20240307,,,,0.889,,,anthropic,,,,,,,,0.889,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:13.105970+00:00,,,,,,False,,False,0.0,False,0.0,0.889,0.889,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.889,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.195028+00:00,benchmark_result,,,,True,,,,,,53.0,,claude-3-haiku-20240307,,,,0.859,,,anthropic,,,,,,,,0.859,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:11.195028+00:00,,,,,,False,,False,0.0,False,0.0,0.859,0.859,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.859,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.674804+00:00,benchmark_result,,,,True,,,,,,803.0,,claude-3-haiku-20240307,,,,0.759,,,anthropic,,,,,,,,0.759,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:12.674804+00:00,,,,,,False,,False,0.0,False,0.0,0.759,0.759,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.759,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.889123+00:00,benchmark_result,,,,True,,,,,,419.0,,claude-3-haiku-20240307,,,,0.389,,,anthropic,,,,,,,,0.389,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:11.889123+00:00,,,,,,False,,False,0.0,False,0.0,0.389,0.389,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.389,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.709200+00:00,benchmark_result,,,,True,,,,,,1294.0,,claude-3-haiku-20240307,,,,0.751,,,anthropic,,,,,,,,0.751,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:13.709200+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.751,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.299416+00:00,benchmark_result,,,,True,,,,,,106.0,,claude-3-haiku-20240307,,,,0.752,,,anthropic,,,,,,,,0.752,https://www.anthropic.com/news/claude-3-haiku,,,,,,,,2025-07-19T19:56:11.299416+00:00,,,,,,False,,False,0.0,False,0.0,0.752,0.752,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Haiku,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-03-13,2024.0,3.0,2024-03,Undisclosed,"Claude 3 Haiku is the fastest and most compact model in the Claude 3 family, designed for near-instant responsiveness. It excels at answering simple queries and requests with unmatched speed, making it ideal for seamless AI experiences that mimic human interactions.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.752,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.134917+00:00,benchmark_result,,,,True,,,,,,25.0,,claude-3-opus-20240229,,,,0.964,,,anthropic,,,,,,,,0.964,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.134917+00:00,,,,,,False,,False,0.0,False,0.0,0.964,0.964,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.964,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,3-shot CoT,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.252820+00:00,benchmark_result,,,,True,,,,,,1082.0,,claude-3-opus-20240229,,,,0.868,,,anthropic,,,,,,,,0.868,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.252820+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.868,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"3-shot, F1 Score",,,drop,DROP,,,2025-07-19T19:56:13.013702+00:00,benchmark_result,,,,True,,,,,,956.0,,claude-3-opus-20240229,,,,0.831,,,anthropic,,,,,,,,0.831,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.013702+00:00,,,,,,False,,False,0.0,False,0.0,0.831,0.831,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.831,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT - Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.722913+00:00,benchmark_result,,,,True,,,,,,329.0,,claude-3-opus-20240229,,,,0.504,,,anthropic,,,,,,,,0.504,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.722913+00:00,,,,,,False,,False,0.0,False,0.0,0.504,0.504,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.504,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.101310+00:00,benchmark_result,,,,True,,,,,,1006.0,,claude-3-opus-20240229,,,,0.95,,,anthropic,,,,,,,,0.95,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.101310+00:00,,,,,,False,,False,0.0,False,0.0,0.95,0.95,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.95,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.190975+00:00,benchmark_result,,,,True,,,,,,51.0,,claude-3-opus-20240229,,,,0.954,,,anthropic,,,,,,,,0.954,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.190975+00:00,,,,,,False,,False,0.0,False,0.0,0.954,0.954,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.954,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.668395+00:00,benchmark_result,,,,True,,,,,,799.0,,claude-3-opus-20240229,,,,0.849,,,anthropic,,,,,,,,0.849,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:12.668395+00:00,,,,,,False,,False,0.0,False,0.0,0.849,0.849,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.849,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.882261+00:00,benchmark_result,,,,True,,,,,,415.0,,claude-3-opus-20240229,,,,0.601,,,anthropic,,,,,,,,0.601,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.882261+00:00,,,,,,False,,False,0.0,False,0.0,0.601,0.601,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.601,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,0-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.701952+00:00,benchmark_result,,,,True,,,,,,1290.0,,claude-3-opus-20240229,,,,0.907,,,anthropic,,,,,,,,0.907,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.701952+00:00,,,,,,False,,False,0.0,False,0.0,0.907,0.907,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.907,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.294591+00:00,benchmark_result,,,,True,,,,,,103.0,,claude-3-opus-20240229,,,,0.868,,,anthropic,,,,,,,,0.868,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.294591+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.868,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.496438+00:00,benchmark_result,,,,True,,,,,,208.0,,claude-3-opus-20240229,,,,0.685,,,anthropic,,,,,,,,0.685,https://arxiv.org/pdf/2406.01574,,,,,,,,2025-07-19T19:56:11.496438+00:00,,,,,,False,,False,0.0,False,0.0,0.685,0.685,Unknown,,,,,Undisclosed,Fair (60-69%),Claude 3 Opus,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding, showing the outer limits of what's possible with generative AI.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.685,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.136363+00:00,benchmark_result,,,,True,,,,,,26.0,,claude-3-sonnet-20240229,,,,0.932,,,anthropic,,,,,,,,0.932,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.136363+00:00,,,,,,False,,False,0.0,False,0.0,0.932,0.932,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.932,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,3-shot CoT,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.254531+00:00,benchmark_result,,,,True,,,,,,1083.0,,claude-3-sonnet-20240229,,,,0.829,,,anthropic,,,,,,,,0.829,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.254531+00:00,,,,,,False,,False,0.0,False,0.0,0.829,0.829,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.829,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"3-shot, F1 score",,,drop,DROP,,,2025-07-19T19:56:13.015601+00:00,benchmark_result,,,,True,,,,,,957.0,,claude-3-sonnet-20240229,,,,0.789,,,anthropic,,,,,,,,0.789,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.015601+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.789,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT - Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.724379+00:00,benchmark_result,,,,True,,,,,,330.0,,claude-3-sonnet-20240229,,,,0.404,,,anthropic,,,,,,,,0.404,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.724379+00:00,,,,,,False,,False,0.0,False,0.0,0.404,0.404,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.404,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.102758+00:00,benchmark_result,,,,True,,,,,,1007.0,,claude-3-sonnet-20240229,,,,0.923,,,anthropic,,,,,,,,0.923,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.102758+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.923,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),claude
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.193193+00:00,benchmark_result,,,,True,,,,,,52.0,,claude-3-sonnet-20240229,,,,0.89,,,anthropic,,,,,,,,0.89,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.193193+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.89,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.670119+00:00,benchmark_result,,,,True,,,,,,800.0,,claude-3-sonnet-20240229,,,,0.73,,,anthropic,,,,,,,,0.73,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:12.670119+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.73,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.884160+00:00,benchmark_result,,,,True,,,,,,416.0,,claude-3-sonnet-20240229,,,,0.431,,,anthropic,,,,,,,,0.431,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.884160+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.431,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,0-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.703593+00:00,benchmark_result,,,,True,,,,,,1291.0,,claude-3-sonnet-20240229,,,,0.835,,,anthropic,,,,,,,,0.835,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:13.703593+00:00,,,,,,False,,False,0.0,False,0.0,0.835,0.835,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.835,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.296409+00:00,benchmark_result,,,,True,,,,,,104.0,,claude-3-sonnet-20240229,,,,0.79,,,anthropic,,,,,,,,0.79,https://www.anthropic.com/news/claude-3-family,,,,,,,,2025-07-19T19:56:11.296409+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.79,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.498008+00:00,benchmark_result,,,,True,,,,,,209.0,,claude-3-sonnet-20240229,,,,0.568,,,anthropic,,,,,,,,0.568,https://arxiv.org/pdf/2406.01574,,,,,,,,2025-07-19T19:56:11.498008+00:00,,,,,,False,,False,0.0,False,0.0,0.568,0.568,Unknown,,,,,Undisclosed,Poor (<60%),Claude 3 Sonnet,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-02-29,2024.0,2.0,2024-02,Undisclosed,"Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.568,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,Extended thinking (up to 64K tokens). AIME 2025 using nucleus sampling with a top_p of 0.95.,,,aime-2025,AIME 2025,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2008.0,,claude-opus-4-1-20250805,,,,0.78,,,anthropic,,,,,,,,0.78,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.78,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,Diamond: Extended thinking (up to 64K tokens),,,gpqa,GPQA,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2003.0,,claude-opus-4-1-20250805,,,,0.809,,,anthropic,,,,,,,,0.809,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.809,0.809,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.809,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Extended thinking (up to 64K tokens). Average over 14 non-English languages.,,,mmmlu,MMMLU,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2006.0,,claude-opus-4-1-20250805,,,,0.895,,,anthropic,,,,,,,,0.895,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.895,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Extended thinking (up to 64K tokens),,,mmmu-(validation),MMMU (validation),,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2007.0,,claude-opus-4-1-20250805,,,,0.771,,,anthropic,,,,,,,,0.771,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.771,0.771,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",MMMU (validation),"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",0.771,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),claude
,No extended thinking. Simple scaffold with bash tool and file editing tool via string replacements. Scores reported out of full 500 problems.,,,swe-bench-verified,SWE-Bench Verified,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2001.0,,claude-opus-4-1-20250805,,,,0.745,,,anthropic,,,,,,,,0.745,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.745,0.745,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.745,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps from 30 to 100).",,,tau-bench-airline,TAU-bench Airline,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2005.0,,claude-opus-4-1-20250805,,,,0.56,,,anthropic,,,,,,,,0.56,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.56,0.56,Unknown,,,,,Undisclosed,Poor (<60%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.56,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps from 30 to 100).",,,tau-bench-retail,TAU-bench Retail,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2004.0,,claude-opus-4-1-20250805,,,,0.824,,,anthropic,,,,,,,,0.824,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.824,0.824,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.824,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,No extended thinking. Terminus 1 averaged over 5 trials.,,,terminal-bench,Terminal-bench,,,2025-08-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2002.0,,claude-opus-4-1-20250805,,,,0.433,,,anthropic,,,,,,,,0.433,https://www.anthropic.com/news/claude-opus-4-1,,,,,,,,2025-08-05T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.433,0.433,Unknown,,,,,Undisclosed,Poor (<60%),Claude Opus 4.1,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-05,2025.0,8.0,2025-08,Undisclosed,"Claude Opus 4.1 is a hybrid reasoning model that pushes the frontier for coding and AI agents, featuring a 200K context window. It delivers superior performance and precision for real-world coding and agentic tasks, handling complex multi-step problems with rigor and attention to detail. With extended thinking capabilities, it offers instant responses or extended step-by-step thinking visible through user-friendly summaries. It advances state-of-the-art coding performance to 74.5% on SWE-bench Verified, excels at agentic search and research, and produces human-quality content with exceptional writing abilities. It supports 32K output tokens and adapts to specific coding styles while delivering exceptional quality for extensive generation and refactoring projects.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.433,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,"Extended thinking (up to 64K tokens) with parallel test-time compute (multiple attempts, internal scoring model selection). Nucleus sampling (top_p 0.95). Based on footnotes 4, 5 and blog appendix.",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.468994+00:00,benchmark_result,,,,True,,,,,,702.0,,claude-opus-4-20250514,,,,0.755,,,anthropic,,,,,,,,0.755,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.468994+00:00,,,,,,False,,False,0.0,False,0.0,0.755,0.755,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.755,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,accuracy,,,arc-agi-v2,ARC-AGI v2,,,2025-07-19T19:56:13.923803+00:00,benchmark_result,,,,False,,,,,,1388.0,,claude-opus-4-20250514,,,,0.086,,,anthropic,,,,,,,,0.086,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.923803+00:00,,,,,,False,,False,0.0,False,0.0,0.086,0.086,Unknown,,,,,Undisclosed,Poor (<60%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",ARC-AGI v2,"['reasoning', 'vision', 'spatial_reasoning']",multimodal,False,1.0,en,"ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",0.086,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),claude
,"Diamond: Extended thinking (up to 64K tokens) with parallel test-time compute (multiple attempts, internal scoring model selection). Based on footnote 5 and blog appendix.",,,gpqa,GPQA,,,2025-07-19T19:56:11.734764+00:00,benchmark_result,,,,True,,,,,,337.0,,claude-opus-4-20250514,,,,0.796,,,anthropic,,,,,,,,0.796,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:11.734764+00:00,,,,,,False,,False,0.0,False,0.0,0.796,0.796,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.796,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,Extended thinking (up to 64K tokens). Average over 14 non-English languages. Based on blog appendix and footnote 3.,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.155829+00:00,benchmark_result,,,,True,,,,,,1480.0,,claude-opus-4-20250514,,,,0.888,,,anthropic,,,,,,,,0.888,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:14.155829+00:00,,,,,,False,,False,0.0,False,0.0,0.888,0.888,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.888,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Extended thinking (up to 64K tokens). Based on blog appendix.,,,mmmu-(validation),MMMU (validation),,,2025-07-19T19:56:15.120938+00:00,benchmark_result,,,,True,,,,,,1815.0,,claude-opus-4-20250514,,,,0.765,,,anthropic,,,,,,,,0.765,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:15.120938+00:00,,,,,,False,,False,0.0,False,0.0,0.765,0.765,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",MMMU (validation),"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",0.765,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),claude
,"Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Based on footnote 5 and SWE-bench methodology for high compute.",,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.843719+00:00,benchmark_result,,,,True,,,,,,1351.0,,claude-opus-4-20250514,,,,0.725,,,anthropic,,,,,,,,0.725,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:13.843719+00:00,,,,,,False,,False,0.0,False,0.0,0.725,0.725,Unknown,,,,,Undisclosed,Good (70-79%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.725,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps). Based on blog appendix and TAU-bench methodology.",,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.005622+00:00,benchmark_result,,,,True,,,,,,1775.0,,claude-opus-4-20250514,,,,0.596,,,anthropic,,,,,,,,0.596,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:15.005622+00:00,,,,,,False,,False,0.0,False,0.0,0.596,0.596,Unknown,,,,,Undisclosed,Poor (<60%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.596,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps). Based on blog appendix and TAU-bench methodology.",,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.977090+00:00,benchmark_result,,,,True,,,,,,1761.0,,claude-opus-4-20250514,,,,0.814,,,anthropic,,,,,,,,0.814,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:14.977090+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.814,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Claude Code as agent framework. Based on footnotes 2 and 5.",,,terminal-bench,Terminal-bench,,,2025-07-19T19:56:12.354970+00:00,benchmark_result,,,,True,,,,,,655.0,,claude-opus-4-20250514,,,,0.392,,,anthropic,,,,,,,,0.392,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.354970+00:00,,,,,,False,,False,0.0,False,0.0,0.392,0.392,Unknown,,,,,Undisclosed,Poor (<60%),Claude Opus 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Opus 4 is Anthropic's most powerful model and the world's best coding model, part of the Claude 4 family. It delivers sustained performance on complex, long-running tasks and agent workflows. Opus 4 excels at coding, advanced reasoning, and can use tools (like web search) during extended thinking. It supports parallel tool execution and has improved memory capabilities.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.392,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,"Extended thinking (up to 64K tokens) with parallel test-time compute (multiple attempts, internal scoring model selection). Nucleus sampling (top_p 0.95). Based on footnotes 4, 5 and blog appendix.",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,701.0,,claude-sonnet-4-20250514,,,,0.705,,,anthropic,,,,,,,,0.705,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.705,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"Diamond: Extended thinking (up to 64K tokens) with parallel test-time compute (multiple attempts, internal scoring model selection). Based on footnote 5 and blog appendix.",,,gpqa,GPQA,,,2025-07-19T19:56:11.728759+00:00,benchmark_result,,,,True,,,,,,333.0,,claude-sonnet-4-20250514,,,,0.754,,,anthropic,,,,,,,,0.754,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:11.728759+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.754,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,Extended thinking (up to 64K tokens). Average over 14 non-English languages. Based on blog appendix and footnote 3.,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.154357+00:00,benchmark_result,,,,True,,,,,,1479.0,,claude-sonnet-4-20250514,,,,0.865,,,anthropic,,,,,,,,0.865,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:14.154357+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.865,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Extended thinking (up to 64K tokens). Based on blog appendix.,,,mmmu,MMMU,,,2025-07-19T19:56:12.199608+00:00,benchmark_result,,,,True,,,,,,583.0,,claude-sonnet-4-20250514,,,,0.744,,,anthropic,,,,,,,,0.744,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.199608+00:00,,,,,,False,,False,0.0,False,0.0,0.744,0.744,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.744,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Based on footnote 5 and SWE-bench methodology for high compute.",,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.840540+00:00,benchmark_result,,,,True,,,,,,1349.0,,claude-sonnet-4-20250514,,,,0.727,,,anthropic,,,,,,,,0.727,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:13.840540+00:00,,,,,,False,,False,0.0,False,0.0,0.727,0.727,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.727,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps). Based on blog appendix and TAU-bench methodology.",,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.002282+00:00,benchmark_result,,,,True,,,,,,1773.0,,claude-sonnet-4-20250514,,,,0.6,,,anthropic,,,,,,,,0.6,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:15.002282+00:00,,,,,,False,,False,0.0,False,0.0,0.6,0.6,Unknown,,,,,Undisclosed,Fair (60-69%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.6,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),claude
,"Extended thinking with tool use (up to 64K tokens, prompt addendum, increased max steps). Based on blog appendix and TAU-bench methodology.",,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.973668+00:00,benchmark_result,,,,True,,,,,,1759.0,,claude-sonnet-4-20250514,,,,0.805,,,anthropic,,,,,,,,0.805,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:14.973668+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.805,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,"Parallel test-time compute (multiple attempts, internal scoring model selection). No extended thinking. Claude Code as agent framework. Based on footnotes 2 and 5.",,,terminal-bench,Terminal-bench,,,2025-07-19T19:56:12.353338+00:00,benchmark_result,,,,True,,,,,,654.0,,claude-sonnet-4-20250514,,,,0.355,,,anthropic,,,,,,,,0.355,https://www.anthropic.com/news/claude-4,,,,,,,,2025-07-19T19:56:12.353338+00:00,,,,,,False,,False,0.0,False,0.0,0.355,0.355,Unknown,,,,,Undisclosed,Poor (<60%),Claude Sonnet 4,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-22,2025.0,5.0,2025-05,Undisclosed,"Claude Sonnet 4, part of the Claude 4 family, is a significant upgrade to Claude Sonnet 3.7. It excels in coding (72.7% on SWE-bench) and reasoning, responding more precisely to instructions. Sonnet 4 offers an optimal mix of capability and practicality, with enhanced steerability, and supports extended thinking with tool use.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.355,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,High school math competition,,,aime-2025,AIME 2025,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,704.0,,claude-sonnet-4-5-20250929,,,,0.87,,,anthropic,,,,,,,,0.87,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.87,0.87,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.87,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Graduate-level reasoning,,,gpqa,GPQA,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,705.0,,claude-sonnet-4-5-20250929,,,,0.834,,,anthropic,,,,,,,,0.834,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.834,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Multilingual Q&A,,,mmmlu,MMMLU,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,706.0,,claude-sonnet-4-5-20250929,,,,0.891,,,anthropic,,,,,,,,0.891,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.891,0.891,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.891,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Visual reasoning,,,mmmuval,MMMUval,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,710.0,,claude-sonnet-4-5-20250929,,,,0.778,,,anthropic,,,,,,,,0.778,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,MMMUval,"['vision', 'general', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"Validation set for MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning) benchmark, designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning across Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering.",0.778,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),claude
,Computer use,,,osworld,OSWorld,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,703.0,,claude-sonnet-4-5-20250929,,,,0.614,,,anthropic,,,,,,,,0.614,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.614,0.614,Unknown,,,,,Undisclosed,Fair (60-69%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,OSWorld,"['multimodal', 'general', 'vision']",multimodal,False,1.0,en,"OSWorld: The first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across Ubuntu, Windows, and macOS with 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows",0.614,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),claude
,Agentic coding,,,swe-bench-verified-(agentic-coding),SWE-bench Verified (Agentic Coding),,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,701.0,,claude-sonnet-4-5-20250929,,,,0.772,,,anthropic,,,,,,,,0.772,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.772,0.772,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,SWE-bench Verified (Agentic Coding),"['reasoning', 'code']",text,False,1.0,en,"SWE-bench Verified is a human-filtered subset of 500 software engineering problems drawn from real GitHub issues across 12 popular Python repositories. Given a codebase and an issue description, language models are tasked with generating patches that resolve the described problems. This benchmark evaluates AI's real-world agentic coding skills by requiring models to navigate complex codebases, understand software engineering problems, and coordinate changes across multiple functions, classes, and files to fix well-defined issues with clear descriptions.",0.772,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,Agentic tool use,,,tau-bench-airline,TAU-bench Airline,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,708.0,,claude-sonnet-4-5-20250929,,,,0.7,,,anthropic,,,,,,,,0.7,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.7,0.7,Unknown,,,,,Undisclosed,Good (70-79%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.7,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),claude
,Agentic tool use,,,tau-bench-retail,TAU-bench Retail,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,707.0,,claude-sonnet-4-5-20250929,,,,0.862,,,anthropic,,,,,,,,0.862,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.862,0.862,Unknown,,,,,Undisclosed,Very Good (80-89%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.862,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),claude
,Agentic terminal coding,,,terminal-bench,Terminal-Bench,,,2025-09-29T19:56:12.466833+00:00,benchmark_result,,,,True,,,,,,702.0,,claude-sonnet-4-5-20250929,,,,0.5,,,anthropic,,,,,,,,0.5,https://www.anthropic.com/news/claude-sonnet-4-5,,,,,,,,2025-09-29T19:56:12.466833+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),Claude Sonnet 4.5,anthropic,Anthropic,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-09-29,2025.0,9.0,2025-09,Undisclosed,Claude Sonnet 4.5 is the best coding model in the world. It's the strongest model for building complex agents. It’s the best model at using computers. And it shows substantial gains in reasoning and math. Highest intelligence across most tasks with exceptional agent and coding capabilities.,Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.5,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),claude
,pass@1,,,cruxeval-o,CruxEval-O,,,2025-07-19T19:56:15.151317+00:00,benchmark_result,,,,True,,,,,,1823.0,,codestral-22b,,,,0.513,,,mistral,,,,,,,,0.513,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:15.151317+00:00,,,,,,False,,False,0.0,False,0.0,0.513,0.513,Unknown,,,,,Undisclosed,Poor (<60%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",CruxEval-O,['reasoning'],text,False,1.0,en,"CruxEval-O is the output prediction task of the CRUXEval benchmark, designed to evaluate code reasoning, understanding, and execution capabilities. It consists of 800 Python functions (3-13 lines) where models must predict the output given a function and input. The benchmark tests fundamental code execution reasoning abilities and goes beyond simple code generation to assess deeper understanding of program behavior.",0.513,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),codestral
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.685855+00:00,benchmark_result,,,,True,,,,,,809.0,,codestral-22b,,,,0.811,,,mistral,,,,,,,,0.811,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:12.685855+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.811,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),codestral
,pass@1,,,humaneval-average,HumanEval-Average,,,2025-07-19T19:56:15.174206+00:00,benchmark_result,,,,True,,,,,,1827.0,,codestral-22b,,,,0.615,,,mistral,,,,,,,,0.615,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:15.174206+00:00,,,,,,False,,False,0.0,False,0.0,0.615,0.615,Unknown,,,,,Undisclosed,Fair (60-69%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",HumanEval-Average,['reasoning'],text,False,1.0,en,"A variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.615,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),codestral
,pass@1,,,humanevalfim-average,HumanEvalFIM-Average,,,2025-07-19T19:56:15.169908+00:00,benchmark_result,,,,True,,,,,,1826.0,,codestral-22b,,,,0.916,,,mistral,,,,,,,,0.916,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:15.169908+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",HumanEvalFIM-Average,['general'],text,False,1.0,en,"Average evaluation of HumanEval Fill-in-the-Middle benchmark variants (single-line, multi-line, random-span) for assessing code infilling capabilities of language models",0.916,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),codestral
,pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.517772+00:00,benchmark_result,,,,True,,,,,,1196.0,,codestral-22b,,,,0.782,,,mistral,,,,,,,,0.782,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:13.517772+00:00,,,,,,False,,False,0.0,False,0.0,0.782,0.782,Unknown,,,,,Undisclosed,Good (70-79%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00782,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),codestral
,pass@1,,,repobench,RepoBench,,,2025-07-19T19:56:15.155008+00:00,benchmark_result,,,,True,,,,,,1824.0,,codestral-22b,,,,0.34,,,mistral,,,,,,,,0.34,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:15.155008+00:00,,,,,,False,,False,0.0,False,0.0,0.34,0.34,Unknown,,,,,Undisclosed,Poor (<60%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",RepoBench,"['reasoning', 'code']",text,False,1.0,en,"RepoBench is a benchmark for evaluating repository-level code auto-completion systems through three interconnected tasks: RepoBench-R (retrieval of relevant code snippets across files), RepoBench-C (code completion with cross-file and in-file context), and RepoBench-P (pipeline combining retrieval and prediction). Supports Python and Java programming languages and addresses the gap in evaluating real-world, multi-file programming scenarios by providing a more complete comparison of performance in auto-completion systems.",0.34,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),codestral
,pass@1,,,spider,Spider,,,2025-07-19T19:56:15.159626+00:00,benchmark_result,,,,True,,,,,,1825.0,,codestral-22b,,,,0.635,,,mistral,,,,,,,,0.635,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:15.159626+00:00,,,,,,False,,False,0.0,False,0.0,0.635,0.635,Unknown,,,,,Undisclosed,Fair (60-69%),Codestral-22B,mistral,Mistral AI,22200000000.0,22200000000.0,True,0.0,False,False,mnpl_0_1,Restricted/Community,2024-05-29,2024.0,5.0,2024-05,Very Large (>70B),"A 22B parameter code generation model trained on 80+ programming languages including Python, Java, C, C++, JavaScript, and Bash. Supports both instruction-following and fill-in-the-middle (FIM) capabilities for code completion and generation tasks.",Spider,"['language', 'reasoning']",text,False,1.0,en,"A large-scale, complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. Contains 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables, covering 138 different domains. Requires models to generalize to both new SQL queries and new database schemas, making it distinct from previous semantic parsing tasks that use single databases.",0.635,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),codestral
,Standardized Evaluation,,,arc-c,ARC-C,,,2025-07-19T19:56:11.062949+00:00,benchmark_result,,,,True,,,,,,1.0,,command-r-plus-04-2024,,,,0.7099,,,cohere,,,,,,,,0.7099,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.062949+00:00,,,,,,False,,False,0.0,False,0.0,0.7099,0.7099,Unknown,,,,,Undisclosed,Good (70-79%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.7099,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),command
,Standardized Evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:11.401017+00:00,benchmark_result,,,,True,,,,,,157.0,,command-r-plus-04-2024,,,,0.707,,,cohere,,,,,,,,0.707,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.401017+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.707,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),command
,Standardized Evaluation,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.149067+00:00,benchmark_result,,,,True,,,,,,32.0,,command-r-plus-04-2024,,,,0.886,,,cohere,,,,,,,,0.886,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.149067+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.886,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),command
,Standardized Evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.202939+00:00,benchmark_result,,,,True,,,,,,56.0,,command-r-plus-04-2024,,,,0.757,,,cohere,,,,,,,,0.757,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.202939+00:00,,,,,,False,,False,0.0,False,0.0,0.757,0.757,Unknown,,,,,Undisclosed,Good (70-79%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.757,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),command
,Standardized Evaluation,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.341733+00:00,benchmark_result,,,,True,,,,,,131.0,,command-r-plus-04-2024,,,,0.563,,,cohere,,,,,,,,0.563,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.341733+00:00,,,,,,False,,False,0.0,False,0.0,0.563,0.563,Unknown,,,,,Undisclosed,Poor (<60%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.563,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),command
,Standardized Evaluation,,,winogrande,Winogrande,,,2025-07-19T19:56:11.378573+00:00,benchmark_result,,,,True,,,,,,147.0,,command-r-plus-04-2024,,,,0.854,,,cohere,,,,,,,,0.854,https://huggingface.co/CohereForAI/c4ai-command-r-plus,,,,,,,,2025-07-19T19:56:11.378573+00:00,,,,,,False,,False,0.0,False,0.0,0.854,0.854,Unknown,,,,,Undisclosed,Very Good (80-89%),Command R+,cohere,Cohere,104000000000.0,104000000000.0,True,0.0,False,False,cc_by_nc,Other,2024-08-30,2024.0,8.0,2024-08,Very Large (>70B),"C4AI Command R+ is a 104 billion parameter model with advanced capabilities, including Retrieval Augmented Generation (RAG) and multi-step tool use, optimized for multilingual tasks.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.854,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),command
,Thinking mode,,,aider-polyglot,Aider-Polyglot,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9610.0,,deepseek-r1-0528,,,,0.716,,,deepseek,,,,,,,,0.716,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.716,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,"Pass@1, Thinking mode",,,aime-2024,AIME 2024,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9614.0,,deepseek-r1-0528,,,,0.914,,,deepseek,,,,,,,,0.914,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.914,0.914,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.914,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,"Pass@1, Thinking mode",,,aime-2025,AIME 2025,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9615.0,,deepseek-r1-0528,,,,0.875,,,deepseek,,,,,,,,0.875,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.875,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Search agent with pre-defined workflow,,,browsecomp,BrowseComp,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9605.0,,deepseek-r1-0528,,,,0.089,,,deepseek,,,,,,,,0.089,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with pre-defined workflow,,,False,,False,0.0,False,0.0,0.089,0.089,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.089,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Search agent with pre-defined workflow,,,browsecomp-zh,BrowseComp-zh,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9606.0,,deepseek-r1-0528,,,,0.357,,,deepseek,,,,,,,,0.357,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with pre-defined workflow,,,False,,False,0.0,False,0.0,0.357,0.357,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",BrowseComp-zh,"['reasoning', 'search']",text,True,1.0,zh,"A high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web, consisting of 289 multi-hop questions spanning 11 diverse domains including Film & TV, Technology, Medicine, and History. Questions are reverse-engineered from short, objective, and easily verifiable answers, requiring sophisticated reasoning and information reconciliation beyond basic retrieval. The benchmark addresses linguistic, infrastructural, and censorship-related complexities in Chinese web environments.",0.357,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Div1 Rating, Thinking mode",,,codeforces,Codeforces,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9609.0,,deepseek-r1-0528,,,,0.6433,,,deepseek,,,,,,,,0.6433,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.6433,0.6433,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.00021443333333333333,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Pass@1, Thinking mode",,,gpqa,GPQA,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9603.0,,deepseek-r1-0528,,,,0.81,,,deepseek,,,,,,,,0.81,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.81,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Pass@1, Thinking mode",,,hmmt-2025,HMMT 2025,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9616.0,,deepseek-r1-0528,,,,0.794,,,deepseek,,,,,,,,0.794,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.794,0.794,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.794,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,"Pass@1, Thinking mode, text-only subset",,,humanity's-last-exam,Humanity's Last Exam,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9604.0,,deepseek-r1-0528,,,,0.177,,,deepseek,,,,,,,,0.177,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Text-only subset evaluation,,,False,,False,0.0,False,0.0,0.177,0.177,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.177,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Pass@1, 2408-2505, Thinking mode",,,livecodebench,LiveCodeBench,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9608.0,,deepseek-r1-0528,,,,0.733,,,deepseek,,,,,,,,0.733,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.733,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Thinking mode,,,mmlu-pro,MMLU-Pro,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9602.0,,deepseek-r1-0528,,,,0.85,,,deepseek,,,,,,,,0.85,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.85,0.85,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.85,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Thinking mode,,,mmlu-redux,MMLU-Redux,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9601.0,,deepseek-r1-0528,,,,0.934,,,deepseek,,,,,,,,0.934,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.934,0.934,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.934,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Search agent evaluation,,,simpleqa,SimpleQA,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9607.0,,deepseek-r1-0528,,,,0.923,,,deepseek,,,,,,,,0.923,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.923,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Agent mode,,,swe-bench-multilingual,SWE-Bench Multilingual,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9612.0,,deepseek-r1-0528,,,,0.305,,,deepseek,,,,,,,,0.305,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with internal code agent framework,,,False,,False,0.0,False,0.0,0.305,0.305,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",SWE-bench Multilingual,"['reasoning', 'code']",text,True,1.0,en,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",0.305,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Agent mode,,,swe-bench-verified,SWE-Bench Verified,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9611.0,,deepseek-r1-0528,,,,0.446,,,deepseek,,,,,,,,0.446,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with internal code agent framework,,,False,,False,0.0,False,0.0,0.446,0.446,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.446,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Terminus 1 framework,,,terminal-bench,Terminal-Bench,,,2025-05-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9613.0,,deepseek-r1-0528,,,,0.057,,,deepseek,,,,,,,,0.057,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.057,0.057,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-R1-0528,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-05-28,2025.0,5.0,2025-05,Very Large (>70B),"DeepSeek-R1-0528 is the May 28, 2025 version of DeepSeek's reasoning model. It features advanced thinking capabilities and serves as a benchmark comparison for newer models like DeepSeek-V3.1. This model excels in complex reasoning tasks, mathematical problem-solving, and code generation through its thinking mode approach.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.057,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.987242+00:00,benchmark_result,,,,True,,,,,,467.0,,deepseek-r1-distill-llama-70b,,,,0.867,,,deepseek,,,,,,,,0.867,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B,,,,,,,,2025-07-19T19:56:11.989505+00:00,,,,,,False,,False,0.0,False,0.0,0.867,0.867,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Llama 70B,deepseek,DeepSeek,70600000000.0,70600000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.867,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.700874+00:00,benchmark_result,,,,True,,,,,,315.0,,deepseek-r1-distill-llama-70b,,,,0.652,,,deepseek,,,,,,,,0.652,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B,,,,,,,,2025-07-19T19:56:11.700874+00:00,,,,,,False,,False,0.0,False,0.0,0.652,0.652,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek R1 Distill Llama 70B,deepseek,DeepSeek,70600000000.0,70600000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.652,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.386337+00:00,benchmark_result,,,,True,,,,,,1135.0,,deepseek-r1-distill-llama-70b,,,,0.575,,,deepseek,,,,,,,,0.575,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B,,,,,,,,2025-07-19T19:56:13.386337+00:00,,,,,,False,,False,0.0,False,0.0,0.575,0.575,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Llama 70B,deepseek,DeepSeek,70600000000.0,70600000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.575,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.048302+00:00,benchmark_result,,,,True,,,,,,503.0,,deepseek-r1-distill-llama-70b,,,,0.945,,,deepseek,,,,,,,,0.945,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B,,,,,,,,2025-07-19T19:56:12.048302+00:00,,,,,,False,,False,0.0,False,0.0,0.945,0.945,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek R1 Distill Llama 70B,deepseek,DeepSeek,70600000000.0,70600000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.945,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.984093+00:00,benchmark_result,,,,True,,,,,,465.0,,deepseek-r1-distill-llama-8b,,,,0.8,,,deepseek,,,,,,,,0.8,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B,,,,,,,,2025-07-19T19:56:11.985582+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Llama 8B,deepseek,DeepSeek,8030000000.0,8030000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.8,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.699365+00:00,benchmark_result,,,,True,,,,,,314.0,,deepseek-r1-distill-llama-8b,,,,0.49,,,deepseek,,,,,,,,0.49,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B,,,,,,,,2025-07-19T19:56:11.699365+00:00,,,,,,False,,False,0.0,False,0.0,0.49,0.49,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Llama 8B,deepseek,DeepSeek,8030000000.0,8030000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.49,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.384499+00:00,benchmark_result,,,,True,,,,,,1134.0,,deepseek-r1-distill-llama-8b,,,,0.396,,,deepseek,,,,,,,,0.396,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B,,,,,,,,2025-07-19T19:56:13.384499+00:00,,,,,,False,,False,0.0,False,0.0,0.396,0.396,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Llama 8B,deepseek,DeepSeek,8030000000.0,8030000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.396,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.046427+00:00,benchmark_result,,,,True,,,,,,502.0,,deepseek-r1-distill-llama-8b,,,,0.891,,,deepseek,,,,,,,,0.891,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B,,,,,,,,2025-07-19T19:56:12.046427+00:00,,,,,,False,,False,0.0,False,0.0,0.891,0.891,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Llama 8B,deepseek,DeepSeek,8030000000.0,8030000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.891,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.976978+00:00,benchmark_result,,,,True,,,,,,461.0,,deepseek-r1-distill-qwen-1.5b,,,,0.527,,,deepseek,,,,,,,,0.527,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,,,,,,,,2025-07-19T19:56:11.978475+00:00,,,,,,False,,False,0.0,False,0.0,0.527,0.527,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 1.5B,deepseek,DeepSeek,1780000000.0,1780000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.527,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.694071+00:00,benchmark_result,,,,True,,,,,,311.0,,deepseek-r1-distill-qwen-1.5b,,,,0.338,,,deepseek,,,,,,,,0.338,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,,,,,,,,2025-07-19T19:56:11.694071+00:00,,,,,,False,,False,0.0,False,0.0,0.338,0.338,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 1.5B,deepseek,DeepSeek,1780000000.0,1780000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.338,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.362673+00:00,benchmark_result,,,,True,,,,,,1130.0,,deepseek-r1-distill-qwen-1.5b,,,,0.169,,,deepseek,,,,,,,,0.169,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,,,,,,,,2025-07-19T19:56:13.362673+00:00,,,,,,False,,False,0.0,False,0.0,0.169,0.169,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 1.5B,deepseek,DeepSeek,1780000000.0,1780000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.169,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.041592+00:00,benchmark_result,,,,True,,,,,,499.0,,deepseek-r1-distill-qwen-1.5b,,,,0.839,,,deepseek,,,,,,,,0.839,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B,,,,,,,,2025-07-19T19:56:12.041592+00:00,,,,,,False,,False,0.0,False,0.0,0.839,0.839,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Qwen 1.5B,deepseek,DeepSeek,1780000000.0,1780000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.839,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.991646+00:00,benchmark_result,,,,True,,,,,,469.0,,deepseek-r1-distill-qwen-14b,,,,0.8,,,deepseek,,,,,,,,0.8,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,,,,,,,,2025-07-19T19:56:11.993518+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Qwen 14B,deepseek,DeepSeek,14800000000.0,14800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.8,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.702334+00:00,benchmark_result,,,,True,,,,,,316.0,,deepseek-r1-distill-qwen-14b,,,,0.591,,,deepseek,,,,,,,,0.591,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,,,,,,,,2025-07-19T19:56:11.702334+00:00,,,,,,False,,False,0.0,False,0.0,0.591,0.591,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 14B,deepseek,DeepSeek,14800000000.0,14800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.591,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.387993+00:00,benchmark_result,,,,True,,,,,,1136.0,,deepseek-r1-distill-qwen-14b,,,,0.531,,,deepseek,,,,,,,,0.531,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,,,,,,,,2025-07-19T19:56:13.387993+00:00,,,,,,False,,False,0.0,False,0.0,0.531,0.531,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 14B,deepseek,DeepSeek,14800000000.0,14800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.531,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.050287+00:00,benchmark_result,,,,True,,,,,,504.0,,deepseek-r1-distill-qwen-14b,,,,0.939,,,deepseek,,,,,,,,0.939,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B,,,,,,,,2025-07-19T19:56:12.050287+00:00,,,,,,False,,False,0.0,False,0.0,0.939,0.939,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek R1 Distill Qwen 14B,deepseek,DeepSeek,14800000000.0,14800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.939,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.995645+00:00,benchmark_result,,,,True,,,,,,471.0,,deepseek-r1-distill-qwen-32b,,,,0.833,,,deepseek,,,,,,,,0.833,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,,,,,,,,2025-07-19T19:56:11.997517+00:00,,,,,,False,,False,0.0,False,0.0,0.833,0.833,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Qwen 32B,deepseek,DeepSeek,32800000000.0,32800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.833,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.703902+00:00,benchmark_result,,,,True,,,,,,317.0,,deepseek-r1-distill-qwen-32b,,,,0.621,,,deepseek,,,,,,,,0.621,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,,,,,,,,2025-07-19T19:56:11.703902+00:00,,,,,,False,,False,0.0,False,0.0,0.621,0.621,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek R1 Distill Qwen 32B,deepseek,DeepSeek,32800000000.0,32800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.621,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.389729+00:00,benchmark_result,,,,True,,,,,,1137.0,,deepseek-r1-distill-qwen-32b,,,,0.572,,,deepseek,,,,,,,,0.572,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,,,,,,,,2025-07-19T19:56:13.389729+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 32B,deepseek,DeepSeek,32800000000.0,32800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.572,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.051744+00:00,benchmark_result,,,,True,,,,,,505.0,,deepseek-r1-distill-qwen-32b,,,,0.943,,,deepseek,,,,,,,,0.943,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B,,,,,,,,2025-07-19T19:56:12.051744+00:00,,,,,,False,,False,0.0,False,0.0,0.943,0.943,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek R1 Distill Qwen 32B,deepseek,DeepSeek,32800000000.0,32800000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.943,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.973870+00:00,benchmark_result,,,,True,,,,,,459.0,,deepseek-r1-distill-qwen-7b,,,,0.833,,,deepseek,,,,,,,,0.833,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,,,,,,,,2025-07-19T19:56:11.975371+00:00,,,,,,False,,False,0.0,False,0.0,0.833,0.833,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Distill Qwen 7B,deepseek,DeepSeek,7620000000.0,7620000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.833,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,"Diamond, Pass@1",,,gpqa,GPQA,,,2025-07-19T19:56:11.692702+00:00,benchmark_result,,,,True,,,,,,310.0,,deepseek-r1-distill-qwen-7b,,,,0.491,,,deepseek,,,,,,,,0.491,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,,,,,,,,2025-07-19T19:56:11.692702+00:00,,,,,,False,,False,0.0,False,0.0,0.491,0.491,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 7B,deepseek,DeepSeek,7620000000.0,7620000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.491,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.360567+00:00,benchmark_result,,,,True,,,,,,1129.0,,deepseek-r1-distill-qwen-7b,,,,0.376,,,deepseek,,,,,,,,0.376,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,,,,,,,,2025-07-19T19:56:13.360567+00:00,,,,,,False,,False,0.0,False,0.0,0.376,0.376,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Distill Qwen 7B,deepseek,DeepSeek,7620000000.0,7620000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.376,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.039853+00:00,benchmark_result,,,,True,,,,,,498.0,,deepseek-r1-distill-qwen-7b,,,,0.928,,,deepseek,,,,,,,,0.928,https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B,,,,,,,,2025-07-19T19:56:12.039853+00:00,,,,,,False,,False,0.0,False,0.0,0.928,0.928,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek R1 Distill Qwen 7B,deepseek,DeepSeek,7620000000.0,7620000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1 is the first-generation reasoning model built atop DeepSeek-V3 (671B total parameters, 37B activated per token). It incorporates large-scale reinforcement learning (RL) to enhance its chain-of-thought and reasoning capabilities, delivering strong performance in math, code, and multi-step reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.928,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Cons@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.970600+00:00,benchmark_result,,,,True,,,,,,457.0,,deepseek-r1-zero,,,,0.867,,,deepseek,,,,,,,,0.867,https://arxiv.org/abs/2501.12948,,,,,,,,2025-07-19T19:56:11.972162+00:00,,,,,,False,,False,0.0,False,0.0,0.867,0.867,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek R1 Zero,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.867,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Pass@1 Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.691175+00:00,benchmark_result,,,,True,,,,,,309.0,,deepseek-r1-zero,,,,0.733,,,deepseek,,,,,,,,0.733,https://arxiv.org/abs/2501.12948,,,,,,,,2025-07-19T19:56:11.691175+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek R1 Zero,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.733,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.357962+00:00,benchmark_result,,,,True,,,,,,1128.0,,deepseek-r1-zero,,,,0.5,,,deepseek,,,,,,,,0.5,https://arxiv.org/abs/2501.12948,,,,,,,,2025-07-19T19:56:13.357962+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek R1 Zero,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.5,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.038172+00:00,benchmark_result,,,,True,,,,,,497.0,,deepseek-r1-zero,,,,0.959,,,deepseek,,,,,,,,0.959,https://arxiv.org/abs/2501.12948,,,,,,,,2025-07-19T19:56:12.038172+00:00,,,,,,False,,False,0.0,False,0.0,0.959,0.959,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek R1 Zero,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit,Open & Permissive,2025-01-20,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning. With RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors. However, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.959,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),deepseek
,Score,,,aider,Aider,,,2025-07-19T19:56:14.574890+00:00,benchmark_result,,,,True,,,,,,1627.0,,deepseek-v2.5,,,,0.722,,,deepseek,,,,,,,,0.722,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:14.574890+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",Aider,"['reasoning', 'code']",text,False,1.0,en,"Aider is a comprehensive code editing benchmark based on 133 practice exercises from Exercism's Python repository, designed to evaluate AI models' ability to translate natural language coding requests into executable code that passes unit tests. The benchmark measures end-to-end code editing capabilities, including GPT's ability to edit existing code and format code changes for automated saving to local files. The Aider Polyglot variant extends this evaluation across 225 challenging exercises spanning C++, Go, Java, JavaScript, Python, and Rust, making it a standard benchmark for assessing multilingual code editing performance in AI research.",0.722,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Score,,,alignbench,AlignBench,,,2025-07-19T19:56:14.550691+00:00,benchmark_result,,,,True,,,,,,1619.0,,deepseek-v2.5,,,,0.804,,,deepseek,,,,,,,,0.804,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:14.550691+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",AlignBench,"['general', 'language', 'math', 'reasoning', 'roleplay']",text,True,1.0,en,"AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",0.804,True,False,True,True,True,False,False,False,True,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Score,,,alpacaeval-2.0,AlpacaEval 2.0,,,2025-07-19T19:56:15.041535+00:00,benchmark_result,,,,True,,,,,,1790.0,,deepseek-v2.5,,,,0.505,,,deepseek,,,,,,,,0.505,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:15.041535+00:00,,,,,,False,,False,0.0,False,0.0,0.505,0.505,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",AlpacaEval 2.0,"['general', 'creativity', 'reasoning']",text,False,1.0,en,"AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",0.505,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Score,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.104170+00:00,benchmark_result,,,,True,,,,,,1456.0,,deepseek-v2.5,,,,0.762,,,deepseek,,,,,,,,0.762,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:14.104170+00:00,,,,,,False,,False,0.0,False,0.0,0.762,0.762,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.762,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Score,,,bbh,BBH,,,2025-07-19T19:56:13.046694+00:00,benchmark_result,,,,True,,,,,,974.0,,deepseek-v2.5,,,,0.843,,,deepseek,,,,,,,,0.843,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:13.046694+00:00,,,,,,False,,False,0.0,False,0.0,0.843,0.843,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.843,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Score,,,ds-arena-code,DS-Arena-Code,,,2025-07-19T19:56:15.060324+00:00,benchmark_result,,,,True,,,,,,1797.0,,deepseek-v2.5,,,,0.631,,,deepseek,,,,,,,,0.631,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:15.060324+00:00,,,,,,False,,False,0.0,False,0.0,0.631,0.631,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",DS-Arena-Code,['reasoning'],text,False,1.0,en,"Data Science Arena Code benchmark for evaluating LLMs on realistic data science code generation tasks. Tests capabilities in complex data processing, analysis, and programming across popular Python libraries used in data science workflows.",0.631,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,Score,,,ds-fim-eval,DS-FIM-Eval,,,2025-07-19T19:56:15.056487+00:00,benchmark_result,,,,True,,,,,,1796.0,,deepseek-v2.5,,,,0.783,,,deepseek,,,,,,,,0.783,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:15.056487+00:00,,,,,,False,,False,0.0,False,0.0,0.783,0.783,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",DS-FIM-Eval,['general'],text,False,1.0,en,DeepSeek's internal Fill-in-the-Middle evaluation dataset for measuring code completion performance improvements in data science contexts,0.783,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Score,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.091340+00:00,benchmark_result,,,,True,,,,,,1000.0,,deepseek-v2.5,,,,0.951,,,deepseek,,,,,,,,0.951,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:13.091340+00:00,,,,,,False,,False,0.0,False,0.0,0.951,0.951,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.951,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),deepseek
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.656959+00:00,benchmark_result,,,,True,,,,,,792.0,,deepseek-v2.5,,,,0.89,,,deepseek,,,,,,,,0.89,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:12.656959+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.89,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Pass@1,,,humaneval-mul,HumanEval-Mul,,,2025-07-19T19:56:15.037209+00:00,benchmark_result,,,,True,,,,,,1789.0,,deepseek-v2.5,,,,0.738,,,deepseek,,,,,,,,0.738,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:15.037209+00:00,,,,,,False,,False,0.0,False,0.0,0.738,0.738,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",HumanEval-Mul,['reasoning'],text,True,1.0,en,"A multilingual variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.738,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Score,,,livecodebench(01-09),LiveCodeBench(01-09),,,2025-07-19T19:56:15.052983+00:00,benchmark_result,,,,True,,,,,,1795.0,,deepseek-v2.5,,,,0.418,,,deepseek,,,,,,,,0.418,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:15.052983+00:00,,,,,,False,,False,0.0,False,0.0,0.418,0.418,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",LiveCodeBench(01-09),"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.418,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Score,,,math,MATH,,,2025-07-19T19:56:11.874944+00:00,benchmark_result,,,,True,,,,,,411.0,,deepseek-v2.5,,,,0.747,,,deepseek,,,,,,,,0.747,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:11.874944+00:00,,,,,,False,,False,0.0,False,0.0,0.747,0.747,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.747,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Score,,,mmlu,MMLU,,,2025-07-19T19:56:11.277903+00:00,benchmark_result,,,,True,,,,,,94.0,,deepseek-v2.5,,,,0.804,,,deepseek,,,,,,,,0.804,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:11.277903+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.804,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.525856+00:00,benchmark_result,,,,True,,,,,,1608.0,,deepseek-v2.5,,,,0.902,,,deepseek,,,,,,,,0.902,https://www.deepseek.com/,,,,,,,,2025-07-19T19:56:14.525856+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00902,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Score,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.830793+00:00,benchmark_result,,,,True,,,,,,1345.0,,deepseek-v2.5,,,,0.168,,,deepseek,,,,,,,,0.168,https://huggingface.co/deepseek-ai/DeepSeek-V2.5,,,,,,,,2025-07-19T19:56:13.830793+00:00,,,,,,False,,False,0.0,False,0.0,0.168,0.168,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V2.5,deepseek,DeepSeek,236000000000.0,236000000000.0,True,0.0,False,False,deepseek,Restricted/Community,2024-05-08,2024.0,5.0,2024-05,Very Large (>70B),"DeepSeek-V2.5 is an upgraded version that combines DeepSeek-V2-Chat and DeepSeek-Coder-V2-Instruct, integrating general and coding abilities. It better aligns with human preferences and has been optimized in various aspects, including writing and instruction following.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.168,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Accuracy,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.374175+00:00,benchmark_result,,,,True,,,,,,663.0,,deepseek-v3,,,,0.496,,,deepseek,,,,,,,,0.496,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.374175+00:00,,,,,,False,,False,0.0,False,0.0,0.496,0.496,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.496,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Accuracy,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.796886+00:00,benchmark_result,,,,True,,,,,,1330.0,,deepseek-v3,,,,0.797,,,deepseek,,,,,,,,0.797,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:13.796886+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.797,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.980196+00:00,benchmark_result,,,,True,,,,,,463.0,,deepseek-v3,,,,0.392,,,deepseek,,,,,,,,0.392,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.980196+00:00,,,,,,False,,False,0.0,False,0.0,0.392,0.392,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.392,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Exact Match,,,c-eval,C-Eval,,,2025-07-19T19:56:11.928060+00:00,benchmark_result,,,,True,,,,,,438.0,,deepseek-v3,,,,0.865,,,deepseek,,,,,,,,0.865,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.928060+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",C-Eval,"['general', 'reasoning']",text,True,1.0,en,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",0.865,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Exact Match,,,cluewsc,CLUEWSC,,,2025-07-19T19:56:12.237991+00:00,benchmark_result,,,,True,,,,,,600.0,,deepseek-v3,,,,0.909,,,deepseek,,,,,,,,0.909,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.237991+00:00,,,,,,False,,False,0.0,False,0.0,0.909,0.909,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",CLUEWSC,"['language', 'reasoning']",text,True,1.0,en,"CLUEWSC2020 is the Chinese version of the Winograd Schema Challenge, part of the CLUE benchmark. It focuses on pronoun disambiguation and coreference resolution, requiring models to determine which noun a pronoun refers to in a sentence. The dataset contains 1,244 training samples and 304 development samples extracted from contemporary Chinese literature.",0.909,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Pass@1,,,cnmo-2024,CNMO 2024,,,2025-07-19T19:56:12.493124+00:00,benchmark_result,,,,True,,,,,,711.0,,deepseek-v3,,,,0.432,,,deepseek,,,,,,,,0.432,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.493124+00:00,,,,,,False,,False,0.0,False,0.0,0.432,0.432,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",CNMO 2024,['math'],text,False,1.0,en,China Mathematical Olympiad 2024 - A challenging mathematics competition.,0.432,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Correct,,,csimpleqa,CSimpleQA,,,2025-07-19T19:56:11.937598+00:00,benchmark_result,,,,True,,,,,,442.0,,deepseek-v3,,,,0.648,,,deepseek,,,,,,,,0.648,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.937598+00:00,,,,,,False,,False,0.0,False,0.0,0.648,0.648,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",CSimpleQA,"['general', 'language']",text,True,1.0,en,"Chinese SimpleQA is the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions. It contains 3,000 high-quality questions spanning 6 major topics with 99 diverse subtopics, designed to assess Chinese factual knowledge across humanities, science, engineering, culture, and society.",0.648,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,3-shot F1,,,drop,DROP,,,2025-07-19T19:56:13.005931+00:00,benchmark_result,,,,True,,,,,,951.0,,deepseek-v3,,,,0.916,,,deepseek,,,,,,,,0.916,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:13.005931+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Accuracy,,,frames,FRAMES,,,2025-07-19T19:56:14.958906+00:00,benchmark_result,,,,True,,,,,,1753.0,,deepseek-v3,,,,0.733,,,deepseek,,,,,,,,0.733,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:14.958906+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",FRAMES,"['reasoning', 'search']",text,False,1.0,en,"Factuality, Retrieval, And reasoning MEasurement Set - a unified evaluation dataset of 824 challenging multi-hop questions for testing retrieval-augmented generation systems across factuality, retrieval accuracy, and reasoning capabilities, requiring integration of 2-15 Wikipedia articles per question",0.733,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.695757+00:00,benchmark_result,,,,True,,,,,,312.0,,deepseek-v3,,,,0.591,,,deepseek,,,,,,,,0.591,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.695757+00:00,,,,,,False,,False,0.0,False,0.0,0.591,0.591,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.591,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,humaneval-mul,HumanEval-Mul,,,2025-07-19T19:56:15.035409+00:00,benchmark_result,,,,True,,,,,,1788.0,,deepseek-v3,,,,0.826,,,deepseek,,,,,,,,0.826,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:15.035409+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",HumanEval-Mul,['reasoning'],text,True,1.0,en,"A multilingual variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.826,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Prompt Strict,,,ifeval,IFEval,,,2025-07-19T19:56:12.280659+00:00,benchmark_result,,,,True,,,,,,622.0,,deepseek-v3,,,,0.861,,,deepseek,,,,,,,,0.861,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.280659+00:00,,,,,,False,,False,0.0,False,0.0,0.861,0.861,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.861,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.364940+00:00,benchmark_result,,,,True,,,,,,1131.0,,deepseek-v3,,,,0.376,,,deepseek,,,,,,,,0.376,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:13.372242+00:00,,,,,,False,,False,0.0,False,0.0,0.376,0.376,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.376,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Accuracy,,,longbench-v2,LongBench v2,,,2025-07-19T19:56:15.031520+00:00,benchmark_result,,,,True,,,,,,1787.0,,deepseek-v3,,,,0.487,,,deepseek,,,,,,,,0.487,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:15.031520+00:00,,,,,,False,,False,0.0,False,0.0,0.487,0.487,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",LongBench v2,"['long_context', 'reasoning', 'general']",text,True,1.0,en,"LongBench v2 is a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. It consists of 503 challenging multiple-choice questions with contexts ranging from 8k to 2M words across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding.",0.487,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Exact Match,,,math-500,MATH-500,,,2025-07-19T19:56:12.043125+00:00,benchmark_result,,,,True,,,,,,500.0,,deepseek-v3,,,,0.902,,,deepseek,,,,,,,,0.902,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.043125+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.902,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Exact Match,,,mmlu,MMLU,,,2025-07-19T19:56:11.275957+00:00,benchmark_result,,,,True,,,,,,93.0,,deepseek-v3,,,,0.885,,,deepseek,,,,,,,,0.885,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.275957+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.885,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Exact Match,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.485394+00:00,benchmark_result,,,,True,,,,,,202.0,,deepseek-v3,,,,0.759,,,deepseek,,,,,,,,0.759,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.485394+00:00,,,,,,False,,False,0.0,False,0.0,0.759,0.759,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.759,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Exact Match,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.548864+00:00,benchmark_result,,,,True,,,,,,737.0,,deepseek-v3,,,,0.891,,,deepseek,,,,,,,,0.891,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:12.548864+00:00,,,,,,False,,False,0.0,False,0.0,0.891,0.891,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.891,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Correct,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.549943+00:00,benchmark_result,,,,True,,,,,,235.0,,deepseek-v3,,,,0.249,,,deepseek,,,,,,,,0.249,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:11.549943+00:00,,,,,,False,,False,0.0,False,0.0,0.249,0.249,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.249,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Resolved,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.828562+00:00,benchmark_result,,,,True,,,,,,1344.0,,deepseek-v3,,,,0.42,,,deepseek,,,,,,,,0.42,https://github.com/deepseek-ai/DeepSeek-V3,,,,,,,,2025-07-19T19:56:13.828562+00:00,,,,,,False,,False,0.0,False,0.0,0.42,0.42,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.42,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.999879+00:00,benchmark_result,,,,True,,,,,,473.0,,deepseek-v3-0324,,,,0.594,,,deepseek,,,,,,,,0.594,https://api-docs.deepseek.com/news/news250325,,,,,,,,2025-07-19T19:56:11.999879+00:00,,,,,,False,,False,0.0,False,0.0,0.594,0.594,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3 0324,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2025-03-25,2025.0,3.0,2025-03,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.594,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.705537+00:00,benchmark_result,,,,True,,,,,,318.0,,deepseek-v3-0324,,,,0.684,,,deepseek,,,,,,,,0.684,https://api-docs.deepseek.com/news/news250325,,,,,,,,2025-07-19T19:56:11.705537+00:00,,,,,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3 0324,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2025-03-25,2025.0,3.0,2025-03,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.684,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,Pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.392232+00:00,benchmark_result,,,,True,,,,,,1138.0,,deepseek-v3-0324,,,,0.492,,,deepseek,,,,,,,,0.492,https://api-docs.deepseek.com/news/news250325,,,,,,,,2025-07-19T19:56:13.392232+00:00,,,,,,False,,False,0.0,False,0.0,0.492,0.492,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3 0324,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2025-03-25,2025.0,3.0,2025-03,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.492,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1,,,math-500,MATH-500,,,2025-07-19T19:56:12.053333+00:00,benchmark_result,,,,True,,,,,,506.0,,deepseek-v3-0324,,,,0.94,,,deepseek,,,,,,,,0.94,https://api-docs.deepseek.com/news/news250325,,,,,,,,2025-07-19T19:56:12.053333+00:00,,,,,,False,,False,0.0,False,0.0,0.94,0.94,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3 0324,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2025-03-25,2025.0,3.0,2025-03,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.94,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Exact Match,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.488686+00:00,benchmark_result,,,,True,,,,,,204.0,,deepseek-v3-0324,,,,0.812,,,deepseek,,,,,,,,0.812,https://api-docs.deepseek.com/news/news250325,,,,,,,,2025-07-19T19:56:11.488686+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3 0324,deepseek,DeepSeek,671000000000.0,671000000000.0,True,14800000000000.0,True,False,mit_+_model_license_(commercial_use_allowed),Other,2025-03-25,2025.0,3.0,2025-03,Very Large (>70B),"A powerful Mixture-of-Experts (MoE) language model with 671B total parameters (37B activated per token). Features Multi-head Latent Attention (MLA), auxiliary-loss-free load balancing, and multi-token prediction training. Pre-trained on 14.8T tokens with strong performance in reasoning, math, and code tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.812,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Non-Thinking mode,,,aider-polyglot,Aider-Polyglot,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9510.0,,deepseek-v3.1,,,,0.684,,,deepseek,,,,,,,,0.684,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 68.4%, Thinking: 76.3%",,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.684,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,"Pass@1, Non-Thinking mode",,,aime-2024,AIME 2024,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9514.0,,deepseek-v3.1,,,,0.663,,,deepseek,,,,,,,,0.663,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 66.3%, Thinking: 93.1%",,,False,,False,0.0,False,0.0,0.663,0.663,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.663,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,"Pass@1, Non-Thinking mode",,,aime-2025,AIME 2025,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9515.0,,deepseek-v3.1,,,,0.498,,,deepseek,,,,,,,,0.498,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 49.8%, Thinking: 88.4%",,,False,,False,0.0,False,0.0,0.498,0.498,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.498,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Thinking mode with search agent,,,browsecomp,BrowseComp,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9505.0,,deepseek-v3.1,,,,0.3,,,deepseek,,,,,,,,0.3,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Search agent with commercial API + webpage filter + 128K context,,,False,,False,0.0,False,0.0,0.3,0.3,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.3,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Thinking mode with search agent,,,browsecomp-zh,BrowseComp-zh,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9506.0,,deepseek-v3.1,,,,0.492,,,deepseek,,,,,,,,0.492,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Search agent with commercial API + webpage filter + 128K context,,,False,,False,0.0,False,0.0,0.492,0.492,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",BrowseComp-zh,"['reasoning', 'search']",text,True,1.0,zh,"A high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web, consisting of 289 multi-hop questions spanning 11 diverse domains including Film & TV, Technology, Medicine, and History. Questions are reverse-engineered from short, objective, and easily verifiable answers, requiring sophisticated reasoning and information reconciliation beyond basic retrieval. The benchmark addresses linguistic, infrastructural, and censorship-related complexities in Chinese web environments.",0.492,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Div1 Rating, Thinking mode",,,codeforces,Codeforces,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9509.0,,deepseek-v3.1,,,,0.697,,,deepseek,,,,,,,,0.697,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Codeforces Div1 rating in thinking mode,,,False,,False,0.0,False,0.0,0.697,0.697,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.0002323333333333333,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Pass@1, Non-Thinking mode",,,gpqa,GPQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9503.0,,deepseek-v3.1,,,,0.749,,,deepseek,,,,,,,,0.749,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 74.9%, Thinking: 80.1%",,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.749,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,"Pass@1, Non-Thinking mode",,,hmmt-2025,HMMT 2025,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9516.0,,deepseek-v3.1,,,,0.335,,,deepseek,,,,,,,,0.335,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 33.5%, Thinking: 84.2%",,,False,,False,0.0,False,0.0,0.335,0.335,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.335,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Pass@1, Thinking mode, text-only subset",,,humanity's-last-exam,Humanity's Last Exam,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9504.0,,deepseek-v3.1,,,,0.159,,,deepseek,,,,,,,,0.159,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Thinking mode only, text-only subset",,,False,,False,0.0,False,0.0,0.159,0.159,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.159,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Pass@1, 2408-2505, Non-Thinking mode",,,livecodebench,LiveCodeBench,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9508.0,,deepseek-v3.1,,,,0.564,,,deepseek,,,,,,,,0.564,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 56.4%, Thinking: 74.8%",,,False,,False,0.0,False,0.0,0.564,0.564,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.564,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Non-Thinking mode,,,mmlu-pro,MMLU-Pro,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9502.0,,deepseek-v3.1,,,,0.837,,,deepseek,,,,,,,,0.837,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 83.7%, Thinking: 84.8%",,,False,,False,0.0,False,0.0,0.837,0.837,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.837,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Non-Thinking mode,,,mmlu-redux,MMLU-Redux,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9501.0,,deepseek-v3.1,,,,0.918,,,deepseek,,,,,,,,0.918,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"Non-thinking: 91.8%, Thinking: 93.7%",,,False,,False,0.0,False,0.0,0.918,0.918,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.918,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,Thinking mode with search agent,,,simpleqa,SimpleQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9507.0,,deepseek-v3.1,,,,0.934,,,deepseek,,,,,,,,0.934,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Search agent evaluation,,,False,,False,0.0,False,0.0,0.934,0.934,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.934,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),deepseek
,"Agent mode, Non-Thinking",,,swe-bench-multilingual,SWE-Bench Multilingual,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9512.0,,deepseek-v3.1,,,,0.545,,,deepseek,,,,,,,,0.545,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with internal code agent framework,,,False,,False,0.0,False,0.0,0.545,0.545,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",SWE-bench Multilingual,"['reasoning', 'code']",text,True,1.0,en,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",0.545,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,"Agent mode, Non-Thinking",,,swe-bench-verified,SWE-Bench Verified,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9511.0,,deepseek-v3.1,,,,0.66,,,deepseek,,,,,,,,0.66,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Evaluated with internal code agent framework,,,False,,False,0.0,False,0.0,0.66,0.66,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.66,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,"Terminus 1 framework, Non-Thinking",,,terminal-bench,Terminal-Bench,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9513.0,,deepseek-v3.1,,,,0.313,,,deepseek,,,,,,,,0.313,https://huggingface.co/deepseek-ai/DeepSeek-V3.1,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.313,0.313,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.1,deepseek,DeepSeek,671000000000.0,671000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-01-10,2025.0,1.0,2025-01,Very Large (>70B),"DeepSeek-V3.1 is a hybrid model supporting both thinking and non-thinking modes through different chat templates. Built on DeepSeek-V3.1-Base with a two-phase long context extension (32K phase: 630B tokens, 128K phase: 209B tokens), it features 671B total parameters with 37B activated. Key improvements include smarter tool calling through post-training optimization, higher thinking efficiency achieving comparable quality to DeepSeek-R1-0528 while responding more quickly, and UE8M0 FP8 scale data format for model weights and activations. The model excels in both reasoning tasks (thinking mode) and practical applications (non-thinking mode), with particularly strong performance in code agent tasks, math competitions, and search-based problem solving.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.313,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Reasoning Mode (w/o Tool Use),,,aider-polyglot,Aider-Polyglot,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9528.0,,deepseek-v3.2-exp,,,,0.745,,,deepseek,,,,,,,,0.745,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.745,0.745,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.745,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Pass@1 (Reasoning Mode w/o Tool Use),,,aime-2025,AIME 2025,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9525.0,,deepseek-v3.2-exp,,,,0.893,,,deepseek,,,,,,,,0.893,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.893,0.893,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.893,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Agentic Tool Use,,,browsecomp,BrowseComp,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9529.0,,deepseek-v3.2-exp,,,,0.401,,,deepseek,,,,,,,,0.401,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.401,0.401,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.401,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Agentic Tool Use,,,browsecomp-zh,BrowseComp-zh,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9530.0,,deepseek-v3.2-exp,,,,0.479,,,deepseek,,,,,,,,0.479,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.479,0.479,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,BrowseComp-zh,"['reasoning', 'search']",text,True,1.0,zh,"A high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web, consisting of 289 multi-hop questions spanning 11 diverse domains including Film & TV, Technology, Medicine, and History. Questions are reverse-engineered from short, objective, and easily verifiable answers, requiring sophisticated reasoning and information reconciliation beyond basic retrieval. The benchmark addresses linguistic, infrastructural, and censorship-related complexities in Chinese web environments.",0.479,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Div1 rating (Reasoning Mode),,,codeforces,Codeforces,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9527.0,,deepseek-v3.2-exp,,,,0.707,,,deepseek,,,,,,,,0.707,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,Raw rating ≈ 2121; normalized by 3000 max,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.00023566666666666666,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Reasoning Mode (w/o Tool Use),,,gpqa,GPQA,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9522.0,,deepseek-v3.2-exp,,,,0.799,,,deepseek,,,,,,,,0.799,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.799,0.799,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.799,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Pass@1 (Reasoning Mode w/o Tool Use),,,hmmt-2025,HMMT 2025,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9526.0,,deepseek-v3.2-exp,,,,0.836,,,deepseek,,,,,,,,0.836,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.836,0.836,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.836,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Reasoning Mode (w/o Tool Use),,,humanity's-last-exam,Humanity's Last Exam,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9523.0,,deepseek-v3.2-exp,,,,0.198,,,deepseek,,,,,,,,0.198,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,Text-only subset where applicable,,,False,,False,0.0,False,0.0,0.198,0.198,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.198,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Pass@1 (Reasoning Mode w/o Tool Use),,,livecodebench,LiveCodeBench,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9524.0,,deepseek-v3.2-exp,,,,0.741,,,deepseek,,,,,,,,0.741,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.741,0.741,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.741,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),deepseek
,Reasoning Mode (w/o Tool Use),,,mmlu-pro,MMLU-Pro,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9521.0,,deepseek-v3.2-exp,,,,0.85,,,deepseek,,,,,,,,0.85,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.85,0.85,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.85,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),deepseek
,Agentic Tool Use,,,simpleqa,SimpleQA,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9531.0,,deepseek-v3.2-exp,,,,0.971,,,deepseek,,,,,,,,0.971,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.971,0.971,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.971,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),deepseek
,Agentic Tool Use,,,swe-bench-multilingual,SWE-Bench Multilingual,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9533.0,,deepseek-v3.2-exp,,,,0.579,,,deepseek,,,,,,,,0.579,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.579,0.579,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,SWE-bench Multilingual,"['reasoning', 'code']",text,True,1.0,en,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",0.579,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Agentic Tool Use,,,swe-bench-verified,SWE-Bench Verified,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9532.0,,deepseek-v3.2-exp,,,,0.678,,,deepseek,,,,,,,,0.678,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.678,0.678,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.678,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),deepseek
,Agentic Tool Use,,,terminal-bench,Terminal-Bench,,,2025-09-29T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9534.0,,deepseek-v3.2-exp,,,,0.377,,,deepseek,,,,,,,,0.377,https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp,,,,,,,,2025-09-29T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.377,0.377,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek-V3.2-Exp,deepseek,DeepSeek,685000000000.0,685000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-09-29,2025.0,9.0,2025-09,Very Large (>70B),DeepSeek-V3.2-Exp is an experimental iteration introducing DeepSeek Sparse Attention (DSA) to improve long-context training and inference efficiency while keeping output quality on par with V3.1. It explores fine-grained sparse attention for extended sequence processing.,Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.377,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,test,,,ai2d,AI2D,,,2025-07-19T19:56:13.636398+00:00,benchmark_result,,,,True,,,,,,1256.0,,deepseek-vl2,,,,0.814,,,deepseek,,,,,,,,0.814,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.636398+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.814,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,chartqa,ChartQA,,,2025-07-19T19:56:12.812840+00:00,benchmark_result,,,,True,,,,,,868.0,,deepseek-vl2,,,,0.86,,,deepseek,,,,,,,,0.86,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.812840+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.86,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,docvqa,DocVQA,,,2025-07-19T19:56:12.852402+00:00,benchmark_result,,,,True,,,,,,890.0,,deepseek-vl2,,,,0.933,,,deepseek,,,,,,,,0.933,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.852402+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.933,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),deepseek
,test,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.614094+00:00,benchmark_result,,,,True,,,,,,1244.0,,deepseek-vl2,,,,0.781,,,deepseek,,,,,,,,0.781,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.614094+00:00,,,,,,False,,False,0.0,False,0.0,0.781,0.781,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.781,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,testmini,,,mathvista,MathVista,,,2025-07-19T19:56:12.096047+00:00,benchmark_result,,,,True,,,,,,528.0,,deepseek-vl2,,,,0.628,,,deepseek,,,,,,,,0.628,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.096047+00:00,,,,,,False,,False,0.0,False,0.0,0.628,0.628,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.628,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,en test,,,mmbench,MMBench,,,2025-07-19T19:56:14.245378+00:00,benchmark_result,,,,True,,,,,,1513.0,,deepseek-vl2,,,,0.796,,,deepseek,,,,,,,,0.796,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.247008+00:00,,,,,,False,,False,0.0,False,0.0,0.796,0.796,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.796,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,cn test,,,mmbench-v1.1,MMBench-V1.1,,,2025-07-19T19:56:14.873346+00:00,benchmark_result,,,,True,,,,,,1727.0,,deepseek-vl2,,,,0.792,,,deepseek,,,,,,,,0.792,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.873346+00:00,,,,,,False,,False,0.0,False,0.0,0.792,0.792,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench-V1.1,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"Version 1.1 of MMBench, an improved bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",0.792,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,Standard Evaluation,,,mme,MME,,,2025-07-19T19:56:15.025040+00:00,benchmark_result,,,,True,,,,,,1784.0,,deepseek-vl2,,,,0.2253,,,deepseek,,,,,,,,0.2253,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:15.025040+00:00,,,,,,False,,False,0.0,False,0.0,0.2253,0.2253,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MME,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,A comprehensive evaluation benchmark for Multimodal Large Language Models measuring both perception and cognition abilities across 14 subtasks. Features manually designed instruction-answer pairs to avoid data leakage and provides systematic quantitative assessment of MLLM capabilities.,0.2253,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,val,,,mmmu,MMMU,,,2025-07-19T19:56:12.181251+00:00,benchmark_result,,,,True,,,,,,574.0,,deepseek-vl2,,,,0.511,,,deepseek,,,,,,,,0.511,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.181251+00:00,,,,,,False,,False,0.0,False,0.0,0.511,0.511,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.511,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,mmstar,MMStar,,,2025-07-19T19:56:14.669907+00:00,benchmark_result,,,,True,,,,,,1663.0,,deepseek-vl2,,,,0.613,,,deepseek,,,,,,,,0.613,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.669907+00:00,,,,,,False,,False,0.0,False,0.0,0.613,0.613,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.613,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,Standard Evaluation,,,mmt-bench,MMT-Bench,,,2025-07-19T19:56:14.678247+00:00,benchmark_result,,,,True,,,,,,1667.0,,deepseek-vl2,,,,0.636,,,deepseek,,,,,,,,0.636,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.678247+00:00,,,,,,False,,False,0.0,False,0.0,0.636,0.636,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMT-Bench,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMT-Bench is a comprehensive multimodal benchmark for evaluating Large Vision-Language Models towards multitask AGI. It comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",0.636,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,Standard Evaluation,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.320020+00:00,benchmark_result,,,,True,,,,,,1542.0,,deepseek-vl2,,,,0.811,,,deepseek,,,,,,,,0.811,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.320020+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.811,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,Standard Evaluation,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.601290+00:00,benchmark_result,,,,True,,,,,,1635.0,,deepseek-vl2,,,,0.684,,,deepseek,,,,,,,,0.684,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.601290+00:00,,,,,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.684,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,val,,,textvqa,TextVQA,,,2025-07-19T19:56:12.902069+00:00,benchmark_result,,,,True,,,,,,912.0,,deepseek-vl2,,,,0.842,,,deepseek,,,,,,,,0.842,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.902069+00:00,,,,,,False,,False,0.0,False,0.0,0.842,0.842,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2,deepseek,DeepSeek,27000000000.0,27000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.842,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,ai2d,AI2D,,,2025-07-19T19:56:13.640145+00:00,benchmark_result,,,,True,,,,,,1258.0,,deepseek-vl2-small,,,,0.8,,,deepseek,,,,,,,,0.8,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.640145+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.8,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,chartqa,ChartQA,,,2025-07-19T19:56:12.816278+00:00,benchmark_result,,,,True,,,,,,870.0,,deepseek-vl2-small,,,,0.845,,,deepseek,,,,,,,,0.845,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.816278+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.845,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,docvqa,DocVQA,,,2025-07-19T19:56:12.857733+00:00,benchmark_result,,,,True,,,,,,892.0,,deepseek-vl2-small,,,,0.923,,,deepseek,,,,,,,,0.923,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.857733+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.923,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),deepseek
,test,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.617970+00:00,benchmark_result,,,,True,,,,,,1246.0,,deepseek-vl2-small,,,,0.758,,,deepseek,,,,,,,,0.758,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.617970+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.758,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,testmini,,,mathvista,MathVista,,,2025-07-19T19:56:12.100314+00:00,benchmark_result,,,,True,,,,,,530.0,,deepseek-vl2-small,,,,0.607,,,deepseek,,,,,,,,0.607,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.100314+00:00,,,,,,False,,False,0.0,False,0.0,0.607,0.607,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.607,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,en test,,,mmbench,MMBench,,,2025-07-19T19:56:14.252930+00:00,benchmark_result,,,,True,,,,,,1517.0,,deepseek-vl2-small,,,,0.803,,,deepseek,,,,,,,,0.803,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.254459+00:00,,,,,,False,,False,0.0,False,0.0,0.803,0.803,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.803,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,cn test,,,mmbench-v1.1,MMBench-V1.1,,,2025-07-19T19:56:14.876824+00:00,benchmark_result,,,,True,,,,,,1729.0,,deepseek-vl2-small,,,,0.793,,,deepseek,,,,,,,,0.793,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.876824+00:00,,,,,,False,,False,0.0,False,0.0,0.793,0.793,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench-V1.1,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"Version 1.1 of MMBench, an improved bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",0.793,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,Standard Evaluation,,,mme,MME,,,2025-07-19T19:56:15.028315+00:00,benchmark_result,,,,True,,,,,,1786.0,,deepseek-vl2-small,,,,0.2123,,,deepseek,,,,,,,,0.2123,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:15.028315+00:00,,,,,,False,,False,0.0,False,0.0,0.2123,0.2123,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MME,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,A comprehensive evaluation benchmark for Multimodal Large Language Models measuring both perception and cognition abilities across 14 subtasks. Features manually designed instruction-answer pairs to avoid data leakage and provides systematic quantitative assessment of MLLM capabilities.,0.2123,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,val,,,mmmu,MMMU,,,2025-07-19T19:56:12.184966+00:00,benchmark_result,,,,True,,,,,,576.0,,deepseek-vl2-small,,,,0.48,,,deepseek,,,,,,,,0.48,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.184966+00:00,,,,,,False,,False,0.0,False,0.0,0.48,0.48,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.48,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,mmstar,MMStar,,,2025-07-19T19:56:14.672978+00:00,benchmark_result,,,,True,,,,,,1665.0,,deepseek-vl2-small,,,,0.57,,,deepseek,,,,,,,,0.57,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.672978+00:00,,,,,,False,,False,0.0,False,0.0,0.57,0.57,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.57,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,mmt-bench,MMT-Bench,,,2025-07-19T19:56:14.683443+00:00,benchmark_result,,,,True,,,,,,1669.0,,deepseek-vl2-small,,,,0.629,,,deepseek,,,,,,,,0.629,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.683443+00:00,,,,,,False,,False,0.0,False,0.0,0.629,0.629,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMT-Bench,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMT-Bench is a comprehensive multimodal benchmark for evaluating Large Vision-Language Models towards multitask AGI. It comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",0.629,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,Standard Evaluation,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.324965+00:00,benchmark_result,,,,True,,,,,,1544.0,,deepseek-vl2-small,,,,0.834,,,deepseek,,,,,,,,0.834,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.324965+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.834,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,Standard Evaluation,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.604508+00:00,benchmark_result,,,,True,,,,,,1637.0,,deepseek-vl2-small,,,,0.654,,,deepseek,,,,,,,,0.654,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.604508+00:00,,,,,,False,,False,0.0,False,0.0,0.654,0.654,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.654,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,val,,,textvqa,TextVQA,,,2025-07-19T19:56:12.906237+00:00,benchmark_result,,,,True,,,,,,914.0,,deepseek-vl2-small,,,,0.834,,,deepseek,,,,,,,,0.834,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.906237+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Small,deepseek,DeepSeek,16000000000.0,16000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.834,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,ai2d,AI2D,,,2025-07-19T19:56:13.638556+00:00,benchmark_result,,,,True,,,,,,1257.0,,deepseek-vl2-tiny,,,,0.716,,,deepseek,,,,,,,,0.716,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.638556+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.716,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),deepseek
,test,,,chartqa,ChartQA,,,2025-07-19T19:56:12.814592+00:00,benchmark_result,,,,True,,,,,,869.0,,deepseek-vl2-tiny,,,,0.81,,,deepseek,,,,,,,,0.81,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.814592+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.81,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,docvqa,DocVQA,,,2025-07-19T19:56:12.854588+00:00,benchmark_result,,,,True,,,,,,891.0,,deepseek-vl2-tiny,,,,0.889,,,deepseek,,,,,,,,0.889,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.854588+00:00,,,,,,False,,False,0.0,False,0.0,0.889,0.889,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.889,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,test,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.616113+00:00,benchmark_result,,,,True,,,,,,1245.0,,deepseek-vl2-tiny,,,,0.661,,,deepseek,,,,,,,,0.661,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:13.616113+00:00,,,,,,False,,False,0.0,False,0.0,0.661,0.661,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.661,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,testmini,,,mathvista,MathVista,,,2025-07-19T19:56:12.098477+00:00,benchmark_result,,,,True,,,,,,529.0,,deepseek-vl2-tiny,,,,0.536,,,deepseek,,,,,,,,0.536,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.098477+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.536,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,en test,,,mmbench,MMBench,,,2025-07-19T19:56:14.249349+00:00,benchmark_result,,,,True,,,,,,1515.0,,deepseek-vl2-tiny,,,,0.692,,,deepseek,,,,,,,,0.692,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.251060+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.692,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,cn test,,,mmbench-v1.1,MMBench-V1.1,,,2025-07-19T19:56:14.875207+00:00,benchmark_result,,,,True,,,,,,1728.0,,deepseek-vl2-tiny,,,,0.683,,,deepseek,,,,,,,,0.683,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.875207+00:00,,,,,,False,,False,0.0,False,0.0,0.683,0.683,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMBench-V1.1,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"Version 1.1 of MMBench, an improved bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",0.683,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,Standard Evaluation,,,mme,MME,,,2025-07-19T19:56:15.026734+00:00,benchmark_result,,,,True,,,,,,1785.0,,deepseek-vl2-tiny,,,,0.1915,,,deepseek,,,,,,,,0.1915,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:15.026734+00:00,,,,,,False,,False,0.0,False,0.0,0.1915,0.1915,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MME,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,A comprehensive evaluation benchmark for Multimodal Large Language Models measuring both perception and cognition abilities across 14 subtasks. Features manually designed instruction-answer pairs to avoid data leakage and provides systematic quantitative assessment of MLLM capabilities.,0.1915,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,val,,,mmmu,MMMU,,,2025-07-19T19:56:12.183016+00:00,benchmark_result,,,,True,,,,,,575.0,,deepseek-vl2-tiny,,,,0.407,,,deepseek,,,,,,,,0.407,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.183016+00:00,,,,,,False,,False,0.0,False,0.0,0.407,0.407,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.407,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,mmstar,MMStar,,,2025-07-19T19:56:14.671412+00:00,benchmark_result,,,,True,,,,,,1664.0,,deepseek-vl2-tiny,,,,0.459,,,deepseek,,,,,,,,0.459,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.671412+00:00,,,,,,False,,False,0.0,False,0.0,0.459,0.459,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.459,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,mmt-bench,MMT-Bench,,,2025-07-19T19:56:14.681683+00:00,benchmark_result,,,,True,,,,,,1668.0,,deepseek-vl2-tiny,,,,0.532,,,deepseek,,,,,,,,0.532,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.681683+00:00,,,,,,False,,False,0.0,False,0.0,0.532,0.532,Unknown,,,,,Undisclosed,Poor (<60%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",MMT-Bench,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMT-Bench is a comprehensive multimodal benchmark for evaluating Large Vision-Language Models towards multitask AGI. It comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",0.532,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),deepseek
,Standard Evaluation,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.321888+00:00,benchmark_result,,,,True,,,,,,1543.0,,deepseek-vl2-tiny,,,,0.809,,,deepseek,,,,,,,,0.809,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.321888+00:00,,,,,,False,,False,0.0,False,0.0,0.809,0.809,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.809,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,Standard Evaluation,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.602948+00:00,benchmark_result,,,,True,,,,,,1636.0,,deepseek-vl2-tiny,,,,0.642,,,deepseek,,,,,,,,0.642,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:14.602948+00:00,,,,,,False,,False,0.0,False,0.0,0.642,0.642,Unknown,,,,,Undisclosed,Fair (60-69%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.642,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),deepseek
,val,,,textvqa,TextVQA,,,2025-07-19T19:56:12.904238+00:00,benchmark_result,,,,True,,,,,,913.0,,deepseek-vl2-tiny,,,,0.807,,,deepseek,,,,,,,,0.807,https://arxiv.org/pdf/2412.10302,,,,,,,,2025-07-19T19:56:12.904238+00:00,,,,,,False,,False,0.0,False,0.0,0.807,0.807,Unknown,,,,,Undisclosed,Very Good (80-89%),DeepSeek VL2 Tiny,deepseek,DeepSeek,3000000000.0,3000000000.0,True,0.0,False,True,deepseek,Restricted/Community,2024-12-13,2024.0,12.0,2024-12,Very Large (>70B),"An advanced series of large Mixture-of-Experts (MoE) Vision-Language Models that significantly improves upon its predecessor, DeepSeek-VL. DeepSeek-VL2 demonstrates superior capabilities across various tasks, including but not limited to visual question answering, optical character recognition, document/table/chart understanding, and visual grounding.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.807,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),deepseek
,,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.845635+00:00,benchmark_result,,,,True,,,,,,1352.0,,devstral-medium-2507,,,,0.616,,,mistral,,,,,,,,0.616,https://mistral.ai/news/devstral-2507,,,,,,,,2025-07-19T19:56:13.845635+00:00,,,,,,False,,False,0.0,False,0.0,0.616,0.616,Unknown,,,,,Undisclosed,Fair (60-69%),Devstral Medium,mistral,Mistral AI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-07-10,2025.0,7.0,2025-07,Undisclosed,"Devstral Medium builds upon the strengths of Devstral Small and takes performance to the next level with a score of 61.6% on SWE-Bench Verified. Devstral Medium is available through the Mistral public API, and offers exceptional performance at a competitive price point, making it an ideal choice for businesses and developers looking for a high-quality, cost-effective model.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.616,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),devstral
,OpenHands scaffold,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.847228+00:00,benchmark_result,,,,True,,,,,,1353.0,,devstral-small-2507,,,,0.536,,,mistral,,,,,,,,0.536,https://huggingface.co/mistralai/Devstral-Small-2507,,,,,,,,2025-07-19T19:56:13.847228+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),Devstral Small 1.1,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Devstral Small 1.1 (also called devstral-small-2507) is based on the Mistral-Small-3.1 foundation model and contains approximately 24 billion parameters. It supports a 128k token context window, which allows it to handle multi-file code inputs and long prompts typical in software engineering workflows. The model is fine-tuned specifically for structured outputs, including XML and function-calling formats. This makes it compatible with agent frameworks such as OpenHands and suitable for tasks like program navigation, multi-step edits, and code search. It is licensed under Apache 2.0 and available for both research and commercial use.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.536,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),devstral
,Accuracy,,,big-bench,BIG-Bench,,,2025-07-19T19:56:13.928761+00:00,benchmark_result,,,,False,,,,,,1390.0,,gemini-1.0-pro,,,,0.75,,,google,,,,,,,,0.75,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:13.928761+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",BIG-Bench,"['reasoning', 'math', 'language']",text,True,1.0,en,"Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark consisting of 204+ tasks designed to probe large language models and extrapolate their future capabilities. It covers diverse domains including linguistics, mathematics, common-sense reasoning, biology, physics, social bias, software development, and more. The benchmark focuses on tasks believed to be beyond current language model capabilities and includes both English and non-English tasks across multiple languages.",0.75,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.922622+00:00,benchmark_result,,,,True,,,,,,920.0,,gemini-1.0-pro,,,,0.557,,,google,,,,,,,,0.557,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:12.922622+00:00,,,,,,False,,False,0.0,False,0.0,0.557,0.557,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.557,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,fleurs,FLEURS,,,2025-07-19T19:56:13.946039+00:00,benchmark_result,,,,False,,,,,,1397.0,,gemini-1.0-pro,,,,0.064,,,google,,,,,,,,0.064,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:13.946039+00:00,,,,,,False,,False,0.0,False,0.0,0.064,0.064,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",FLEURS,"['language', 'speech-to-text']",audio,True,100.0,en,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",0.00064,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.607534+00:00,benchmark_result,,,,False,,,,,,264.0,,gemini-1.0-pro,,,,0.279,,,google,,,,,,,,0.279,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.607534+00:00,,,,,,False,,False,0.0,False,0.0,0.279,0.279,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.279,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.817378+00:00,benchmark_result,,,,False,,,,,,378.0,,gemini-1.0-pro,,,,0.326,,,google,,,,,,,,0.326,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.817378+00:00,,,,,,False,,False,0.0,False,0.0,0.326,0.326,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.326,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.073663+00:00,benchmark_result,,,,False,,,,,,516.0,,gemini-1.0-pro,,,,0.466,,,google,,,,,,,,0.466,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:12.073663+00:00,,,,,,False,,False,0.0,False,0.0,0.466,0.466,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.466,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.221259+00:00,benchmark_result,,,,True,,,,,,64.0,,gemini-1.0-pro,,,,0.718,,,google,,,,,,,,0.718,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.221259+00:00,,,,,,False,,False,0.0,False,0.0,0.718,0.718,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.718,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.139083+00:00,benchmark_result,,,,False,,,,,,553.0,,gemini-1.0-pro,,,,0.479,,,google,,,,,,,,0.479,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:12.139083+00:00,,,,,,False,,False,0.0,False,0.0,0.479,0.479,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.479,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,wmt23,WMT23,,,2025-07-19T19:56:13.937549+00:00,benchmark_result,,,,False,,,,,,1393.0,,gemini-1.0-pro,,,,0.717,,,google,,,,,,,,0.717,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:13.937549+00:00,,,,,,False,,False,0.0,False,0.0,0.717,0.717,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.0 Pro,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-02-15,2024.0,2.0,2024-02,Undisclosed,"Gemini 1.0 Pro is a Natural Language Processing (NLP) model designed for tasks such as multi-turn text and code chat, and code generation. It supports text input and output, making it ideal for natural language tasks. The model is optimized for handling complex conversations and generating code snippets. It offers adjustable safety settings and supports function calling, but does not support JSON mode, JSON schema, or system instructions. The latest stable version is gemini-1.0-pro-001, and it was last updated in February 2024.",WMT23,['language'],text,True,1.0,en,"The Eighth Conference on Machine Translation (WMT23) benchmark evaluating machine translation systems across 8 language pairs (14 translation directions) including general, biomedical, literary, and low-resource language translation tasks. Features specialized shared tasks for quality estimation, metrics evaluation, sign language translation, and discourse-level literary translation with professional human assessment.",0.717,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy (4-shot),,,amc-2022-23,AMC_2022_23,,,2025-07-19T19:56:13.997413+00:00,benchmark_result,,,,True,,,,,,1417.0,,gemini-1.5-flash,,,,0.348,,,google,,,,,,,,0.348,https://www.maa.org/math-competitions/amc-1012,,,,,,,,2025-07-19T19:56:13.997413+00:00,,,,,,False,,False,0.0,False,0.0,0.348,0.348,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",AMC_2022_23,"['math', 'reasoning']",text,False,1.0,en,"American Mathematics Competition problems from the 2022-23 academic year, consisting of multiple-choice mathematics competition problems designed for high school students. These problems require advanced mathematical reasoning, problem-solving strategies, and mathematical knowledge covering topics like algebra, geometry, number theory, and combinatorics. The benchmark is derived from the official AMC competitions sponsored by the Mathematical Association of America.",0.348,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy (3-shot),,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.235605+00:00,benchmark_result,,,,True,,,,,,1072.0,,gemini-1.5-flash,,,,0.855,,,google,,,,,,,,0.855,https://arxiv.org/abs/2206.04615,,,,,,,,2025-07-19T19:56:13.235605+00:00,,,,,,False,,False,0.0,False,0.0,0.855,0.855,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.855,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Word Error Rate,,,fleurs,FLEURS,,,2025-07-19T19:56:13.949679+00:00,benchmark_result,,,,True,,,,,,1399.0,,gemini-1.5-flash,,,,0.096,,,google,,,,,,,,0.096,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.949679+00:00,,,,,,False,,False,0.0,False,0.0,0.096,0.096,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",FLEURS,"['language', 'speech-to-text']",audio,True,100.0,en,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",0.00096,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy (0-shot),,,functionalmath,FunctionalMATH,,,2025-07-19T19:56:13.991969+00:00,benchmark_result,,,,True,,,,,,1415.0,,gemini-1.5-flash,,,,0.536,,,google,,,,,,,,0.536,https://arxiv.org/abs/2201.04723,,,,,,,,2025-07-19T19:56:13.991969+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",FunctionalMATH,"['math', 'reasoning']",text,False,1.0,en,"A functional variant of the MATH benchmark that tests language models' ability to generalize reasoning patterns across different problem instances, revealing the reasoning gap between static and functional performance.",0.536,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.622361+00:00,benchmark_result,,,,True,,,,,,272.0,,gemini-1.5-flash,,,,0.51,,,google,,,,,,,,0.51,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.622361+00:00,,,,,,False,,False,0.0,False,0.0,0.51,0.51,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.51,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy (11-shot),,,gsm8k,GSM8k,,,2025-07-19T19:56:13.060014+00:00,benchmark_result,,,,True,,,,,,981.0,,gemini-1.5-flash,,,,0.862,,,google,,,,,,,,0.862,https://arxiv.org/abs/2110.14168,,,,,,,,2025-07-19T19:56:13.060014+00:00,,,,,,False,,False,0.0,False,0.0,0.862,0.862,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.862,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy (10-shot),,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.168455+00:00,benchmark_result,,,,True,,,,,,40.0,,gemini-1.5-flash,,,,0.865,,,google,,,,,,,,0.865,https://arxiv.org/abs/1905.07830,,,,,,,,2025-07-19T19:56:11.168455+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.865,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.436585+00:00,benchmark_result,,,,True,,,,,,1158.0,,gemini-1.5-flash,,,,0.472,,,google,,,,,,,,0.472,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.436585+00:00,,,,,,False,,False,0.0,False,0.0,0.472,0.472,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.472,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass Rate,,,humaneval,HumanEval,,,2025-07-19T19:56:12.617215+00:00,benchmark_result,,,,True,,,,,,768.0,,gemini-1.5-flash,,,,0.743,,,google,,,,,,,,0.743,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:12.617215+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.743,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.826586+00:00,benchmark_result,,,,True,,,,,,383.0,,gemini-1.5-flash,,,,0.779,,,google,,,,,,,,0.779,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.826586+00:00,,,,,,False,,False,0.0,False,0.0,0.779,0.779,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.779,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.077492+00:00,benchmark_result,,,,True,,,,,,518.0,,gemini-1.5-flash,,,,0.658,,,google,,,,,,,,0.658,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:12.077492+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.658,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy (8-shot),,,mgsm,MGSM,,,2025-07-19T19:56:13.676395+00:00,benchmark_result,,,,True,,,,,,1276.0,,gemini-1.5-flash,,,,0.826,,,google,,,,,,,,0.826,https://arxiv.org/abs/2305.08916,,,,,,,,2025-07-19T19:56:13.676395+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.826,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.229674+00:00,benchmark_result,,,,True,,,,,,69.0,,gemini-1.5-flash,,,,0.789,,,google,,,,,,,,0.789,https://arxiv.org/abs/2403.05530,,,,,,,,2025-07-19T19:56:11.229674+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.789,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.426986+00:00,benchmark_result,,,,True,,,,,,168.0,,gemini-1.5-flash,,,,0.673,,,google,,,,,,,,0.673,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.426986+00:00,,,,,,False,,False,0.0,False,0.0,0.673,0.673,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.673,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.153019+00:00,benchmark_result,,,,True,,,,,,560.0,,gemini-1.5-flash,,,,0.623,,,google,,,,,,,,0.623,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:12.153019+00:00,,,,,,False,,False,0.0,False,0.0,0.623,0.623,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.623,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,mrcr,MRCR,,,2025-07-19T19:56:13.896456+00:00,benchmark_result,,,,True,,,,,,1376.0,,gemini-1.5-flash,,,,0.719,,,google,,,,,,,,0.719,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.896456+00:00,,,,,,False,,False,0.0,False,0.0,0.719,0.719,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.719,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.525034+00:00,benchmark_result,,,,True,,,,,,1199.0,,gemini-1.5-flash,,,,0.798,,,google,,,,,,,,0.798,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.525034+00:00,,,,,,False,,False,0.0,False,0.0,0.798,0.798,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.798,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy (0-shot),,,physicsfinals,PhysicsFinals,,,2025-07-19T19:56:13.986673+00:00,benchmark_result,,,,True,,,,,,1413.0,,gemini-1.5-flash,,,,0.574,,,google,,,,,,,,0.574,https://arxiv.org/abs/2303.16416,,,,,,,,2025-07-19T19:56:13.986673+00:00,,,,,,False,,False,0.0,False,0.0,0.574,0.574,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",PhysicsFinals,"['physics', 'math', 'reasoning']",text,False,1.0,en,"PHYSICS is a comprehensive benchmark for university-level physics problem solving, containing 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. Even advanced models like o3-mini achieve only 59.9% accuracy.",0.574,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.882991+00:00,benchmark_result,,,,True,,,,,,1369.0,,gemini-1.5-flash,,,,0.489,,,google,,,,,,,,0.489,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.882991+00:00,,,,,,False,,False,0.0,False,0.0,0.489,0.489,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.489,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,video-mme,Video-MME,,,2025-07-19T19:56:13.908485+00:00,benchmark_result,,,,True,,,,,,1381.0,,gemini-1.5-flash,,,,0.761,,,google,,,,,,,,0.761,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.908485+00:00,,,,,,False,,False,0.0,False,0.0,0.761,0.761,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",Video-MME,"['multimodal', 'vision', 'reasoning']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",0.761,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemini
,Score,,,wmt23,WMT23,,,2025-07-19T19:56:13.940965+00:00,benchmark_result,,,,True,,,,,,1395.0,,gemini-1.5-flash,,,,0.741,,,google,,,,,,,,0.741,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.940965+00:00,,,,,,False,,False,0.0,False,0.0,0.741,0.741,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",WMT23,['language'],text,True,1.0,en,"The Eighth Conference on Machine Translation (WMT23) benchmark evaluating machine translation systems across 8 language pairs (14 translation directions) including general, biomedical, literary, and low-resource language translation tasks. Features specialized shared tasks for quality estimation, metrics evaluation, sign language translation, and discourse-level literary translation with professional human assessment.",0.741,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,xstest,XSTest,,,2025-07-19T19:56:14.004109+00:00,benchmark_result,,,,True,,,,,,1419.0,,gemini-1.5-flash,,,,0.97,,,google,,,,,,,,0.97,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:14.004109+00:00,,,,,,False,,False,0.0,False,0.0,0.97,0.97,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 1.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. It supports audio, images, video, and text input, and produces text output. The model is optimized for generating code, extracting data, editing text, and more, making it ideal for narrow, high-frequency tasks.",XSTest,['safety'],text,False,1.0,en,"XSTest is a test suite designed to identify exaggerated safety behaviours in large language models. It comprises 450 prompts: 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models should refuse. The benchmark systematically evaluates whether models refuse to respond to clearly safe prompts due to overly cautious safety mechanisms.",0.97,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gemini
,Speech recognition accuracy (1 - WER),,,fleurs,FLEURS,,,2025-07-19T19:56:13.951665+00:00,benchmark_result,,,,True,,,,,,1400.0,,gemini-1.5-flash-8b,,,,0.864,,,google,,,,,,,,0.864,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.951665+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",FLEURS,"['language', 'speech-to-text']",audio,True,100.0,en,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",0.00864,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy on expert-written science questions,,,gpqa,GPQA,,,2025-07-19T19:56:11.635441+00:00,benchmark_result,,,,True,,,,,,277.0,,gemini-1.5-flash-8b,,,,0.384,,,google,,,,,,,,0.384,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.635441+00:00,,,,,,False,,False,0.0,False,0.0,0.384,0.384,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.384,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy on competition-level math problems,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.447290+00:00,benchmark_result,,,,True,,,,,,1163.0,,gemini-1.5-flash-8b,,,,0.328,,,google,,,,,,,,0.328,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.447290+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.328,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy on mathematical problem-solving tasks,,,math,MATH,,,2025-07-19T19:56:11.834192+00:00,benchmark_result,,,,True,,,,,,387.0,,gemini-1.5-flash-8b,,,,0.587,,,google,,,,,,,,0.587,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.834192+00:00,,,,,,False,,False,0.0,False,0.0,0.587,0.587,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.587,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Visual mathematical reasoning accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.078820+00:00,benchmark_result,,,,True,,,,,,519.0,,gemini-1.5-flash-8b,,,,0.547,,,google,,,,,,,,0.547,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:12.078820+00:00,,,,,,False,,False,0.0,False,0.0,0.547,0.547,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.547,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Multiple choice accuracy across enhanced MMLU dataset with higher difficulty tasks,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.436045+00:00,benchmark_result,,,,True,,,,,,173.0,,gemini-1.5-flash-8b,,,,0.587,,,google,,,,,,,,0.587,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:11.436045+00:00,,,,,,False,,False,0.0,False,0.0,0.587,0.587,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.587,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Multimodal understanding accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.154594+00:00,benchmark_result,,,,True,,,,,,561.0,,gemini-1.5-flash-8b,,,,0.537,,,google,,,,,,,,0.537,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:12.154594+00:00,,,,,,False,,False,0.0,False,0.0,0.537,0.537,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.537,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Long-context comprehension accuracy,,,mrcr,MRCR,,,2025-07-19T19:56:13.898262+00:00,benchmark_result,,,,True,,,,,,1377.0,,gemini-1.5-flash-8b,,,,0.547,,,google,,,,,,,,0.547,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.898262+00:00,,,,,,False,,False,0.0,False,0.0,0.547,0.547,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.547,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass rate on code generation tasks across multiple programming languages,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.531432+00:00,benchmark_result,,,,True,,,,,,1203.0,,gemini-1.5-flash-8b,,,,0.755,,,google,,,,,,,,0.755,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.531432+00:00,,,,,,False,,False,0.0,False,0.0,0.755,0.755,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.755,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Visual understanding evaluation,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.885058+00:00,benchmark_result,,,,True,,,,,,1370.0,,gemini-1.5-flash-8b,,,,0.409,,,google,,,,,,,,0.409,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.885058+00:00,,,,,,False,,False,0.0,False,0.0,0.409,0.409,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.409,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Video analysis accuracy,,,video-mme,Video-MME,,,2025-07-19T19:56:13.910273+00:00,benchmark_result,,,,True,,,,,,1382.0,,gemini-1.5-flash-8b,,,,0.662,,,google,,,,,,,,0.662,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.910273+00:00,,,,,,False,,False,0.0,False,0.0,0.662,0.662,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",Video-MME,"['multimodal', 'vision', 'reasoning']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",0.662,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,Translation quality score,,,wmt23,WMT23,,,2025-07-19T19:56:13.942779+00:00,benchmark_result,,,,True,,,,,,1396.0,,gemini-1.5-flash-8b,,,,0.726,,,google,,,,,,,,0.726,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:13.942779+00:00,,,,,,False,,False,0.0,False,0.0,0.726,0.726,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",WMT23,['language'],text,True,1.0,en,"The Eighth Conference on Machine Translation (WMT23) benchmark evaluating machine translation systems across 8 language pairs (14 translation directions) including general, biomedical, literary, and low-resource language translation tasks. Features specialized shared tasks for quality estimation, metrics evaluation, sign language translation, and discourse-level literary translation with professional human assessment.",0.726,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Safe request fulfillment rate,,,xstest,XSTest,,,2025-07-19T19:56:14.005888+00:00,benchmark_result,,,,True,,,,,,1420.0,,gemini-1.5-flash-8b,,,,0.926,,,google,,,,,,,,0.926,https://deepmind.google/technologies/gemini/flash/,,,,,,,,2025-07-19T19:56:14.005888+00:00,,,,,,False,,False,0.0,False,0.0,0.926,0.926,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 1.5 Flash 8B,google,Google,8000000000.0,8000000000.0,True,0.0,False,True,proprietary,Proprietary,2024-03-15,2024.0,3.0,2024-03,Very Large (>70B),"A multimodal model capable of processing audio, images, video, and text with high efficiency. Features JSON mode, function calling, code execution, and system instructions support. Optimized for fast inference with 8B parameters.",XSTest,['safety'],text,False,1.0,en,"XSTest is a test suite designed to identify exaggerated safety behaviours in large language models. It comprises 450 prompts: 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models should refuse. The benchmark systematically evaluates whether models refuse to respond to clearly safe prompts due to overly cautious safety mechanisms.",0.926,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,4-shot,,,amc-2022-23,AMC_2022_23,,,2025-07-19T19:56:13.995700+00:00,benchmark_result,,,,True,,,,,,1416.0,,gemini-1.5-pro,,,,0.464,,,google,,,,,,,,0.464,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.995700+00:00,,,,,,False,,False,0.0,False,0.0,0.464,0.464,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",AMC_2022_23,"['math', 'reasoning']",text,False,1.0,en,"American Mathematics Competition problems from the 2022-23 academic year, consisting of multiple-choice mathematics competition problems designed for high school students. These problems require advanced mathematical reasoning, problem-solving strategies, and mathematical knowledge covering topics like algebra, geometry, number theory, and combinatorics. The benchmark is derived from the official AMC competitions sponsored by the Mathematical Association of America.",0.464,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,3-shot,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.231702+00:00,benchmark_result,,,,True,,,,,,1070.0,,gemini-1.5-pro,,,,0.892,,,google,,,,,,,,0.892,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.231702+00:00,,,,,,False,,False,0.0,False,0.0,0.892,0.892,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.892,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Variable shots,,,drop,DROP,,,2025-07-19T19:56:12.994980+00:00,benchmark_result,,,,True,,,,,,945.0,,gemini-1.5-pro,,,,0.749,,,google,,,,,,,,0.749,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:12.994980+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.749,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Word Error Rate,,,fleurs,FLEURS,,,2025-07-19T19:56:13.947638+00:00,benchmark_result,,,,True,,,,,,1398.0,,gemini-1.5-pro,,,,0.067,,,google,,,,,,,,0.067,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.947638+00:00,,,,,,False,,False,0.0,False,0.0,0.067,0.067,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",FLEURS,"['language', 'speech-to-text']",audio,True,100.0,en,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",0.00067,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,0-shot,,,functionalmath,FunctionalMATH,,,2025-07-19T19:56:13.990248+00:00,benchmark_result,,,,True,,,,,,1414.0,,gemini-1.5-pro,,,,0.646,,,google,,,,,,,,0.646,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.990248+00:00,,,,,,False,,False,0.0,False,0.0,0.646,0.646,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",FunctionalMATH,"['math', 'reasoning']",text,False,1.0,en,"A functional variant of the MATH benchmark that tests language models' ability to generalize reasoning patterns across different problem instances, revealing the reasoning gap between static and functional performance.",0.646,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.614440+00:00,benchmark_result,,,,True,,,,,,268.0,,gemini-1.5-pro,,,,0.591,,,google,,,,,,,,0.591,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:11.614440+00:00,,,,,,False,,False,0.0,False,0.0,0.591,0.591,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.591,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,11-shot,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.055992+00:00,benchmark_result,,,,True,,,,,,979.0,,gemini-1.5-pro,,,,0.908,,,google,,,,,,,,0.908,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.055992+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.908,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.158919+00:00,benchmark_result,,,,True,,,,,,37.0,,gemini-1.5-pro,,,,0.933,,,google,,,,,,,,0.933,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:11.158919+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.933,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,Accuracy,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.434888+00:00,benchmark_result,,,,True,,,,,,1157.0,,gemini-1.5-pro,,,,0.52,,,google,,,,,,,,0.52,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.434888+00:00,,,,,,False,,False,0.0,False,0.0,0.52,0.52,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.52,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.613548+00:00,benchmark_result,,,,True,,,,,,766.0,,gemini-1.5-pro,,,,0.841,,,google,,,,,,,,0.841,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:12.613548+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.841,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.822515+00:00,benchmark_result,,,,True,,,,,,381.0,,gemini-1.5-pro,,,,0.865,,,google,,,,,,,,0.865,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:11.822515+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.865,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.075702+00:00,benchmark_result,,,,True,,,,,,517.0,,gemini-1.5-pro,,,,0.681,,,google,,,,,,,,0.681,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:12.075702+00:00,,,,,,False,,False,0.0,False,0.0,0.681,0.681,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.681,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,8-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.674684+00:00,benchmark_result,,,,True,,,,,,1275.0,,gemini-1.5-pro,,,,0.875,,,google,,,,,,,,0.875,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.674684+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.875,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.226593+00:00,benchmark_result,,,,True,,,,,,67.0,,gemini-1.5-pro,,,,0.859,,,google,,,,,,,,0.859,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:11.226593+00:00,,,,,,False,,False,0.0,False,0.0,0.859,0.859,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.859,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.425109+00:00,benchmark_result,,,,True,,,,,,167.0,,gemini-1.5-pro,,,,0.758,,,google,,,,,,,,0.758,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:11.425109+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.758,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.145100+00:00,benchmark_result,,,,True,,,,,,556.0,,gemini-1.5-pro,,,,0.659,,,google,,,,,,,,0.659,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:12.145100+00:00,,,,,,False,,False,0.0,False,0.0,0.659,0.659,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.659,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,mrcr,MRCR,,,2025-07-19T19:56:13.891629+00:00,benchmark_result,,,,True,,,,,,1373.0,,gemini-1.5-pro,,,,0.826,,,google,,,,,,,,0.826,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.891629+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.826,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.523328+00:00,benchmark_result,,,,True,,,,,,1198.0,,gemini-1.5-pro,,,,0.854,,,google,,,,,,,,0.854,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.523328+00:00,,,,,,False,,False,0.0,False,0.0,0.854,0.854,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.854,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,0-shot,,,physicsfinals,PhysicsFinals,,,2025-07-19T19:56:13.984883+00:00,benchmark_result,,,,True,,,,,,1412.0,,gemini-1.5-pro,,,,0.639,,,google,,,,,,,,0.639,https://arxiv.org/pdf/2403.05530,,,,,,,,2025-07-19T19:56:13.984883+00:00,,,,,,False,,False,0.0,False,0.0,0.639,0.639,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",PhysicsFinals,"['physics', 'math', 'reasoning']",text,False,1.0,en,"PHYSICS is a comprehensive benchmark for university-level physics problem solving, containing 1,297 expert-annotated problems covering six core areas: classical mechanics, quantum mechanics, thermodynamics and statistical mechanics, electromagnetism, atomic physics, and optics. Each problem requires advanced physics knowledge and mathematical reasoning. Even advanced models like o3-mini achieve only 59.9% accuracy.",0.639,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.877591+00:00,benchmark_result,,,,True,,,,,,1366.0,,gemini-1.5-pro,,,,0.539,,,google,,,,,,,,0.539,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.877591+00:00,,,,,,False,,False,0.0,False,0.0,0.539,0.539,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.539,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,video-mme,Video-MME,,,2025-07-19T19:56:13.906552+00:00,benchmark_result,,,,True,,,,,,1380.0,,gemini-1.5-pro,,,,0.786,,,google,,,,,,,,0.786,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.906552+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",Video-MME,"['multimodal', 'vision', 'reasoning']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",0.786,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemini
,Score,,,wmt23,WMT23,,,2025-07-19T19:56:13.939104+00:00,benchmark_result,,,,True,,,,,,1394.0,,gemini-1.5-pro,,,,0.751,,,google,,,,,,,,0.751,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:13.939104+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",WMT23,['language'],text,True,1.0,en,"The Eighth Conference on Machine Translation (WMT23) benchmark evaluating machine translation systems across 8 language pairs (14 translation directions) including general, biomedical, literary, and low-resource language translation tasks. Features specialized shared tasks for quality estimation, metrics evaluation, sign language translation, and discourse-level literary translation with professional human assessment.",0.751,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Safety Compliance,,,xstest,XSTest,,,2025-07-19T19:56:14.002222+00:00,benchmark_result,,,,True,,,,,,1418.0,,gemini-1.5-pro,,,,0.988,,,google,,,,,,,,0.988,https://deepmind.google/technologies/gemini/pro/,,,,,,,,2025-07-19T19:56:14.002222+00:00,,,,,,False,,False,0.0,False,0.0,0.988,0.988,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 1.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-01,2024.0,5.0,2024-05,Undisclosed,"Gemini 1.5 Pro is a mid-size multimodal model optimized for a wide range of reasoning tasks. It can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text.",XSTest,['safety'],text,False,1.0,en,"XSTest is a test suite designed to identify exaggerated safety behaviours in large language models. It comprises 450 prompts: 250 safe prompts across ten prompt types that well-calibrated models should not refuse to comply with, and 200 unsafe prompts as contrasts that models should refuse. The benchmark systematically evaluates whether models refuse to respond to clearly safe prompts due to overly cautious safety mechanisms.",0.988,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gemini
,Natural language to SQL conversion evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.423568+00:00,benchmark_result,,,,True,,,,,,1152.0,,gemini-2.0-flash,,,,0.569,,,google,,,,,,,,0.569,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.423568+00:00,,,,,,False,,False,0.0,False,0.0,0.569,0.569,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.569,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Automatic speech translation (BLEU score) across 21 languages,,,covost2,CoVoST2,,,2025-07-19T19:56:13.962212+00:00,benchmark_result,,,,True,,,,,,1404.0,,gemini-2.0-flash,,,,0.392,,,google,,,,,,,,0.392,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.962212+00:00,,,,,,False,,False,0.0,False,0.0,0.392,0.392,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",CoVoST2,"['audio', 'speech-to-text', 'language']",audio,True,1.0,en,"CoVoST 2 is a large-scale multilingual speech translation corpus derived from Common Voice, covering translations from 21 languages into English and from English into 15 languages. The dataset contains 2,880 hours of speech with 78K speakers for speech translation research.",0.392,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Video analysis across multiple domains,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.926117+00:00,benchmark_result,,,,True,,,,,,922.0,,gemini-2.0-flash,,,,0.715,,,google,,,,,,,,0.715,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:12.926117+00:00,,,,,,False,,False,0.0,False,0.0,0.715,0.715,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.715,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),gemini
,Ability to provide factuality correct responses given documents and diverse user requests,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.278460+00:00,benchmark_result,,,,True,,,,,,1095.0,,gemini-2.0-flash,,,,0.836,,,google,,,,,,,,0.836,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.278460+00:00,,,,,,False,,False,0.0,False,0.0,0.836,0.836,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.836,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,"Challenging dataset of questions written by domain experts in biology, physics, and chemistry",,,gpqa,GPQA,,,2025-07-19T19:56:11.639283+00:00,benchmark_result,,,,True,,,,,,279.0,,gemini-2.0-flash,,,,0.621,,,google,,,,,,,,0.621,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:11.639283+00:00,,,,,,False,,False,0.0,False,0.0,0.621,0.621,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.621,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,"Competition-level math problems, Held out dataset AIME/AMC-like",,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.449979+00:00,benchmark_result,,,,True,,,,,,1164.0,,gemini-2.0-flash,,,,0.63,,,google,,,,,,,,0.63,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.449979+00:00,,,,,,False,,False,0.0,False,0.0,0.63,0.63,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.63,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Code generation in Python. Code Generation subset covering more recent examples: 06/01/2024 - 10/05/2024,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.317443+00:00,benchmark_result,,,,True,,,,,,1111.0,,gemini-2.0-flash,,,,0.351,,,google,,,,,,,,0.351,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.317443+00:00,,,,,,False,,False,0.0,False,0.0,0.351,0.351,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.351,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,"Challenging math problems including algebra, geometry, pre-calculus, and others",,,math,MATH,,,2025-07-19T19:56:11.835842+00:00,benchmark_result,,,,True,,,,,,388.0,,gemini-2.0-flash,,,,0.897,,,google,,,,,,,,0.897,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:11.835842+00:00,,,,,,False,,False,0.0,False,0.0,0.897,0.897,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.897,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Enhanced version of MMLU dataset evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.437540+00:00,benchmark_result,,,,True,,,,,,174.0,,gemini-2.0-flash,,,,0.764,,,google,,,,,,,,0.764,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:11.437540+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.764,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Multi-discipline college-level multimodal understanding and reasoning problems,,,mmmu,MMMU,,,2025-07-19T19:56:12.156776+00:00,benchmark_result,,,,True,,,,,,562.0,,gemini-2.0-flash,,,,0.707,,,google,,,,,,,,0.707,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:12.156776+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.707,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,"Novel, diagnostic long-context understanding evaluation",,,mrcr,MRCR,,,2025-07-19T19:56:13.900780+00:00,benchmark_result,,,,True,,,,,,1378.0,,gemini-2.0-flash,,,,0.692,,,google,,,,,,,,0.692,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.900780+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.692,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Code generation evaluation across multiple languages,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.533525+00:00,benchmark_result,,,,True,,,,,,1204.0,,gemini-2.0-flash,,,,0.929,,,google,,,,,,,,0.929,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.533525+00:00,,,,,,False,,False,0.0,False,0.0,0.929,0.929,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.929,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,Visual understanding in chat models with challenging everyday examples,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.886575+00:00,benchmark_result,,,,True,,,,,,1371.0,,gemini-2.0-flash,,,,0.563,,,google,,,,,,,,0.563,https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/,,,,,,,,2025-07-19T19:56:13.886575+00:00,,,,,,False,,False,0.0,False,0.0,0.563,0.563,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-01,2024.0,12.0,2024-12,Undisclosed,"Next-generation model featuring superior speed, native tool use, multimodal generation, and a 1M token context window. Supports audio, images, video, and text input with capabilities for structured outputs, function calling, code execution, search, and multimodal operations.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.563,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,- evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.415349+00:00,benchmark_result,,,,True,,,,,,1148.0,,gemini-2.0-flash-lite,,,,0.574,,,google,,,,,,,,0.574,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.415349+00:00,,,,,,False,,False,0.0,False,0.0,0.574,0.574,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.574,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Automatic speech translation (BLEU score) across 21 languages,,,covost2,CoVoST2,,,2025-07-19T19:56:13.960537+00:00,benchmark_result,,,,True,,,,,,1403.0,,gemini-2.0-flash-lite,,,,0.384,,,google,,,,,,,,0.384,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.960537+00:00,,,,,,False,,False,0.0,False,0.0,0.384,0.384,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,CoVoST2,"['audio', 'speech-to-text', 'language']",audio,True,1.0,en,"CoVoST 2 is a large-scale multilingual speech translation corpus derived from Common Voice, covering translations from 21 languages into English and from English into 15 languages. The dataset contains 2,880 hours of speech with 78K speakers for speech translation research.",0.384,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Video analysis across multiple domains,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.924659+00:00,benchmark_result,,,,True,,,,,,921.0,,gemini-2.0-flash-lite,,,,0.672,,,google,,,,,,,,0.672,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:12.924659+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.672,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,- evaluation,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.264333+00:00,benchmark_result,,,,True,,,,,,1088.0,,gemini-2.0-flash-lite,,,,0.836,,,google,,,,,,,,0.836,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.264333+00:00,,,,,,False,,False,0.0,False,0.0,0.836,0.836,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.836,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,0-shot evaluation,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.543616+00:00,benchmark_result,,,,True,,,,,,1209.0,,gemini-2.0-flash-lite,,,,0.782,,,google,,,,,,,,0.782,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.543616+00:00,,,,,,False,,False,0.0,False,0.0,0.782,0.782,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.782,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.611234+00:00,benchmark_result,,,,True,,,,,,266.0,,gemini-2.0-flash-lite,,,,0.515,,,google,,,,,,,,0.515,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:11.611234+00:00,,,,,,False,,False,0.0,False,0.0,0.515,0.515,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.515,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,0-shot evaluation,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.433332+00:00,benchmark_result,,,,True,,,,,,1156.0,,gemini-2.0-flash-lite,,,,0.553,,,google,,,,,,,,0.553,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.433332+00:00,,,,,,False,,False,0.0,False,0.0,0.553,0.553,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.553,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass@1,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.771288+00:00,benchmark_result,,,,True,,,,,,1320.0,,gemini-2.0-flash-lite,,,,0.289,,,google,,,,,,,,0.289,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.771288+00:00,,,,,,False,,False,0.0,False,0.0,0.289,0.289,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.289,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,standard,,,math,MATH,,,2025-07-19T19:56:11.819524+00:00,benchmark_result,,,,True,,,,,,379.0,,gemini-2.0-flash-lite,,,,0.868,,,google,,,,,,,,0.868,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:11.819524+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.868,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Chain-of-Thought accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.423223+00:00,benchmark_result,,,,True,,,,,,166.0,,gemini-2.0-flash-lite,,,,0.716,,,google,,,,,,,,0.716,https://developers.googleblog.com/en/gemini-2-family-expands/,,,,,,,,2025-07-19T19:56:11.423223+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.716,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Multi-discipline college-level multimodal understanding and reasoning problems,,,mmmu,MMMU,,,2025-07-19T19:56:12.141505+00:00,benchmark_result,,,,True,,,,,,554.0,,gemini-2.0-flash-lite,,,,0.68,,,google,,,,,,,,0.68,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:12.141505+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.68,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Long-context comprehension accuracy,,,mrcr-1m,MRCR 1M,,,2025-07-19T19:56:13.956748+00:00,benchmark_result,,,,True,,,,,,1402.0,,gemini-2.0-flash-lite,,,,0.58,,,google,,,,,,,,0.58,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:13.956748+00:00,,,,,,False,,False,0.0,False,0.0,0.58,0.58,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,MRCR 1M,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR 1M is a variant of the Multi-Round Coreference Resolution benchmark designed for testing extremely long context capabilities with approximately 1 million tokens. It evaluates models' ability to maintain reasoning and attention across ultra-long conversations.,0.58,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Factuality,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.535234+00:00,benchmark_result,,,,True,,,,,,226.0,,gemini-2.0-flash-lite,,,,0.217,,,google,,,,,,,,0.217,https://ai.google.dev/gemini-api/docs/models,,,,,,,,2025-07-19T19:56:11.535234+00:00,,,,,,False,,False,0.0,False,0.0,0.217,0.217,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.0 Flash-Lite,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-05,2025.0,2.0,2025-02,Undisclosed,A Gemini 2.0 Flash model optimized for cost efficiency and low latency,SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.217,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Enhanced reasoning on competition-level math prompts,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.952263+00:00,benchmark_result,,,,True,,,,,,448.0,,gemini-2.0-flash-thinking,,,,0.733,,,google,,,,,,,,0.733,https://ai.google.dev/gemini-api/docs/models/gemini#evaluation,,,,,,,,2025-07-19T19:56:11.952263+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash Thinking,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-21,2025.0,1.0,2025-01,Undisclosed,"Gemini 2.0 Flash Thinking is a enhanced reasoning model, capable of showing its thoughts to improve performance and explainability. Combining speed and performance, Gemini 2.0 Flash Thinking also excels in science and math, showing its thinking to solve complex problems.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.733,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Challenging science questions requiring chain-of-thought reasoning,,,gpqa,GPQA,,,2025-07-19T19:56:11.620752+00:00,benchmark_result,,,,True,,,,,,271.0,,gemini-2.0-flash-thinking,,,,0.742,,,google,,,,,,,,0.742,https://ai.google.dev/gemini-api/docs/models/gemini#evaluation,,,,,,,,2025-07-19T19:56:11.620752+00:00,,,,,,False,,False,0.0,False,0.0,0.742,0.742,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash Thinking,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-21,2025.0,1.0,2025-01,Undisclosed,"Gemini 2.0 Flash Thinking is a enhanced reasoning model, capable of showing its thoughts to improve performance and explainability. Combining speed and performance, Gemini 2.0 Flash Thinking also excels in science and math, showing its thinking to solve complex problems.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.742,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Image-text QA across various domains,,,mmmu,MMMU,,,2025-07-19T19:56:12.151038+00:00,benchmark_result,,,,True,,,,,,559.0,,gemini-2.0-flash-thinking,,,,0.754,,,google,,,,,,,,0.754,https://ai.google.dev/gemini-api/docs/models/gemini#evaluation,,,,,,,,2025-07-19T19:56:12.151038+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.0 Flash Thinking,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-21,2025.0,1.0,2025-01,Undisclosed,"Gemini 2.0 Flash Thinking is a enhanced reasoning model, capable of showing its thoughts to improve performance and explainability. Combining speed and performance, Gemini 2.0 Flash Thinking also excels in science and math, showing its thinking to solve complex problems.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.754,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,whole,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.370513+00:00,benchmark_result,,,,True,,,,,,661.0,,gemini-2.5-flash,,,,0.619,,,google,,,,,,,,0.619,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:12.370513+00:00,,,,,,False,,False,0.0,False,0.0,0.619,0.619,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.619,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Diff-Fenced,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.795058+00:00,benchmark_result,,,,True,,,,,,1329.0,,gemini-2.5-flash,,,,0.567,,,google,,,,,,,,0.567,https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025,,,,,,,,2025-07-19T19:56:13.795058+00:00,,,,,,False,,False,0.0,False,0.0,0.567,0.567,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.567,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.950448+00:00,benchmark_result,,,,True,,,,,,447.0,,gemini-2.5-flash,,,,0.88,,,google,,,,,,,,0.88,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:11.950448+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.88,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Pass@1,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.428509+00:00,benchmark_result,,,,True,,,,,,683.0,,gemini-2.5-flash,,,,0.72,,,google,,,,,,,,0.72,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:12.428509+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.72,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Accuracy,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.271323+00:00,benchmark_result,,,,True,,,,,,1091.0,,gemini-2.5-flash,,,,0.853,,,google,,,,,,,,0.853,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:13.271323+00:00,,,,,,False,,False,0.0,False,0.0,0.853,0.853,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.853,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.550549+00:00,benchmark_result,,,,True,,,,,,1212.0,,gemini-2.5-flash,,,,0.884,,,google,,,,,,,,0.884,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:13.550549+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.884,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.619078+00:00,benchmark_result,,,,True,,,,,,270.0,,gemini-2.5-flash,,,,0.828,,,google,,,,,,,,0.828,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:11.619078+00:00,,,,,,False,,False,0.0,False,0.0,0.828,0.828,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.828,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.518055+00:00,benchmark_result,,,,True,,,,,,720.0,,gemini-2.5-flash,,,,0.11,,,google,,,,,,,,0.11,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:12.518055+00:00,,,,,,False,,False,0.0,False,0.0,0.11,0.11,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.11,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass@1,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.773194+00:00,benchmark_result,,,,True,,,,,,1321.0,,gemini-2.5-flash,,,,0.639,,,google,,,,,,,,0.639,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:13.773194+00:00,,,,,,False,,False,0.0,False,0.0,0.639,0.639,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.639,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Pass@1,,,mmmu,MMMU,,,2025-07-19T19:56:12.148985+00:00,benchmark_result,,,,True,,,,,,558.0,,gemini-2.5-flash,,,,0.797,,,google,,,,,,,,0.797,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:12.148985+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.797,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,1M-pointwise,,,mrcr,MRCR,,,2025-07-19T19:56:13.893404+00:00,benchmark_result,,,,True,,,,,,1374.0,,gemini-2.5-flash,,,,0.32,,,google,,,,,,,,0.32,https://blog.google/technology/google-deepmind/google-gemini-updates-io-2025,,,,,,,,2025-07-19T19:56:13.895016+00:00,,,,,,False,,False,0.0,False,0.0,0.32,0.32,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.32,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.540281+00:00,benchmark_result,,,,True,,,,,,229.0,,gemini-2.5-flash,,,,0.269,,,google,,,,,,,,0.269,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:11.540281+00:00,,,,,,False,,False,0.0,False,0.0,0.269,0.269,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.269,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.822771+00:00,benchmark_result,,,,True,,,,,,1341.0,,gemini-2.5-flash,,,,0.604,,,google,,,,,,,,0.604,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:13.822771+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.604,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.880772+00:00,benchmark_result,,,,True,,,,,,1368.0,,gemini-2.5-flash,,,,0.654,,,google,,,,,,,,0.654,https://developers.googleblog.com/en/start-building-with-gemini-25-flash/,,,,,,,,2025-07-19T19:56:13.880772+00:00,,,,,,False,,False,0.0,False,0.0,0.654,0.654,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Flash,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"A thinking model designed for a balance between price and performance. It builds upon Gemini 2.0 Flash with upgraded reasoning, hybrid thinking control, multimodal capabilities (text, image, video, audio input), and a 1M token input context window.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.654,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,Code editing,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.366506+00:00,benchmark_result,,,,True,,,,,,659.0,,gemini-2.5-flash-lite,,,,0.267,,,google,,,,,,,,0.267,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:12.366506+00:00,,,,,,False,,False,0.0,False,0.0,0.267,0.267,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.267,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Mathematics,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.422347+00:00,benchmark_result,,,,True,,,,,,681.0,,gemini-2.5-flash-lite,,,,0.498,,,google,,,,,,,,0.498,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:12.422347+00:00,,,,,,False,,False,0.0,False,0.0,0.498,0.498,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.498,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Default,,,arc,Arc,,,2025-07-19T19:56:13.969921+00:00,benchmark_result,,,,True,,,,,,1406.0,,gemini-2.5-flash-lite,,,,0.025,,,google,,,,,,,,0.025,https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash-lite,,,,,,,,2025-07-19T19:56:13.969921+00:00,,,,,,False,,False,0.0,False,0.0,0.025,0.025,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",Arc,"['reasoning', 'general']",multimodal,False,1.0,en,"The Abstraction and Reasoning Corpus (ARC) is a benchmark designed to measure human-like general fluid intelligence through grid-based reasoning tasks. It consists of 800 tasks (400 training, 400 evaluation) where each task presents input-output grids that require understanding abstract patterns and transformations. Test-takers must produce exactly correct output grids for all test inputs in a task to solve it, with 3 trials allowed per test input. ARC aims to enable fair comparisons of general intelligence between AI systems and humans using priors designed to be as close as possible to innate human priors.",0.025,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Factuality,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.267251+00:00,benchmark_result,,,,True,,,,,,1089.0,,gemini-2.5-flash-lite,,,,0.841,,,google,,,,,,,,0.841,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.267251+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.841,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Multilingual performance,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.546251+00:00,benchmark_result,,,,True,,,,,,1210.0,,gemini-2.5-flash-lite,,,,0.811,,,google,,,,,,,,0.811,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.546251+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.811,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.612808+00:00,benchmark_result,,,,True,,,,,,267.0,,gemini-2.5-flash-lite,,,,0.646,,,google,,,,,,,,0.646,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:11.612808+00:00,,,,,,False,,False,0.0,False,0.0,0.646,0.646,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.646,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,No tools,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.514286+00:00,benchmark_result,,,,True,,,,,,718.0,,gemini-2.5-flash-lite,,,,0.051,,,google,,,,,,,,0.051,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:12.514286+00:00,,,,,,False,,False,0.0,False,0.0,0.051,0.051,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.051,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Code generation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.300809+00:00,benchmark_result,,,,True,,,,,,1104.0,,gemini-2.5-flash-lite,,,,0.337,,,google,,,,,,,,0.337,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.300809+00:00,,,,,,False,,False,0.0,False,0.0,0.337,0.337,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.337,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Visual reasoning,,,mmmu,MMMU,,,2025-07-19T19:56:12.143254+00:00,benchmark_result,,,,True,,,,,,555.0,,gemini-2.5-flash-lite,,,,0.729,,,google,,,,,,,,0.729,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:12.143254+00:00,,,,,,False,,False,0.0,False,0.0,0.729,0.729,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.729,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Long context 128k average. 8 needle.,,,mrcr-v2,MRCR v2,,,2025-07-19T19:56:13.966057+00:00,benchmark_result,,,,True,,,,,,1405.0,,gemini-2.5-flash-lite,,,,0.166,,,google,,,,,,,,0.166,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.966057+00:00,,,,,,False,,False,0.0,False,0.0,0.166,0.166,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",MRCR v2,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR v2 (Multi-Round Coreference Resolution version 2) is an enhanced version of the synthetic long-context reasoning task. It extends the original MRCR framework with improved evaluation criteria and additional complexity for testing models' ability to maintain attention and reasoning across extended contexts.,0.166,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Factuality,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.536893+00:00,benchmark_result,,,,True,,,,,,227.0,,gemini-2.5-flash-lite,,,,0.107,,,google,,,,,,,,0.107,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:11.536893+00:00,,,,,,False,,False,0.0,False,0.0,0.107,0.107,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.107,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Agentic coding single attempt,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.819222+00:00,benchmark_result,,,,True,,,,,,1339.0,,gemini-2.5-flash-lite,,,,0.316,,,google,,,,,,,,0.316,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.819222+00:00,,,,,,False,,False,0.0,False,0.0,0.316,0.316,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.316,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Reka,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.875989+00:00,benchmark_result,,,,True,,,,,,1365.0,,gemini-2.5-flash-lite,,,,0.513,,,google,,,,,,,,0.513,https://deepmind.google/models/gemini/flash-lite/,,,,,,,,2025-07-19T19:56:13.875989+00:00,,,,,,False,,False,0.0,False,0.0,0.513,0.513,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Flash-Lite,google,Google,,0.0,False,0.0,False,True,creative_commons_attribution_4_0_license,Open & Permissive,2025-06-17,2025.0,6.0,2025-06,Undisclosed,"Gemini 2.5 Flash-Lite is a model developed by Google DeepMind, designed to handle various tasks including reasoning, science, mathematics, code generation, and more. It features advanced capabilities in multilingual performance and long context understanding. It is optimized for low latency use cases, supporting multimodal input with a 1 million-token context length.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.513,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,-,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.364634+00:00,benchmark_result,,,,True,,,,,,658.0,,gemini-2.5-pro,,,,0.765,,,google,,,,,,,,0.765,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:12.364634+00:00,,,,,,False,,False,0.0,False,0.0,0.765,0.765,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.765,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Diff,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.793176+00:00,benchmark_result,,,,True,,,,,,1328.0,,gemini-2.5-pro,,,,0.727,,,google,,,,,,,,0.727,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.793176+00:00,,,,,,False,,False,0.0,False,0.0,0.727,0.727,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.727,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.948567+00:00,benchmark_result,,,,True,,,,,,446.0,,gemini-2.5-pro,,,,0.92,,,google,,,,,,,,0.92,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:11.948567+00:00,,,,,,False,,False,0.0,False,0.0,0.92,0.92,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.92,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,Pass@1,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.417055+00:00,benchmark_result,,,,True,,,,,,679.0,,gemini-2.5-pro,,,,0.83,,,google,,,,,,,,0.83,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:12.417055+00:00,,,,,,False,,False,0.0,False,0.0,0.83,0.83,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.83,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,accuracy,,,arc-agi-v2,ARC-AGI v2,,,2025-07-19T19:56:13.918991+00:00,benchmark_result,,,,False,,,,,,1385.0,,gemini-2.5-pro,,,,0.049,,,google,,,,,,,,0.049,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.918991+00:00,,,,,,False,,False,0.0,False,0.0,0.049,0.049,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",ARC-AGI v2,"['reasoning', 'vision', 'spatial_reasoning']",multimodal,False,1.0,en,"ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",0.049,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.540318+00:00,benchmark_result,,,,True,,,,,,1207.0,,gemini-2.5-pro,,,,0.886,,,google,,,,,,,,0.886,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.540318+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.886,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.605360+00:00,benchmark_result,,,,True,,,,,,263.0,,gemini-2.5-pro,,,,0.83,,,google,,,,,,,,0.83,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:11.605360+00:00,,,,,,False,,False,0.0,False,0.0,0.83,0.83,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.83,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.511856+00:00,benchmark_result,,,,True,,,,,,717.0,,gemini-2.5-pro,,,,0.178,,,google,,,,,,,,0.178,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:12.511856+00:00,,,,,,False,,False,0.0,False,0.0,0.178,0.178,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.178,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Pass@1,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.763325+00:00,benchmark_result,,,,True,,,,,,1318.0,,gemini-2.5-pro,,,,0.756,,,google,,,,,,,,0.756,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.763325+00:00,,,,,,False,,False,0.0,False,0.0,0.756,0.756,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.756,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,Pass@1,,,mmmu,MMMU,,,2025-07-19T19:56:12.137517+00:00,benchmark_result,,,,True,,,,,,552.0,,gemini-2.5-pro,,,,0.796,,,google,,,,,,,,0.796,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:12.137517+00:00,,,,,,False,,False,0.0,False,0.0,0.796,0.796,Unknown,,,,,Undisclosed,Good (70-79%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.796,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemini
,128k-average,,,mrcr,MRCR,,,2025-07-19T19:56:13.889867+00:00,benchmark_result,,,,True,,,,,,1372.0,,gemini-2.5-pro,,,,0.93,,,google,,,,,,,,0.93,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.889867+00:00,,,,,,False,,False,0.0,False,0.0,0.93,0.93,Unknown,,,,,Undisclosed,Excellent (90%+),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",MRCR,"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR (Multi-Round Coreference Resolution) is a synthetic long-context reasoning task where models must navigate long conversations to reproduce specific model outputs. It tests the ability to distinguish between similar requests and reason about ordering while maintaining attention across extended contexts.,0.93,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemini
,Pointwise,,,mrcr-1m-(pointwise),MRCR 1M (pointwise),,,2025-07-19T19:56:13.915166+00:00,benchmark_result,,,,True,,,,,,1384.0,,gemini-2.5-pro,,,,0.829,,,google,,,,,,,,0.829,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.915166+00:00,,,,,,False,,False,0.0,False,0.0,0.829,0.829,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",MRCR 1M (pointwise),"['long_context', 'reasoning', 'general']",text,False,1.0,en,"MRCR 1M (pointwise) is a variant of the Multi-Round Coreference Resolution benchmark that uses pointwise evaluation for ultra-long contexts (~1M tokens). This version evaluates each response independently rather than comparatively, testing models' absolute performance on long-context reasoning tasks.",0.829,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.532774+00:00,benchmark_result,,,,True,,,,,,225.0,,gemini-2.5-pro,,,,0.508,,,google,,,,,,,,0.508,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:11.532774+00:00,,,,,,False,,False,0.0,False,0.0,0.508,0.508,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.508,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Accuracy,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.816932+00:00,benchmark_result,,,,True,,,,,,1338.0,,gemini-2.5-pro,,,,0.632,,,google,,,,,,,,0.632,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.816932+00:00,,,,,,False,,False,0.0,False,0.0,0.632,0.632,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.632,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.874453+00:00,benchmark_result,,,,True,,,,,,1364.0,,gemini-2.5-pro,,,,0.656,,,google,,,,,,,,0.656,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.874453+00:00,,,,,,False,,False,0.0,False,0.0,0.656,0.656,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.656,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,Accuracy,,,video-mme,Video-MME,,,2025-07-19T19:56:13.904547+00:00,benchmark_result,,,,True,,,,,,1379.0,,gemini-2.5-pro,,,,0.848,,,google,,,,,,,,0.848,https://deepmind.google/models/gemini/pro/,,,,,,,,2025-07-19T19:56:13.904547+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Our most intelligent AI model, built for the agentic era. Gemini 2.5 Pro leads on common benchmarks with enhanced reasoning, multimodal capabilities (text, image, video, audio input), and a 1M token context window.",Video-MME,"['multimodal', 'vision', 'reasoning']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",0.848,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemini
,Diff-fenced,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.368655+00:00,benchmark_result,,,,True,,,,,,660.0,,gemini-2.5-pro-preview-06-05,,,,0.822,,,google,,,,,,,,0.822,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:12.368655+00:00,,,,,,False,,False,0.0,False,0.0,0.822,0.822,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.822,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Single attempt,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.425843+00:00,benchmark_result,,,,True,,,,,,682.0,,gemini-2.5-pro-preview-06-05,,,,0.88,,,google,,,,,,,,0.88,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:12.425843+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.88,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Factuality,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.269434+00:00,benchmark_result,,,,True,,,,,,1090.0,,gemini-2.5-pro-preview-06-05,,,,0.878,,,google,,,,,,,,0.878,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:13.269434+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.878,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Multilingual performance,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.548453+00:00,benchmark_result,,,,True,,,,,,1211.0,,gemini-2.5-pro-preview-06-05,,,,0.892,,,google,,,,,,,,0.892,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:13.548453+00:00,,,,,,False,,False,0.0,False,0.0,0.892,0.892,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.892,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,Single attempt Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.617404+00:00,benchmark_result,,,,True,,,,,,269.0,,gemini-2.5-pro-preview-06-05,,,,0.864,,,google,,,,,,,,0.864,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:11.617404+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.864,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,No tools,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.516239+00:00,benchmark_result,,,,True,,,,,,719.0,,gemini-2.5-pro-preview-06-05,,,,0.216,,,google,,,,,,,,0.216,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:12.516239+00:00,,,,,,False,,False,0.0,False,0.0,0.216,0.216,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.216,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Single attempt (1/1/2025-5/1/2025),,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.303010+00:00,benchmark_result,,,,True,,,,,,1105.0,,gemini-2.5-pro-preview-06-05,,,,0.69,,,google,,,,,,,,0.69,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:13.303010+00:00,,,,,,False,,False,0.0,False,0.0,0.69,0.69,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.69,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Single attempt,,,mmmu,MMMU,,,2025-07-19T19:56:12.146880+00:00,benchmark_result,,,,True,,,,,,557.0,,gemini-2.5-pro-preview-06-05,,,,0.82,,,google,,,,,,,,0.82,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:12.146880+00:00,,,,,,False,,False,0.0,False,0.0,0.82,0.82,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.82,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,1M pointwise,,,mrcr-v2-(8-needle),MRCR v2 (8-needle),,,2025-07-19T19:56:14.013534+00:00,benchmark_result,,,,True,,,,,,1422.0,,gemini-2.5-pro-preview-06-05,,,,0.164,,,google,,,,,,,,0.164,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:14.016258+00:00,,,,,,False,,False,0.0,False,0.0,0.164,0.164,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",MRCR v2 (8-needle),"['long_context', 'reasoning', 'general']",text,False,1.0,en,MRCR v2 (8-needle) is a variant of the Multi-Round Coreference Resolution benchmark that includes 8 needle items to retrieve from long contexts. This tests models' ability to simultaneously track and reason about multiple pieces of information across extended conversations.,0.164,True,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Factuality,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.538432+00:00,benchmark_result,,,,True,,,,,,228.0,,gemini-2.5-pro-preview-06-05,,,,0.54,,,google,,,,,,,,0.54,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:11.538432+00:00,,,,,,False,,False,0.0,False,0.0,0.54,0.54,Unknown,,,,,Undisclosed,Poor (<60%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.54,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,Multiple attempts,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.820885+00:00,benchmark_result,,,,True,,,,,,1340.0,,gemini-2.5-pro-preview-06-05,,,,0.672,,,google,,,,,,,,0.672,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:13.820885+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.672,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,Image understanding,,,vibe-eval,Vibe-Eval,,,2025-07-19T19:56:13.879257+00:00,benchmark_result,,,,True,,,,,,1367.0,,gemini-2.5-pro-preview-06-05,,,,0.672,,,google,,,,,,,,0.672,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:13.879257+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",Vibe-Eval,"['multimodal', 'vision', 'general']",multimodal,False,1.0,en,"VIBE-Eval is a hard evaluation suite for measuring progress of multimodal language models, consisting of 269 visual understanding prompts with gold-standard responses authored by experts. The benchmark has dual objectives: vibe checking multimodal chat models for day-to-day tasks and rigorously testing frontier models, with the hard set containing >50% questions that all frontier models answer incorrectly.",0.672,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemini
,Video understanding,,,videommmu,VideoMMMU,,,2025-07-19T19:56:14.009959+00:00,benchmark_result,,,,True,,,,,,1421.0,,gemini-2.5-pro-preview-06-05,,,,0.836,,,google,,,,,,,,0.836,https://blog.google/products/gemini/gemini-2-5-pro-latest-preview/,,,,,,,,2025-07-19T19:56:14.009959+00:00,,,,,,False,,False,0.0,False,0.0,0.836,0.836,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini 2.5 Pro Preview 06-05,google,Google,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-06-05,2025.0,6.0,2025-06,Undisclosed,"The latest preview version of Google's most advanced reasoning Gemini model, capable of solving complex problems. Built for the agentic era with enhanced reasoning capabilities, multimodal understanding (text, image, video, audio), and a 1M token context window. Features thinking preview, code execution, grounding with Google Search, system instructions, function calling, and controlled generation. Supports up to 3,000 images per prompt, 45-60 minutes of video, and 8.4 hours of audio.",VideoMMMU,"['multimodal', 'vision', 'reasoning']",multimodal,False,1.0,en,"Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",0.836,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemini
,pass @1,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.434861+00:00,benchmark_result,,,,True,,,,,,685.0,,gemini-diffusion,,,,0.233,,,google,,,,,,,,0.233,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:12.434861+00:00,,,,,,False,,False,0.0,False,0.0,0.233,0.233,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.233,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,big-bench-extra-hard,BIG-Bench Extra Hard,,,2025-07-19T19:56:13.291288+00:00,benchmark_result,,,,True,,,,,,1100.0,,gemini-diffusion,,,,0.15,,,google,,,,,,,,0.15,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:13.291288+00:00,,,,,,False,,False,0.0,False,0.0,0.15,0.15,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",BIG-Bench Extra Hard,"['reasoning', 'general', 'language']",text,False,1.0,en,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",0.15,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,bigcodebench,BigCodeBench,,,2025-07-19T19:56:14.050987+00:00,benchmark_result,,,,True,,,,,,1433.0,,gemini-diffusion,,,,0.454,,,google,,,,,,,,0.454,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:14.050987+00:00,,,,,,False,,False,0.0,False,0.0,0.454,0.454,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",BigCodeBench,"['general', 'reasoning']",text,False,1.0,en,"A benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. Evaluates code generation with diverse function calls and complex instructions, featuring two variants: Complete (code completion based on comprehensive docstrings) and Instruct (generating code from natural language instructions).",0.454,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.559014+00:00,benchmark_result,,,,True,,,,,,1217.0,,gemini-diffusion,,,,0.691,,,google,,,,,,,,0.691,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:13.559014+00:00,,,,,,False,,False,0.0,False,0.0,0.691,0.691,Unknown,,,,,Undisclosed,Fair (60-69%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.691,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemini
,pass @1,,,gpqa,GPQA,,,2025-07-19T19:56:11.637311+00:00,benchmark_result,,,,True,,,,,,278.0,,gemini-diffusion,,,,0.404,,,google,,,,,,,,0.404,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:11.637311+00:00,,,,,,False,,False,0.0,False,0.0,0.404,0.404,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.404,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.625233+00:00,benchmark_result,,,,True,,,,,,773.0,,gemini-diffusion,,,,0.896,,,google,,,,,,,,0.896,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:12.625233+00:00,,,,,,False,,False,0.0,False,0.0,0.896,0.896,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.896,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemini
,pass @1,,,lbpp-(v2),LBPP (v2),,,2025-07-19T19:56:14.056060+00:00,benchmark_result,,,,True,,,,,,1435.0,,gemini-diffusion,,,,0.568,,,google,,,,,,,,0.568,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:14.056060+00:00,,,,,,False,,False,0.0,False,0.0,0.568,0.568,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",LBPP (v2),['reasoning'],text,False,1.0,en,"LBPP (v2) benchmark - specific documentation not found in official sources, possibly related to language-based planning problems",0.568,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.314684+00:00,benchmark_result,,,,True,,,,,,1110.0,,gemini-diffusion,,,,0.309,,,google,,,,,,,,0.309,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:13.314684+00:00,,,,,,False,,False,0.0,False,0.0,0.309,0.309,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.309,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,pass @1,,,mbpp,MBPP,,,2025-07-19T19:56:13.475906+00:00,benchmark_result,,,,True,,,,,,1175.0,,gemini-diffusion,,,,0.76,,,google,,,,,,,,0.76,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:13.475906+00:00,,,,,,False,,False,0.0,False,0.0,0.76,0.76,Unknown,,,,,Undisclosed,Good (70-79%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0076,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,"pass @1, Non-agentic evaluation (single turn edit only), max prompt length of 32K",,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.824708+00:00,benchmark_result,,,,True,,,,,,1342.0,,gemini-diffusion,,,,0.229,,,google,,,,,,,,0.229,https://deepmind.google/models/gemini-diffusion/,,,,,,,,2025-07-19T19:56:13.824708+00:00,,,,,,False,,False,0.0,False,0.0,0.229,0.229,Unknown,,,,,Undisclosed,Poor (<60%),Gemini Diffusion,google,Google,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-05-20,2025.0,5.0,2025-05,Undisclosed,"Gemini Diffusion is a state-of-the-art, experimental text diffusion model from Google DeepMind. It explores a new kind of language model designed to provide users with greater control, creativity, and speed in text generation. Instead of predicting text token-by-token, it learns to generate outputs by refining noise step-by-step, allowing for rapid iteration and error correction during generation. Key capabilities include rapid response times (reportedly 1479 tokens/sec excluding overhead), generation of more coherent text by outputting entire blocks of tokens at once, and iterative refinement for consistent outputs. It excels at tasks like editing, including in math and code contexts.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.229,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemini
,3-5-shot,,,agieval,AGIEval,,,2025-07-19T19:56:13.975397+00:00,benchmark_result,,,,True,,,,,,1408.0,,gemma-2-27b-it,,,,0.551,,,google,,,,,,,,0.551,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.975397+00:00,,,,,,False,,False,0.0,False,0.0,0.551,0.551,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",AGIEval,"['reasoning', 'general', 'math']",text,False,1.0,en,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",0.551,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.099650+00:00,benchmark_result,,,,True,,,,,,9.0,,gemma-2-27b-it,,,,0.714,,,google,,,,,,,,0.714,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.099650+00:00,,,,,,False,,False,0.0,False,0.0,0.714,0.714,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.714,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot,,,arc-e,ARC-E,,,2025-07-19T19:56:13.203403+00:00,benchmark_result,,,,True,,,,,,1055.0,,gemma-2-27b-it,,,,0.886,,,google,,,,,,,,0.886,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.203403+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.886,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,"3-shot, CoT",,,big-bench,BIG-Bench,,,2025-07-19T19:56:13.932992+00:00,benchmark_result,,,,True,,,,,,1392.0,,gemma-2-27b-it,,,,0.749,,,google,,,,,,,,0.749,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.932992+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",BIG-Bench,"['reasoning', 'math', 'language']",text,True,1.0,en,"Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark consisting of 204+ tasks designed to probe large language models and extrapolate their future capabilities. It covers diverse domains including linguistics, mathematics, common-sense reasoning, biology, physics, social bias, software development, and more. The benchmark focuses on tasks believed to be beyond current language model capabilities and includes both English and non-English tasks across multiple languages.",0.749,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.126514+00:00,benchmark_result,,,,True,,,,,,1021.0,,gemma-2-27b-it,,,,0.848,,,google,,,,,,,,0.848,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.126514+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.848,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,"5-shot, maj@1",,,gsm8k,GSM8k,,,2025-07-19T19:56:13.058102+00:00,benchmark_result,,,,True,,,,,,980.0,,gemma-2-27b-it,,,,0.74,,,google,,,,,,,,0.74,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.058102+00:00,,,,,,False,,False,0.0,False,0.0,0.74,0.74,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.74,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.164247+00:00,benchmark_result,,,,True,,,,,,38.0,,gemma-2-27b-it,,,,0.864,,,google,,,,,,,,0.864,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.164247+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.864,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.615384+00:00,benchmark_result,,,,True,,,,,,767.0,,gemma-2-27b-it,,,,0.518,,,google,,,,,,,,0.518,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:12.615384+00:00,,,,,,False,,False,0.0,False,0.0,0.518,0.518,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.518,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,4-shot,,,math,MATH,,,2025-07-19T19:56:11.824501+00:00,benchmark_result,,,,True,,,,,,382.0,,gemma-2-27b-it,,,,0.423,,,google,,,,,,,,0.423,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.824501+00:00,,,,,,False,,False,0.0,False,0.0,0.423,0.423,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.423,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,3-shot,,,mbpp,MBPP,,,2025-07-19T19:56:13.464425+00:00,benchmark_result,,,,True,,,,,,1170.0,,gemma-2-27b-it,,,,0.626,,,google,,,,,,,,0.626,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.464425+00:00,,,,,,False,,False,0.0,False,0.0,0.626,0.626,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00626,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,"5-shot, top-1",,,mmlu,MMLU,,,2025-07-19T19:56:11.228104+00:00,benchmark_result,,,,True,,,,,,68.0,,gemma-2-27b-it,,,,0.752,,,google,,,,,,,,0.752,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.228104+00:00,,,,,,False,,False,0.0,False,0.0,0.752,0.752,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.752,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.188220+00:00,benchmark_result,,,,True,,,,,,1048.0,,gemma-2-27b-it,,,,0.345,,,google,,,,,,,,0.345,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.188220+00:00,,,,,,False,,False,0.0,False,0.0,0.345,0.345,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.345,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.145819+00:00,benchmark_result,,,,True,,,,,,1030.0,,gemma-2-27b-it,,,,0.832,,,google,,,,,,,,0.832,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.145819+00:00,,,,,,False,,False,0.0,False,0.0,0.832,0.832,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.832,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.168648+00:00,benchmark_result,,,,True,,,,,,1039.0,,gemma-2-27b-it,,,,0.537,,,google,,,,,,,,0.537,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.168648+00:00,,,,,,False,,False,0.0,False,0.0,0.537,0.537,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.537,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.574247+00:00,benchmark_result,,,,True,,,,,,248.0,,gemma-2-27b-it,,,,0.837,,,google,,,,,,,,0.837,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.574247+00:00,,,,,,False,,False,0.0,False,0.0,0.837,0.837,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.837,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:13.212219+00:00,benchmark_result,,,,True,,,,,,1060.0,,gemma-2-27b-it,,,,0.837,,,google,,,,,,,,0.837,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.212219+00:00,,,,,,False,,False,0.0,False,0.0,0.837,0.837,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 27B,google,Google,27200000000.0,27200000000.0,True,13000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 27B IT is an instruction-tuned version of Google's state-of-the-art open language model. Built from the same research and technology as Gemini, it's optimized for dialogue applications through supervised fine-tuning, distillation from larger models, and RLHF. The model excels at text generation tasks including question answering, summarization, and reasoning.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.837,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,3-5-shot evaluation,,,agieval,AGIEval,,,2025-07-19T19:56:13.973652+00:00,benchmark_result,,,,True,,,,,,1407.0,,gemma-2-9b-it,,,,0.528,,,google,,,,,,,,0.528,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.973652+00:00,,,,,,False,,False,0.0,False,0.0,0.528,0.528,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",AGIEval,"['reasoning', 'general', 'math']",text,False,1.0,en,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",0.528,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot evaluation,,,arc-c,ARC-C,,,2025-07-19T19:56:11.097779+00:00,benchmark_result,,,,True,,,,,,8.0,,gemma-2-9b-it,,,,0.684,,,google,,,,,,,,0.684,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.097779+00:00,,,,,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.684,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,arc-e,ARC-E,,,2025-07-19T19:56:13.201834+00:00,benchmark_result,,,,True,,,,,,1054.0,,gemma-2-9b-it,,,,0.88,,,google,,,,,,,,0.88,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.201834+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.88,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,3-shot Chain-of-Thought,,,big-bench,BIG-Bench,,,2025-07-19T19:56:13.930966+00:00,benchmark_result,,,,True,,,,,,1391.0,,gemma-2-9b-it,,,,0.682,,,google,,,,,,,,0.682,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.930966+00:00,,,,,,False,,False,0.0,False,0.0,0.682,0.682,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",BIG-Bench,"['reasoning', 'math', 'language']",text,True,1.0,en,"Beyond the Imitation Game Benchmark (BIG-bench) is a collaborative benchmark consisting of 204+ tasks designed to probe large language models and extrapolate their future capabilities. It covers diverse domains including linguistics, mathematics, common-sense reasoning, biology, physics, social bias, software development, and more. The benchmark focuses on tasks believed to be beyond current language model capabilities and includes both English and non-English tasks across multiple languages.",0.682,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,boolq,BoolQ,,,2025-07-19T19:56:13.124981+00:00,benchmark_result,,,,True,,,,,,1020.0,,gemma-2-9b-it,,,,0.842,,,google,,,,,,,,0.842,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.124981+00:00,,,,,,False,,False,0.0,False,0.0,0.842,0.842,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.842,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,5-shot majority@1,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.053844+00:00,benchmark_result,,,,True,,,,,,978.0,,gemma-2-9b-it,,,,0.686,,,google,,,,,,,,0.686,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.053844+00:00,,,,,,False,,False,0.0,False,0.0,0.686,0.686,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.686,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,10-shot evaluation,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.157090+00:00,benchmark_result,,,,True,,,,,,36.0,,gemma-2-9b-it,,,,0.819,,,google,,,,,,,,0.819,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.157090+00:00,,,,,,False,,False,0.0,False,0.0,0.819,0.819,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.819,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.611318+00:00,benchmark_result,,,,True,,,,,,765.0,,gemma-2-9b-it,,,,0.402,,,google,,,,,,,,0.402,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:12.611318+00:00,,,,,,False,,False,0.0,False,0.0,0.402,0.402,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.402,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,4-shot evaluation,,,math,MATH,,,2025-07-19T19:56:11.821125+00:00,benchmark_result,,,,True,,,,,,380.0,,gemma-2-9b-it,,,,0.366,,,google,,,,,,,,0.366,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.821125+00:00,,,,,,False,,False,0.0,False,0.0,0.366,0.366,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.366,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,3-shot evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.462564+00:00,benchmark_result,,,,True,,,,,,1169.0,,gemma-2-9b-it,,,,0.524,,,google,,,,,,,,0.524,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.462564+00:00,,,,,,False,,False,0.0,False,0.0,0.524,0.524,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00524,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.224994+00:00,benchmark_result,,,,True,,,,,,66.0,,gemma-2-9b-it,,,,0.713,,,google,,,,,,,,0.713,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.224994+00:00,,,,,,False,,False,0.0,False,0.0,0.713,0.713,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.713,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot evaluation,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.186631+00:00,benchmark_result,,,,True,,,,,,1047.0,,gemma-2-9b-it,,,,0.292,,,google,,,,,,,,0.292,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.186631+00:00,,,,,,False,,False,0.0,False,0.0,0.292,0.292,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.292,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,piqa,PIQA,,,2025-07-19T19:56:13.144012+00:00,benchmark_result,,,,True,,,,,,1029.0,,gemma-2-9b-it,,,,0.817,,,google,,,,,,,,0.817,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.144012+00:00,,,,,,False,,False,0.0,False,0.0,0.817,0.817,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.817,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.166311+00:00,benchmark_result,,,,True,,,,,,1038.0,,gemma-2-9b-it,,,,0.534,,,google,,,,,,,,0.534,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:13.166311+00:00,,,,,,False,,False,0.0,False,0.0,0.534,0.534,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.534,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot evaluation,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.572657+00:00,benchmark_result,,,,True,,,,,,247.0,,gemma-2-9b-it,,,,0.766,,,google,,,,,,,,0.766,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.572657+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.766,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,partial score evaluation,,,winogrande,Winogrande,,,2025-07-19T19:56:11.380497+00:00,benchmark_result,,,,True,,,,,,148.0,,gemma-2-9b-it,,,,0.806,,,google,,,,,,,,0.806,https://huggingface.co/blog/gemma2,,,,,,,,2025-07-19T19:56:11.380497+00:00,,,,,,False,,False,0.0,False,0.0,0.806,0.806,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 2 9B,google,Google,9240000000.0,9240000000.0,True,8000000000000.0,True,False,gemma,Restricted/Community,2024-06-27,2024.0,6.0,2024-06,Very Large (>70B),"Gemma 2 9B IT is an instruction-tuned version of Google's Gemma 2 9B base model. It was trained on 8 trillion tokens of web data, code, and math content. The model features sliding window attention, logit soft-capping, and knowledge distillation techniques. It's optimized for dialogue applications through supervised fine-tuning, distillation, RLHF, and model merging using WARP.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.806,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,multimodal evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.621225+00:00,benchmark_result,,,,True,,,,,,1247.0,,gemma-3-12b-it,,,,0.842,,,google,,,,,,,,0.842,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.621225+00:00,,,,,,False,,False,0.0,False,0.0,0.842,0.842,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.842,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,big-bench-extra-hard,BIG-Bench Extra Hard,,,2025-07-19T19:56:13.282747+00:00,benchmark_result,,,,True,,,,,,1096.0,,gemma-3-12b-it,,,,0.163,,,google,,,,,,,,0.163,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.282747+00:00,,,,,,False,,False,0.0,False,0.0,0.163,0.163,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Extra Hard,"['reasoning', 'general', 'language']",text,False,1.0,en,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",0.163,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.226924+00:00,benchmark_result,,,,True,,,,,,1067.0,,gemma-3-12b-it,,,,0.857,,,google,,,,,,,,0.857,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.226924+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.857,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,- evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.413629+00:00,benchmark_result,,,,True,,,,,,1147.0,,gemma-3-12b-it,,,,0.479,,,google,,,,,,,,0.479,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.413629+00:00,,,,,,False,,False,0.0,False,0.0,0.479,0.479,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.479,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.789962+00:00,benchmark_result,,,,True,,,,,,855.0,,gemma-3-12b-it,,,,0.757,,,google,,,,,,,,0.757,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.789962+00:00,,,,,,False,,False,0.0,False,0.0,0.757,0.757,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.757,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,multimodal evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.830839+00:00,benchmark_result,,,,True,,,,,,878.0,,gemma-3-12b-it,,,,0.871,,,google,,,,,,,,0.871,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.830839+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.871,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.563615+00:00,benchmark_result,,,,True,,,,,,1219.0,,gemma-3-12b-it,,,,0.103,,,google,,,,,,,,0.103,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.563615+00:00,,,,,,False,,False,0.0,False,0.0,0.103,0.103,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.103,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,- evaluation,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.262640+00:00,benchmark_result,,,,True,,,,,,1087.0,,gemma-3-12b-it,,,,0.758,,,google,,,,,,,,0.758,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.262640+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.758,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.537058+00:00,benchmark_result,,,,True,,,,,,1205.0,,gemma-3-12b-it,,,,0.695,,,google,,,,,,,,0.695,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.537058+00:00,,,,,,False,,False,0.0,False,0.0,0.695,0.695,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.695,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.600334+00:00,benchmark_result,,,,True,,,,,,261.0,,gemma-3-12b-it,,,,0.409,,,google,,,,,,,,0.409,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.600334+00:00,,,,,,False,,False,0.0,False,0.0,0.409,0.409,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.409,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.052379+00:00,benchmark_result,,,,True,,,,,,977.0,,gemma-3-12b-it,,,,0.944,,,google,,,,,,,,0.944,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.052379+00:00,,,,,,False,,False,0.0,False,0.0,0.944,0.944,Unknown,,,,,Undisclosed,Excellent (90%+),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.944,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemma
,0-shot evaluation,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.427708+00:00,benchmark_result,,,,True,,,,,,1153.0,,gemma-3-12b-it,,,,0.545,,,google,,,,,,,,0.545,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.427708+00:00,,,,,,False,,False,0.0,False,0.0,0.545,0.545,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.545,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.606840+00:00,benchmark_result,,,,True,,,,,,762.0,,gemma-3-12b-it,,,,0.854,,,google,,,,,,,,0.854,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.606840+00:00,,,,,,False,,False,0.0,False,0.0,0.854,0.854,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.854,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.254325+00:00,benchmark_result,,,,True,,,,,,607.0,,gemma-3-12b-it,,,,0.889,,,google,,,,,,,,0.889,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.254325+00:00,,,,,,False,,False,0.0,False,0.0,0.889,0.889,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.889,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,multimodal evaluation,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.604072+00:00,benchmark_result,,,,True,,,,,,1238.0,,gemma-3-12b-it,,,,0.649,,,google,,,,,,,,0.649,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.604072+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.649,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.294686+00:00,benchmark_result,,,,True,,,,,,1101.0,,gemma-3-12b-it,,,,0.246,,,google,,,,,,,,0.246,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.294686+00:00,,,,,,False,,False,0.0,False,0.0,0.246,0.246,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.246,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,math,MATH,,,2025-07-19T19:56:11.815597+00:00,benchmark_result,,,,True,,,,,,377.0,,gemma-3-12b-it,,,,0.838,,,google,,,,,,,,0.838,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.815597+00:00,,,,,,False,,False,0.0,False,0.0,0.838,0.838,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.838,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,multimodal evaluation,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.657019+00:00,benchmark_result,,,,True,,,,,,1266.0,,gemma-3-12b-it,,,,0.629,,,google,,,,,,,,0.629,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.657019+00:00,,,,,,False,,False,0.0,False,0.0,0.629,0.629,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.629,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,3-shot evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.456223+00:00,benchmark_result,,,,True,,,,,,1166.0,,gemma-3-12b-it,,,,0.73,,,google,,,,,,,,0.73,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.456223+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0073,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.415028+00:00,benchmark_result,,,,True,,,,,,163.0,,gemma-3-12b-it,,,,0.606,,,google,,,,,,,,0.606,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.415028+00:00,,,,,,False,,False,0.0,False,0.0,0.606,0.606,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.606,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,multimodal evaluation,,,mmmu-(val),MMMU (val),,,2025-07-19T19:56:13.595790+00:00,benchmark_result,,,,True,,,,,,1235.0,,gemma-3-12b-it,,,,0.596,,,google,,,,,,,,0.596,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.595790+00:00,,,,,,False,,False,0.0,False,0.0,0.596,0.596,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MMMU (val),"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",0.596,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.521277+00:00,benchmark_result,,,,True,,,,,,1197.0,,gemma-3-12b-it,,,,0.807,,,google,,,,,,,,0.807,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.521277+00:00,,,,,,False,,False,0.0,False,0.0,0.807,0.807,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.807,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.528858+00:00,benchmark_result,,,,True,,,,,,224.0,,gemma-3-12b-it,,,,0.063,,,google,,,,,,,,0.063,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.528858+00:00,,,,,,False,,False,0.0,False,0.0,0.063,0.063,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.063,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.882990+00:00,benchmark_result,,,,True,,,,,,903.0,,gemma-3-12b-it,,,,0.677,,,google,,,,,,,,0.677,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.882990+00:00,,,,,,False,,False,0.0,False,0.0,0.677,0.677,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.677,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,multimodal evaluation,,,vqav2-(val),VQAv2 (val),,,2025-07-19T19:56:13.650557+00:00,benchmark_result,,,,True,,,,,,1263.0,,gemma-3-12b-it,,,,0.716,,,google,,,,,,,,0.716,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.650557+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",VQAv2 (val),"['vision', 'multimodal', 'language', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset containing open-ended questions about images that require understanding of vision, language, and commonsense knowledge to answer. VQAv2 addresses bias issues from the original VQA dataset by collecting complementary images such that every question is associated with similar images that result in different answers, forcing models to actually understand visual content rather than relying on language priors.",0.716,False,False,False,True,True,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.578915+00:00,benchmark_result,,,,True,,,,,,1227.0,,gemma-3-12b-it,,,,0.516,,,google,,,,,,,,0.516,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.578915+00:00,,,,,,False,,False,0.0,False,0.0,0.516,0.516,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 12B,google,Google,12000000000.0,12000000000.0,True,12000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 12B is a 12-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.516,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,big-bench-extra-hard,BIG-Bench Extra Hard,,,2025-07-19T19:56:13.289054+00:00,benchmark_result,,,,True,,,,,,1099.0,,gemma-3-1b-it,,,,0.072,,,google,,,,,,,,0.072,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.289054+00:00,,,,,,False,,False,0.0,False,0.0,0.072,0.072,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",BIG-Bench Extra Hard,"['reasoning', 'general', 'language']",text,False,1.0,en,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",0.072,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.240587+00:00,benchmark_result,,,,True,,,,,,1075.0,,gemma-3-1b-it,,,,0.391,,,google,,,,,,,,0.391,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.240587+00:00,,,,,,False,,False,0.0,False,0.0,0.391,0.391,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.391,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,- evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.421336+00:00,benchmark_result,,,,True,,,,,,1151.0,,gemma-3-1b-it,,,,0.064,,,google,,,,,,,,0.064,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.421336+00:00,,,,,,False,,False,0.0,False,0.0,0.064,0.064,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.064,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.574307+00:00,benchmark_result,,,,True,,,,,,1225.0,,gemma-3-1b-it,,,,0.014,,,google,,,,,,,,0.014,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.574307+00:00,,,,,,False,,False,0.0,False,0.0,0.014,0.014,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.014,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,- evaluation,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.276605+00:00,benchmark_result,,,,True,,,,,,1094.0,,gemma-3-1b-it,,,,0.364,,,google,,,,,,,,0.364,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.276605+00:00,,,,,,False,,False,0.0,False,0.0,0.364,0.364,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.364,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.557306+00:00,benchmark_result,,,,True,,,,,,1216.0,,gemma-3-1b-it,,,,0.342,,,google,,,,,,,,0.342,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.557306+00:00,,,,,,False,,False,0.0,False,0.0,0.342,0.342,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.342,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.633668+00:00,benchmark_result,,,,True,,,,,,276.0,,gemma-3-1b-it,,,,0.192,,,google,,,,,,,,0.192,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.633668+00:00,,,,,,False,,False,0.0,False,0.0,0.192,0.192,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.192,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.064705+00:00,benchmark_result,,,,True,,,,,,984.0,,gemma-3-1b-it,,,,0.628,,,google,,,,,,,,0.628,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.064705+00:00,,,,,,False,,False,0.0,False,0.0,0.628,0.628,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.628,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.445125+00:00,benchmark_result,,,,True,,,,,,1162.0,,gemma-3-1b-it,,,,0.158,,,google,,,,,,,,0.158,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.445125+00:00,,,,,,False,,False,0.0,False,0.0,0.158,0.158,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.158,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.623656+00:00,benchmark_result,,,,True,,,,,,772.0,,gemma-3-1b-it,,,,0.415,,,google,,,,,,,,0.415,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.623656+00:00,,,,,,False,,False,0.0,False,0.0,0.415,0.415,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.415,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.260062+00:00,benchmark_result,,,,True,,,,,,610.0,,gemma-3-1b-it,,,,0.802,,,google,,,,,,,,0.802,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.260062+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.802,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.311408+00:00,benchmark_result,,,,True,,,,,,1109.0,,gemma-3-1b-it,,,,0.019,,,google,,,,,,,,0.019,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.311408+00:00,,,,,,False,,False,0.0,False,0.0,0.019,0.019,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.019,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,math,MATH,,,2025-07-19T19:56:11.832121+00:00,benchmark_result,,,,True,,,,,,386.0,,gemma-3-1b-it,,,,0.48,,,google,,,,,,,,0.48,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.832121+00:00,,,,,,False,,False,0.0,False,0.0,0.48,0.48,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.48,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,3-shot evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.474036+00:00,benchmark_result,,,,True,,,,,,1174.0,,gemma-3-1b-it,,,,0.352,,,google,,,,,,,,0.352,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.474036+00:00,,,,,,False,,False,0.0,False,0.0,0.352,0.352,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0035199999999999997,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.434242+00:00,benchmark_result,,,,True,,,,,,172.0,,gemma-3-1b-it,,,,0.147,,,google,,,,,,,,0.147,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.434242+00:00,,,,,,False,,False,0.0,False,0.0,0.147,0.147,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.147,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.529701+00:00,benchmark_result,,,,True,,,,,,1202.0,,gemma-3-1b-it,,,,0.56,,,google,,,,,,,,0.56,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.529701+00:00,,,,,,False,,False,0.0,False,0.0,0.56,0.56,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.56,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.544931+00:00,benchmark_result,,,,True,,,,,,232.0,,gemma-3-1b-it,,,,0.022,,,google,,,,,,,,0.022,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.544931+00:00,,,,,,False,,False,0.0,False,0.0,0.022,0.022,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.022,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.590063+00:00,benchmark_result,,,,True,,,,,,1233.0,,gemma-3-1b-it,,,,0.359,,,google,,,,,,,,0.359,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.590063+00:00,,,,,,False,,False,0.0,False,0.0,0.359,0.359,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 1B,google,Google,1000000000.0,1000000000.0,True,2000000000000.0,True,False,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"The Gemma 3 1B model is a lightweight, 1-billion-parameter language model by Google, optimized for efficiency on resource-limited devices. At 529MB, it processes text at 2,585 tokens/second with a context window of 128,000 tokens. It supports 35+ languages but handles text-only input, unlike larger multimodal Gemma models. This balance of speed and efficiency makes it ideal for fast text processing on mobile and low-power devices.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.359,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.624921+00:00,benchmark_result,,,,True,,,,,,1249.0,,gemma-3-27b-it,,,,0.845,,,google,,,,,,,,0.845,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.624921+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.845,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,big-bench-extra-hard,BIG-Bench Extra Hard,,,2025-07-19T19:56:13.286991+00:00,benchmark_result,,,,True,,,,,,1098.0,,gemma-3-27b-it,,,,0.193,,,google,,,,,,,,0.193,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.286991+00:00,,,,,,False,,False,0.0,False,0.0,0.193,0.193,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Extra Hard,"['reasoning', 'general', 'language']",text,False,1.0,en,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",0.193,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.238868+00:00,benchmark_result,,,,True,,,,,,1074.0,,gemma-3-27b-it,,,,0.876,,,google,,,,,,,,0.876,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.238868+00:00,,,,,,False,,False,0.0,False,0.0,0.876,0.876,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.876,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,- evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.418526+00:00,benchmark_result,,,,True,,,,,,1150.0,,gemma-3-27b-it,,,,0.544,,,google,,,,,,,,0.544,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.418526+00:00,,,,,,False,,False,0.0,False,0.0,0.544,0.544,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.544,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.793657+00:00,benchmark_result,,,,True,,,,,,857.0,,gemma-3-27b-it,,,,0.78,,,google,,,,,,,,0.78,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.793657+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.78,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,multimodal evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.834284+00:00,benchmark_result,,,,True,,,,,,880.0,,gemma-3-27b-it,,,,0.866,,,google,,,,,,,,0.866,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.834284+00:00,,,,,,False,,False,0.0,False,0.0,0.866,0.866,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.866,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.572334+00:00,benchmark_result,,,,True,,,,,,1224.0,,gemma-3-27b-it,,,,0.167,,,google,,,,,,,,0.167,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.572334+00:00,,,,,,False,,False,0.0,False,0.0,0.167,0.167,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.167,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,- evaluation,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.275050+00:00,benchmark_result,,,,True,,,,,,1093.0,,gemma-3-27b-it,,,,0.749,,,google,,,,,,,,0.749,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.275050+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.749,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.555532+00:00,benchmark_result,,,,True,,,,,,1215.0,,gemma-3-27b-it,,,,0.751,,,google,,,,,,,,0.751,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.555532+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.751,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.628803+00:00,benchmark_result,,,,True,,,,,,275.0,,gemma-3-27b-it,,,,0.424,,,google,,,,,,,,0.424,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.628803+00:00,,,,,,False,,False,0.0,False,0.0,0.424,0.424,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.424,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.063038+00:00,benchmark_result,,,,True,,,,,,983.0,,gemma-3-27b-it,,,,0.959,,,google,,,,,,,,0.959,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.063038+00:00,,,,,,False,,False,0.0,False,0.0,0.959,0.959,Unknown,,,,,Undisclosed,Excellent (90%+),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.959,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gemma
,0-shot evaluation,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.443231+00:00,benchmark_result,,,,True,,,,,,1161.0,,gemma-3-27b-it,,,,0.603,,,google,,,,,,,,0.603,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.443231+00:00,,,,,,False,,False,0.0,False,0.0,0.603,0.603,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.603,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.621954+00:00,benchmark_result,,,,True,,,,,,771.0,,gemma-3-27b-it,,,,0.878,,,google,,,,,,,,0.878,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.621954+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.878,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.258406+00:00,benchmark_result,,,,True,,,,,,609.0,,gemma-3-27b-it,,,,0.904,,,google,,,,,,,,0.904,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.258406+00:00,,,,,,False,,False,0.0,False,0.0,0.904,0.904,Unknown,,,,,Undisclosed,Excellent (90%+),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.904,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemma
,multimodal evaluation,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.607541+00:00,benchmark_result,,,,True,,,,,,1240.0,,gemma-3-27b-it,,,,0.706,,,google,,,,,,,,0.706,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.607541+00:00,,,,,,False,,False,0.0,False,0.0,0.706,0.706,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.706,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.308517+00:00,benchmark_result,,,,True,,,,,,1108.0,,gemma-3-27b-it,,,,0.297,,,google,,,,,,,,0.297,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.308517+00:00,,,,,,False,,False,0.0,False,0.0,0.297,0.297,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.297,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,math,MATH,,,2025-07-19T19:56:11.830123+00:00,benchmark_result,,,,True,,,,,,385.0,,gemma-3-27b-it,,,,0.89,,,google,,,,,,,,0.89,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.830123+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.89,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,multimodal evaluation,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.660624+00:00,benchmark_result,,,,True,,,,,,1268.0,,gemma-3-27b-it,,,,0.676,,,google,,,,,,,,0.676,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.660624+00:00,,,,,,False,,False,0.0,False,0.0,0.676,0.676,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.676,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,3-shot evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.472259+00:00,benchmark_result,,,,True,,,,,,1173.0,,gemma-3-27b-it,,,,0.744,,,google,,,,,,,,0.744,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.472259+00:00,,,,,,False,,False,0.0,False,0.0,0.744,0.744,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0074399999999999996,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.432013+00:00,benchmark_result,,,,True,,,,,,171.0,,gemma-3-27b-it,,,,0.675,,,google,,,,,,,,0.675,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.432013+00:00,,,,,,False,,False,0.0,False,0.0,0.675,0.675,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.675,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,multimodal evaluation,,,mmmu-(val),MMMU (val),,,2025-07-19T19:56:13.599826+00:00,benchmark_result,,,,True,,,,,,1237.0,,gemma-3-27b-it,,,,0.649,,,google,,,,,,,,0.649,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.599826+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",MMMU (val),"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",0.649,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.528235+00:00,benchmark_result,,,,True,,,,,,1201.0,,gemma-3-27b-it,,,,0.845,,,google,,,,,,,,0.845,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.528235+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.845,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.543428+00:00,benchmark_result,,,,True,,,,,,231.0,,gemma-3-27b-it,,,,0.1,,,google,,,,,,,,0.1,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.543428+00:00,,,,,,False,,False,0.0,False,0.0,0.1,0.1,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.1,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.886992+00:00,benchmark_result,,,,True,,,,,,905.0,,gemma-3-27b-it,,,,0.651,,,google,,,,,,,,0.651,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.886992+00:00,,,,,,False,,False,0.0,False,0.0,0.651,0.651,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.651,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,multimodal evaluation,,,vqav2-(val),VQAv2 (val),,,2025-07-19T19:56:13.653584+00:00,benchmark_result,,,,True,,,,,,1265.0,,gemma-3-27b-it,,,,0.71,,,google,,,,,,,,0.71,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.653584+00:00,,,,,,False,,False,0.0,False,0.0,0.71,0.71,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",VQAv2 (val),"['vision', 'multimodal', 'language', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset containing open-ended questions about images that require understanding of vision, language, and commonsense knowledge to answer. VQAv2 addresses bias issues from the original VQA dataset by collecting complementary images such that every question is associated with similar images that result in different answers, forcing models to actually understand visual content rather than relying on language priors.",0.71,False,False,False,True,True,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.587542+00:00,benchmark_result,,,,True,,,,,,1232.0,,gemma-3-27b-it,,,,0.534,,,google,,,,,,,,0.534,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.587542+00:00,,,,,,False,,False,0.0,False,0.0,0.534,0.534,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 27B,google,Google,27000000000.0,27000000000.0,True,14000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 27B is a 27-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for complex question answering, summarization, reasoning, and image understanding tasks.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.534,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.622871+00:00,benchmark_result,,,,True,,,,,,1248.0,,gemma-3-4b-it,,,,0.748,,,google,,,,,,,,0.748,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.622871+00:00,,,,,,False,,False,0.0,False,0.0,0.748,0.748,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.748,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,big-bench-extra-hard,BIG-Bench Extra Hard,,,2025-07-19T19:56:13.285056+00:00,benchmark_result,,,,True,,,,,,1097.0,,gemma-3-4b-it,,,,0.11,,,google,,,,,,,,0.11,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.285056+00:00,,,,,,False,,False,0.0,False,0.0,0.11,0.11,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Extra Hard,"['reasoning', 'general', 'language']",text,False,1.0,en,"BIG-Bench Extra Hard (BBEH) is a challenging benchmark that replaces each task in BIG-Bench Hard with a novel task that probes similar reasoning capabilities but exhibits significantly increased difficulty. The benchmark contains 23 tasks testing diverse reasoning skills including many-hop reasoning, causal understanding, spatial reasoning, temporal arithmetic, geometric reasoning, linguistic reasoning, logic puzzles, and humor understanding. Designed to address saturation on existing benchmarks where state-of-the-art models achieve near-perfect scores, BBEH shows substantial room for improvement with best models achieving only 9.8-44.8% average accuracy.",0.11,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.237255+00:00,benchmark_result,,,,True,,,,,,1073.0,,gemma-3-4b-it,,,,0.722,,,google,,,,,,,,0.722,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.237255+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.722,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,- evaluation,,,bird-sql-(dev),Bird-SQL (dev),,,2025-07-19T19:56:13.417046+00:00,benchmark_result,,,,True,,,,,,1149.0,,gemma-3-4b-it,,,,0.363,,,google,,,,,,,,0.363,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.417046+00:00,,,,,,False,,False,0.0,False,0.0,0.363,0.363,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Bird-SQL (dev),['reasoning'],text,False,1.0,en,"BIRD (BIg Bench for LaRge-scale Database Grounded Text-to-SQLs) is a comprehensive text-to-SQL benchmark containing 12,751 question-SQL pairs across 95 databases (33.4 GB total) spanning 37+ professional domains. It evaluates large language models' ability to convert natural language to executable SQL queries in real-world scenarios with complex database schemas and dirty data.",0.363,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.791952+00:00,benchmark_result,,,,True,,,,,,856.0,,gemma-3-4b-it,,,,0.688,,,google,,,,,,,,0.688,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.791952+00:00,,,,,,False,,False,0.0,False,0.0,0.688,0.688,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.688,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,multimodal evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.832468+00:00,benchmark_result,,,,True,,,,,,879.0,,gemma-3-4b-it,,,,0.758,,,google,,,,,,,,0.758,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.832468+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.758,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.570776+00:00,benchmark_result,,,,True,,,,,,1223.0,,gemma-3-4b-it,,,,0.046,,,google,,,,,,,,0.046,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.570776+00:00,,,,,,False,,False,0.0,False,0.0,0.046,0.046,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.046,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,- evaluation,,,facts-grounding,FACTS Grounding,,,2025-07-19T19:56:13.273464+00:00,benchmark_result,,,,True,,,,,,1092.0,,gemma-3-4b-it,,,,0.701,,,google,,,,,,,,0.701,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.273464+00:00,,,,,,False,,False,0.0,False,0.0,0.701,0.701,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",FACTS Grounding,['reasoning'],text,False,1.0,en,"A benchmark evaluating language models' ability to generate factually accurate and well-grounded responses based on long-form input context, comprising 1,719 examples with documents up to 32k tokens requiring detailed responses that are fully grounded in provided documents",0.701,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.553690+00:00,benchmark_result,,,,True,,,,,,1214.0,,gemma-3-4b-it,,,,0.545,,,google,,,,,,,,0.545,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.553690+00:00,,,,,,False,,False,0.0,False,0.0,0.545,0.545,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.545,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.625675+00:00,benchmark_result,,,,True,,,,,,274.0,,gemma-3-4b-it,,,,0.308,,,google,,,,,,,,0.308,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.625675+00:00,,,,,,False,,False,0.0,False,0.0,0.308,0.308,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.308,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.061601+00:00,benchmark_result,,,,True,,,,,,982.0,,gemma-3-4b-it,,,,0.892,,,google,,,,,,,,0.892,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.061601+00:00,,,,,,False,,False,0.0,False,0.0,0.892,0.892,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.892,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot evaluation,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.440350+00:00,benchmark_result,,,,True,,,,,,1160.0,,gemma-3-4b-it,,,,0.43,,,google,,,,,,,,0.43,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.440350+00:00,,,,,,False,,False,0.0,False,0.0,0.43,0.43,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.43,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.620468+00:00,benchmark_result,,,,True,,,,,,770.0,,gemma-3-4b-it,,,,0.713,,,google,,,,,,,,0.713,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.620468+00:00,,,,,,False,,False,0.0,False,0.0,0.713,0.713,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.713,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.256346+00:00,benchmark_result,,,,True,,,,,,608.0,,gemma-3-4b-it,,,,0.902,,,google,,,,,,,,0.902,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.256346+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.902,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gemma
,multimodal evaluation,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.605648+00:00,benchmark_result,,,,True,,,,,,1239.0,,gemma-3-4b-it,,,,0.5,,,google,,,,,,,,0.5,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.605648+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.5,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.306674+00:00,benchmark_result,,,,True,,,,,,1107.0,,gemma-3-4b-it,,,,0.126,,,google,,,,,,,,0.126,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.306674+00:00,,,,,,False,,False,0.0,False,0.0,0.126,0.126,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.126,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,math,MATH,,,2025-07-19T19:56:11.828322+00:00,benchmark_result,,,,True,,,,,,384.0,,gemma-3-4b-it,,,,0.756,,,google,,,,,,,,0.756,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.828322+00:00,,,,,,False,,False,0.0,False,0.0,0.756,0.756,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.756,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,multimodal evaluation,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.659077+00:00,benchmark_result,,,,True,,,,,,1267.0,,gemma-3-4b-it,,,,0.5,,,google,,,,,,,,0.5,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.659077+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.5,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemma
,3-shot evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.469983+00:00,benchmark_result,,,,True,,,,,,1172.0,,gemma-3-4b-it,,,,0.632,,,google,,,,,,,,0.632,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.469983+00:00,,,,,,False,,False,0.0,False,0.0,0.632,0.632,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00632,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.430343+00:00,benchmark_result,,,,True,,,,,,170.0,,gemma-3-4b-it,,,,0.436,,,google,,,,,,,,0.436,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.430343+00:00,,,,,,False,,False,0.0,False,0.0,0.436,0.436,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.436,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,mmmu-(val),MMMU (val),,,2025-07-19T19:56:13.597769+00:00,benchmark_result,,,,True,,,,,,1236.0,,gemma-3-4b-it,,,,0.488,,,google,,,,,,,,0.488,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.597769+00:00,,,,,,False,,False,0.0,False,0.0,0.488,0.488,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",MMMU (val),"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"Validation set of the Massive Multi-discipline Multimodal Understanding and Reasoning benchmark. Features college-level multimodal questions across 6 core disciplines (Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, Tech & Engineering) spanning 30 subjects and 183 subfields with diverse image types including charts, diagrams, maps, and tables.",0.488,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemma
,0-shot evaluation,,,natural2code,Natural2Code,,,2025-07-19T19:56:13.526663+00:00,benchmark_result,,,,True,,,,,,1200.0,,gemma-3-4b-it,,,,0.703,,,google,,,,,,,,0.703,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.526663+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",Natural2Code,"['reasoning', 'general']",text,False,1.0,en,"NaturalCodeBench (NCB) is a challenging code benchmark designed to mirror the complexity and variety of real-world coding tasks. It comprises 402 high-quality problems in Python and Java, selected from natural user queries from online coding services, covering 6 different domains.",0.703,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot evaluation,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.542000+00:00,benchmark_result,,,,True,,,,,,230.0,,gemma-3-4b-it,,,,0.04,,,google,,,,,,,,0.04,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:11.542000+00:00,,,,,,False,,False,0.0,False,0.0,0.04,0.04,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.04,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.885190+00:00,benchmark_result,,,,True,,,,,,904.0,,gemma-3-4b-it,,,,0.578,,,google,,,,,,,,0.578,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:12.885190+00:00,,,,,,False,,False,0.0,False,0.0,0.578,0.578,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.578,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gemma
,multimodal evaluation,,,vqav2-(val),VQAv2 (val),,,2025-07-19T19:56:13.652122+00:00,benchmark_result,,,,True,,,,,,1264.0,,gemma-3-4b-it,,,,0.624,,,google,,,,,,,,0.624,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.652122+00:00,,,,,,False,,False,0.0,False,0.0,0.624,0.624,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",VQAv2 (val),"['vision', 'multimodal', 'language', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset containing open-ended questions about images that require understanding of vision, language, and commonsense knowledge to answer. VQAv2 addresses bias issues from the original VQA dataset by collecting complementary images such that every question is associated with similar images that result in different answers, forcing models to actually understand visual content rather than relying on language priors.",0.624,False,False,False,True,True,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gemma
,0-shot evaluation,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.586157+00:00,benchmark_result,,,,True,,,,,,1231.0,,gemma-3-4b-it,,,,0.468,,,google,,,,,,,,0.468,https://ai.google.dev/gemma/docs/core/model_card_3,,,,,,,,2025-07-19T19:56:13.586157+00:00,,,,,,False,,False,0.0,False,0.0,0.468,0.468,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3 4B,google,Google,4000000000.0,4000000000.0,True,4000000000000.0,True,True,gemma,Restricted/Community,2025-03-12,2025.0,3.0,2025-03,Very Large (>70B),"Gemma 3 4B is a 4-billion-parameter vision-language model from Google, handling text and image input and generating text output. It features a 128K context window, multilingual support, and open weights. Suitable for question answering, summarization, reasoning, and image understanding tasks.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.468,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.102376+00:00,benchmark_result,,,,True,,,,,,10.0,,gemma-3n-e2b,,,,0.517,,,google,,,,,,,,0.517,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:11.102376+00:00,,,,,,False,,False,0.0,False,0.0,0.517,0.517,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.517,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,arc-e,ARC-E,,,2025-07-19T19:56:13.204955+00:00,benchmark_result,,,,True,,,,,,1056.0,,gemma-3n-e2b,,,,0.758,,,google,,,,,,,,0.758,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.204955+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.758,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,few-shot,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.233872+00:00,benchmark_result,,,,True,,,,,,1071.0,,gemma-3n-e2b,,,,0.443,,,google,,,,,,,,0.443,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.233872+00:00,,,,,,False,,False,0.0,False,0.0,0.443,0.443,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.443,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.127882+00:00,benchmark_result,,,,True,,,,,,1022.0,,gemma-3n-e2b,,,,0.764,,,google,,,,,,,,0.764,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.127882+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.764,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,Token F1 score. 1-shot.,,,drop,DROP,,,2025-07-19T19:56:12.996776+00:00,benchmark_result,,,,True,,,,,,946.0,,gemma-3n-e2b,,,,0.539,,,google,,,,,,,,0.539,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:12.996776+00:00,,,,,,False,,False,0.0,False,0.0,0.539,0.539,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.539,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.166470+00:00,benchmark_result,,,,True,,,,,,39.0,,gemma-3n-e2b,,,,0.722,,,google,,,,,,,,0.722,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:11.166470+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.722,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.190039+00:00,benchmark_result,,,,True,,,,,,1049.0,,gemma-3n-e2b,,,,0.155,,,google,,,,,,,,0.155,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.190039+00:00,,,,,,False,,False,0.0,False,0.0,0.155,0.155,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.155,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.147878+00:00,benchmark_result,,,,True,,,,,,1031.0,,gemma-3n-e2b,,,,0.789,,,google,,,,,,,,0.789,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.147878+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.789,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.170669+00:00,benchmark_result,,,,True,,,,,,1040.0,,gemma-3n-e2b,,,,0.488,,,google,,,,,,,,0.488,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.170669+00:00,,,,,,False,,False,0.0,False,0.0,0.488,0.488,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.488,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.576196+00:00,benchmark_result,,,,True,,,,,,249.0,,gemma-3n-e2b,,,,0.608,,,google,,,,,,,,0.608,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:11.576196+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.608,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:13.213740+00:00,benchmark_result,,,,True,,,,,,1061.0,,gemma-3n-e2b,,,,0.668,,,google,,,,,,,,0.668,https://huggingface.co/google/gemma-3n-E2B,,,,,,,,2025-07-19T19:56:13.213740+00:00,,,,,,False,,False,0.0,False,0.0,0.668,0.668,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.668,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Accuracy. 0-shot.,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.437675+00:00,benchmark_result,,,,True,,,,,,686.0,,gemma-3n-e2b-it,,,,0.067,,,google,,,,,,,,0.067,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:12.437675+00:00,,,,,,False,,False,0.0,False,0.0,0.067,0.067,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.067,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,codegolf-v2.2,Codegolf v2.2,,,2025-07-19T19:56:13.787794+00:00,benchmark_result,,,,True,,,,,,1327.0,,gemma-3n-e2b-it,,,,0.11,,,google,,,,,,,,0.11,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.787794+00:00,,,,,,False,,False,0.0,False,0.0,0.11,0.11,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Codegolf v2.2,['code'],text,False,1.0,en,Codegolf v2.2 benchmark,0.11,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.575847+00:00,benchmark_result,,,,True,,,,,,1226.0,,gemma-3n-e2b-it,,,,0.025,,,google,,,,,,,,0.025,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.575847+00:00,,,,,,False,,False,0.0,False,0.0,0.025,0.025,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.025,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,global-mmlu,Global-MMLU,,,2025-07-19T19:56:13.758455+00:00,benchmark_result,,,,True,,,,,,1316.0,,gemma-3n-e2b-it,,,,0.551,,,google,,,,,,,,0.551,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.758455+00:00,,,,,,False,,False,0.0,False,0.0,0.551,0.551,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Global-MMLU,"['general', 'language', 'reasoning']",text,True,1.0,en,"A comprehensive multilingual benchmark covering 42 languages that addresses cultural and linguistic biases in evaluation, with improved translation quality and culturally sensitive question subsets.",0.551,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Accuracy. 0-shot.,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.560513+00:00,benchmark_result,,,,True,,,,,,1218.0,,gemma-3n-e2b-it,,,,0.59,,,google,,,,,,,,0.59,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.560513+00:00,,,,,,False,,False,0.0,False,0.0,0.59,0.59,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.59,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Diamond. 0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.641018+00:00,benchmark_result,,,,True,,,,,,280.0,,gemma-3n-e2b-it,,,,0.248,,,google,,,,,,,,0.248,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:11.641018+00:00,,,,,,False,,False,0.0,False,0.0,0.248,0.248,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.248,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Accuracy. 0-shot.,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.451948+00:00,benchmark_result,,,,True,,,,,,1165.0,,gemma-3n-e2b-it,,,,0.277,,,google,,,,,,,,0.277,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.451948+00:00,,,,,,False,,False,0.0,False,0.0,0.277,0.277,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.277,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,humaneval,HumanEval,,,2025-07-19T19:56:12.626596+00:00,benchmark_result,,,,True,,,,,,774.0,,gemma-3n-e2b-it,,,,0.665,,,google,,,,,,,,0.665,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:12.626596+00:00,,,,,,False,,False,0.0,False,0.0,0.665,0.665,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.665,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot,,,include,Include,,,2025-07-19T19:56:13.735634+00:00,benchmark_result,,,,True,,,,,,1307.0,,gemma-3n-e2b-it,,,,0.386,,,google,,,,,,,,0.386,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.735634+00:00,,,,,,False,,False,0.0,False,0.0,0.386,0.386,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.386,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.320311+00:00,benchmark_result,,,,True,,,,,,1112.0,,gemma-3n-e2b-it,,,,0.132,,,google,,,,,,,,0.132,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.320311+00:00,,,,,,False,,False,0.0,False,0.0,0.132,0.132,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.132,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.777049+00:00,benchmark_result,,,,True,,,,,,1323.0,,gemma-3n-e2b-it,,,,0.186,,,google,,,,,,,,0.186,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.777049+00:00,,,,,,False,,False,0.0,False,0.0,0.186,0.186,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.186,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 3-shot.,,,mbpp,MBPP,,,2025-07-19T19:56:13.477545+00:00,benchmark_result,,,,True,,,,,,1176.0,,gemma-3n-e2b-it,,,,0.566,,,google,,,,,,,,0.566,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.477545+00:00,,,,,,False,,False,0.0,False,0.0,0.566,0.566,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.005659999999999999,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.679623+00:00,benchmark_result,,,,True,,,,,,1278.0,,gemma-3n-e2b-it,,,,0.531,,,google,,,,,,,,0.531,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.679623+00:00,,,,,,False,,False,0.0,False,0.0,0.531,0.531,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.531,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Accuracy. 0-shot.,,,mmlu,MMLU,,,2025-07-19T19:56:11.234595+00:00,benchmark_result,,,,True,,,,,,71.0,,gemma-3n-e2b-it,,,,0.601,,,google,,,,,,,,0.601,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:11.234595+00:00,,,,,,False,,False,0.0,False,0.0,0.601,0.601,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.601,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Accuracy. 0-shot.,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.439365+00:00,benchmark_result,,,,True,,,,,,175.0,,gemma-3n-e2b-it,,,,0.405,,,google,,,,,,,,0.405,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:11.439365+00:00,,,,,,False,,False,0.0,False,0.0,0.405,0.405,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.405,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,mmlu-prox,MMLU-ProX,,,2025-07-19T19:56:13.746554+00:00,benchmark_result,,,,True,,,,,,1312.0,,gemma-3n-e2b-it,,,,0.081,,,google,,,,,,,,0.081,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.746554+00:00,,,,,,False,,False,0.0,False,0.0,0.081,0.081,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.081,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,openai-mmlu,OpenAI MMLU,,,2025-07-19T19:56:14.047435+00:00,benchmark_result,,,,True,,,,,,1432.0,,gemma-3n-e2b-it,,,,0.223,,,google,,,,,,,,0.223,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:14.047435+00:00,,,,,,False,,False,0.0,False,0.0,0.223,0.223,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",OpenAI MMLU,"['general', 'reasoning', 'math', 'legal', 'healthcare', 'finance', 'physics', 'chemistry', 'economics', 'psychology']",text,False,1.0,en,"MMLU (Massive Multitask Language Understanding) is a comprehensive benchmark that measures a text model's multitask accuracy across 57 diverse academic and professional subjects. The test covers elementary mathematics, US history, computer science, law, morality, business ethics, clinical knowledge, and many other domains spanning STEM, humanities, social sciences, and professional fields. To attain high accuracy, models must possess extensive world knowledge and problem-solving ability.",0.223,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Character-level F-score. 0-shot.,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.592107+00:00,benchmark_result,,,,True,,,,,,1234.0,,gemma-3n-e2b-it,,,,0.427,,,google,,,,,,,,0.427,https://huggingface.co/google/gemma-3n-E2B-it,,,,,,,,2025-07-19T19:56:13.592107+00:00,,,,,,False,,False,0.0,False,0.0,0.427,0.427,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.427,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.419451+00:00,benchmark_result,,,,True,,,,,,680.0,,gemma-3n-e2b-it-litert-preview,,,,0.067,,,google,,,,,,,,0.067,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.419451+00:00,,,,,,False,,False,0.0,False,0.0,0.067,0.067,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.067,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot Accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.095909+00:00,benchmark_result,,,,True,,,,,,7.0,,gemma-3n-e2b-it-litert-preview,,,,0.517,,,google,,,,,,,,0.517,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.095909+00:00,,,,,,False,,False,0.0,False,0.0,0.517,0.517,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.517,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,arc-e,ARC-E,,,2025-07-19T19:56:13.199540+00:00,benchmark_result,,,,True,,,,,,1053.0,,gemma-3n-e2b-it-litert-preview,,,,0.758,,,google,,,,,,,,0.758,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.199540+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.758,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,few-shot Accuracy,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.229977+00:00,benchmark_result,,,,True,,,,,,1069.0,,gemma-3n-e2b-it-litert-preview,,,,0.443,,,google,,,,,,,,0.443,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.229977+00:00,,,,,,False,,False,0.0,False,0.0,0.443,0.443,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.443,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,boolq,BoolQ,,,2025-07-19T19:56:13.123278+00:00,benchmark_result,,,,True,,,,,,1019.0,,gemma-3n-e2b-it-litert-preview,,,,0.764,,,google,,,,,,,,0.764,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.123278+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.764,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot pass@1,,,codegolf-v2.2,Codegolf v2.2,,,2025-07-19T19:56:13.783685+00:00,benchmark_result,,,,True,,,,,,1325.0,,gemma-3n-e2b-it-litert-preview,,,,0.11,,,google,,,,,,,,0.11,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.783685+00:00,,,,,,False,,False,0.0,False,0.0,0.11,0.11,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Codegolf v2.2,['code'],text,False,1.0,en,Codegolf v2.2 benchmark,0.11,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,1-shot Token F1 score,,,drop,DROP,,,2025-07-19T19:56:12.993202+00:00,benchmark_result,,,,True,,,,,,944.0,,gemma-3n-e2b-it-litert-preview,,,,0.539,,,google,,,,,,,,0.539,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.993202+00:00,,,,,,False,,False,0.0,False,0.0,0.539,0.539,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.539,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot ECLeKTic score,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.567241+00:00,benchmark_result,,,,True,,,,,,1221.0,,gemma-3n-e2b-it-litert-preview,,,,0.025,,,google,,,,,,,,0.025,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.567241+00:00,,,,,,False,,False,0.0,False,0.0,0.025,0.025,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.025,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,global-mmlu,Global-MMLU,,,2025-07-19T19:56:13.754602+00:00,benchmark_result,,,,True,,,,,,1314.0,,gemma-3n-e2b-it-litert-preview,,,,0.551,,,google,,,,,,,,0.551,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.754602+00:00,,,,,,False,,False,0.0,False,0.0,0.551,0.551,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Global-MMLU,"['general', 'language', 'reasoning']",text,True,1.0,en,"A comprehensive multilingual benchmark covering 42 languages that addresses cultural and linguistic biases in evaluation, with improved translation quality and culturally sensitive question subsets.",0.551,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.542151+00:00,benchmark_result,,,,True,,,,,,1208.0,,gemma-3n-e2b-it-litert-preview,,,,0.59,,,google,,,,,,,,0.59,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.542151+00:00,,,,,,False,,False,0.0,False,0.0,0.59,0.59,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.59,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,"Diamond, 0-shot RelaxedAccuracy/accuracy",,,gpqa,GPQA,,,2025-07-19T19:56:11.609514+00:00,benchmark_result,,,,True,,,,,,265.0,,gemma-3n-e2b-it-litert-preview,,,,0.248,,,google,,,,,,,,0.248,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.609514+00:00,,,,,,False,,False,0.0,False,0.0,0.248,0.248,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.248,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,10-shot Accuracy,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.154889+00:00,benchmark_result,,,,True,,,,,,35.0,,gemma-3n-e2b-it-litert-preview,,,,0.722,,,google,,,,,,,,0.722,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.154889+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.722,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot Accuracy,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.431354+00:00,benchmark_result,,,,True,,,,,,1155.0,,gemma-3n-e2b-it-litert-preview,,,,0.277,,,google,,,,,,,,0.277,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.431354+00:00,,,,,,False,,False,0.0,False,0.0,0.277,0.277,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.277,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.609959+00:00,benchmark_result,,,,True,,,,,,764.0,,gemma-3n-e2b-it-litert-preview,,,,0.665,,,google,,,,,,,,0.665,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.609959+00:00,,,,,,False,,False,0.0,False,0.0,0.665,0.665,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.665,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,include,Include,,,2025-07-19T19:56:13.731041+00:00,benchmark_result,,,,True,,,,,,1305.0,,gemma-3n-e2b-it-litert-preview,,,,0.386,,,google,,,,,,,,0.386,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.731041+00:00,,,,,,False,,False,0.0,False,0.0,0.386,0.386,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.386,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.298197+00:00,benchmark_result,,,,True,,,,,,1103.0,,gemma-3n-e2b-it-litert-preview,,,,0.132,,,google,,,,,,,,0.132,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.298197+00:00,,,,,,False,,False,0.0,False,0.0,0.132,0.132,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.132,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.768006+00:00,benchmark_result,,,,True,,,,,,1319.0,,gemma-3n-e2b-it-litert-preview,,,,0.186,,,google,,,,,,,,0.186,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.768006+00:00,,,,,,False,,False,0.0,False,0.0,0.186,0.186,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.186,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,3-shot pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.460487+00:00,benchmark_result,,,,True,,,,,,1168.0,,gemma-3n-e2b-it-litert-preview,,,,0.566,,,google,,,,,,,,0.566,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.460487+00:00,,,,,,False,,False,0.0,False,0.0,0.566,0.566,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.005659999999999999,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.672774+00:00,benchmark_result,,,,True,,,,,,1274.0,,gemma-3n-e2b-it-litert-preview,,,,0.531,,,google,,,,,,,,0.531,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.672774+00:00,,,,,,False,,False,0.0,False,0.0,0.531,0.531,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.531,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.222830+00:00,benchmark_result,,,,True,,,,,,65.0,,gemma-3n-e2b-it-litert-preview,,,,0.601,,,google,,,,,,,,0.601,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.222830+00:00,,,,,,False,,False,0.0,False,0.0,0.601,0.601,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.601,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.421645+00:00,benchmark_result,,,,True,,,,,,165.0,,gemma-3n-e2b-it-litert-preview,,,,0.405,,,google,,,,,,,,0.405,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.421645+00:00,,,,,,False,,False,0.0,False,0.0,0.405,0.405,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.405,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,mmlu-prox,MMLU-ProX,,,2025-07-19T19:56:13.743201+00:00,benchmark_result,,,,True,,,,,,1310.0,,gemma-3n-e2b-it-litert-preview,,,,0.081,,,google,,,,,,,,0.081,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.743201+00:00,,,,,,False,,False,0.0,False,0.0,0.081,0.081,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.081,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot Accuracy,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.184897+00:00,benchmark_result,,,,True,,,,,,1046.0,,gemma-3n-e2b-it-litert-preview,,,,0.155,,,google,,,,,,,,0.155,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.184897+00:00,,,,,,False,,False,0.0,False,0.0,0.155,0.155,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.155,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,piqa,PIQA,,,2025-07-19T19:56:13.142086+00:00,benchmark_result,,,,True,,,,,,1028.0,,gemma-3n-e2b-it-litert-preview,,,,0.789,,,google,,,,,,,,0.789,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.142086+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.789,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot Accuracy,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.164056+00:00,benchmark_result,,,,True,,,,,,1037.0,,gemma-3n-e2b-it-litert-preview,,,,0.488,,,google,,,,,,,,0.488,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.164056+00:00,,,,,,False,,False,0.0,False,0.0,0.488,0.488,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.488,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot Accuracy,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.571204+00:00,benchmark_result,,,,True,,,,,,246.0,,gemma-3n-e2b-it-litert-preview,,,,0.608,,,google,,,,,,,,0.608,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.571204+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.608,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,5-shot Accuracy,,,winogrande,Winogrande,,,2025-07-19T19:56:13.210650+00:00,benchmark_result,,,,True,,,,,,1059.0,,gemma-3n-e2b-it-litert-preview,,,,0.668,,,google,,,,,,,,0.668,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.210650+00:00,,,,,,False,,False,0.0,False,0.0,0.668,0.668,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.668,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,"ChrF, 0-shot Character-level F-score",,,wmt24++,WMT24++,,,2025-07-19T19:56:13.582347+00:00,benchmark_result,,,,True,,,,,,1229.0,,gemma-3n-e2b-it-litert-preview,,,,0.427,,,google,,,,,,,,0.427,https://huggingface.co/google/gemma-3n-E2B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.582347+00:00,,,,,,False,,False,0.0,False,0.0,0.427,0.427,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E2B Instructed LiteRT (Preview),google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.427,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.091862+00:00,benchmark_result,,,,True,,,,,,5.0,,gemma-3n-e4b,,,,0.616,,,google,,,,,,,,0.616,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:11.091862+00:00,,,,,,False,,False,0.0,False,0.0,0.616,0.616,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.616,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot,,,arc-e,ARC-E,,,2025-07-19T19:56:13.195091+00:00,benchmark_result,,,,True,,,,,,1051.0,,gemma-3n-e4b,,,,0.816,,,google,,,,,,,,0.816,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.195091+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.816,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,few-shot,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.225269+00:00,benchmark_result,,,,True,,,,,,1066.0,,gemma-3n-e4b,,,,0.529,,,google,,,,,,,,0.529,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.225269+00:00,,,,,,False,,False,0.0,False,0.0,0.529,0.529,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.529,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.120054+00:00,benchmark_result,,,,True,,,,,,1017.0,,gemma-3n-e4b,,,,0.816,,,google,,,,,,,,0.816,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.120054+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.816,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,Token F1 score. 1-shot.,,,drop,DROP,,,2025-07-19T19:56:12.989555+00:00,benchmark_result,,,,True,,,,,,942.0,,gemma-3n-e4b,,,,0.608,,,google,,,,,,,,0.608,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:12.989555+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.608,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,10-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.150880+00:00,benchmark_result,,,,True,,,,,,33.0,,gemma-3n-e4b,,,,0.786,,,google,,,,,,,,0.786,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:11.150880+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.786,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.181324+00:00,benchmark_result,,,,True,,,,,,1044.0,,gemma-3n-e4b,,,,0.209,,,google,,,,,,,,0.209,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.181324+00:00,,,,,,False,,False,0.0,False,0.0,0.209,0.209,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.209,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.136080+00:00,benchmark_result,,,,True,,,,,,1026.0,,gemma-3n-e4b,,,,0.81,,,google,,,,,,,,0.81,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.136080+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.81,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.159816+00:00,benchmark_result,,,,True,,,,,,1035.0,,gemma-3n-e4b,,,,0.5,,,google,,,,,,,,0.5,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.159816+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.5,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.567693+00:00,benchmark_result,,,,True,,,,,,244.0,,gemma-3n-e4b,,,,0.702,,,google,,,,,,,,0.702,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:11.567693+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.702,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:13.207598+00:00,benchmark_result,,,,True,,,,,,1057.0,,gemma-3n-e4b,,,,0.717,,,google,,,,,,,,0.717,https://huggingface.co/google/gemma-3n-E4B,,,,,,,,2025-07-19T19:56:13.207598+00:00,,,,,,False,,False,0.0,False,0.0,0.717,0.717,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.717,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,Accuracy. 0-shot.,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.431148+00:00,benchmark_result,,,,True,,,,,,684.0,,gemma-3n-e4b-it,,,,0.116,,,google,,,,,,,,0.116,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:12.431148+00:00,,,,,,False,,False,0.0,False,0.0,0.116,0.116,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.116,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,codegolf-v2.2,Codegolf v2.2,,,2025-07-19T19:56:13.785856+00:00,benchmark_result,,,,True,,,,,,1326.0,,gemma-3n-e4b-it,,,,0.168,,,google,,,,,,,,0.168,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.785856+00:00,,,,,,False,,False,0.0,False,0.0,0.168,0.168,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Codegolf v2.2,['code'],text,False,1.0,en,Codegolf v2.2 benchmark,0.168,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.569227+00:00,benchmark_result,,,,True,,,,,,1222.0,,gemma-3n-e4b-it,,,,0.19,,,google,,,,,,,,0.19,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.569227+00:00,,,,,,False,,False,0.0,False,0.0,0.19,0.19,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.19,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,global-mmlu,Global-MMLU,,,2025-07-19T19:56:13.756363+00:00,benchmark_result,,,,True,,,,,,1315.0,,gemma-3n-e4b-it,,,,0.603,,,google,,,,,,,,0.603,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.756363+00:00,,,,,,False,,False,0.0,False,0.0,0.603,0.603,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Global-MMLU,"['general', 'language', 'reasoning']",text,True,1.0,en,"A comprehensive multilingual benchmark covering 42 languages that addresses cultural and linguistic biases in evaluation, with improved translation quality and culturally sensitive question subsets.",0.603,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Accuracy. 0-shot.,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.552233+00:00,benchmark_result,,,,True,,,,,,1213.0,,gemma-3n-e4b-it,,,,0.645,,,google,,,,,,,,0.645,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.552233+00:00,,,,,,False,,False,0.0,False,0.0,0.645,0.645,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.645,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Diamond. 0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.624084+00:00,benchmark_result,,,,True,,,,,,273.0,,gemma-3n-e4b-it,,,,0.237,,,google,,,,,,,,0.237,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:11.624084+00:00,,,,,,False,,False,0.0,False,0.0,0.237,0.237,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.237,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Accuracy. 0-shot.,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.438271+00:00,benchmark_result,,,,True,,,,,,1159.0,,gemma-3n-e4b-it,,,,0.377,,,google,,,,,,,,0.377,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.438271+00:00,,,,,,False,,False,0.0,False,0.0,0.377,0.377,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.377,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,humaneval,HumanEval,,,2025-07-19T19:56:12.618954+00:00,benchmark_result,,,,True,,,,,,769.0,,gemma-3n-e4b-it,,,,0.75,,,google,,,,,,,,0.75,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:12.618954+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.75,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot,,,include,Include,,,2025-07-19T19:56:13.733461+00:00,benchmark_result,,,,True,,,,,,1306.0,,gemma-3n-e4b-it,,,,0.572,,,google,,,,,,,,0.572,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.733461+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.572,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.304919+00:00,benchmark_result,,,,True,,,,,,1106.0,,gemma-3n-e4b-it,,,,0.132,,,google,,,,,,,,0.132,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.304919+00:00,,,,,,False,,False,0.0,False,0.0,0.132,0.132,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.132,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 0-shot.,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.775429+00:00,benchmark_result,,,,True,,,,,,1322.0,,gemma-3n-e4b-it,,,,0.257,,,google,,,,,,,,0.257,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.775429+00:00,,,,,,False,,False,0.0,False,0.0,0.257,0.257,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.257,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,pass@1. 3-shot.,,,mbpp,MBPP,,,2025-07-19T19:56:13.466832+00:00,benchmark_result,,,,True,,,,,,1171.0,,gemma-3n-e4b-it,,,,0.636,,,google,,,,,,,,0.636,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.466832+00:00,,,,,,False,,False,0.0,False,0.0,0.636,0.636,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00636,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.678210+00:00,benchmark_result,,,,True,,,,,,1277.0,,gemma-3n-e4b-it,,,,0.67,,,google,,,,,,,,0.67,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.678210+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.67,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Accuracy. 0-shot.,,,mmlu,MMLU,,,2025-07-19T19:56:11.232243+00:00,benchmark_result,,,,True,,,,,,70.0,,gemma-3n-e4b-it,,,,0.649,,,google,,,,,,,,0.649,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:11.232243+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.649,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,Accuracy. 0-shot.,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.428457+00:00,benchmark_result,,,,True,,,,,,169.0,,gemma-3n-e4b-it,,,,0.506,,,google,,,,,,,,0.506,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:11.428457+00:00,,,,,,False,,False,0.0,False,0.0,0.506,0.506,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.506,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,mmlu-prox,MMLU-ProX,,,2025-07-19T19:56:13.744918+00:00,benchmark_result,,,,True,,,,,,1311.0,,gemma-3n-e4b-it,,,,0.199,,,google,,,,,,,,0.199,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.744918+00:00,,,,,,False,,False,0.0,False,0.0,0.199,0.199,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.199,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot,,,openai-mmlu,OpenAI MMLU,,,2025-07-19T19:56:14.045887+00:00,benchmark_result,,,,True,,,,,,1431.0,,gemma-3n-e4b-it,,,,0.356,,,google,,,,,,,,0.356,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:14.045887+00:00,,,,,,False,,False,0.0,False,0.0,0.356,0.356,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",OpenAI MMLU,"['general', 'reasoning', 'math', 'legal', 'healthcare', 'finance', 'physics', 'chemistry', 'economics', 'psychology']",text,False,1.0,en,"MMLU (Massive Multitask Language Understanding) is a comprehensive benchmark that measures a text model's multitask accuracy across 57 diverse academic and professional subjects. The test covers elementary mathematics, US history, computer science, law, morality, business ethics, clinical knowledge, and many other domains spanning STEM, humanities, social sciences, and professional fields. To attain high accuracy, models must possess extensive world knowledge and problem-solving ability.",0.356,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Character-level F-score. 0-shot.,,,wmt24++,WMT24++,,,2025-07-19T19:56:13.584588+00:00,benchmark_result,,,,True,,,,,,1230.0,,gemma-3n-e4b-it,,,,0.501,,,google,,,,,,,,0.501,https://huggingface.co/google/gemma-3n-E4B-it,,,,,,,,2025-07-19T19:56:13.584588+00:00,,,,,,False,,False,0.0,False,0.0,0.501,0.501,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed,google,Google,8000000000.0,8000000000.0,True,11000000000000.0,True,True,proprietary,Proprietary,2025-06-26,2025.0,6.0,2025-06,Very Large (>70B),"Gemma 3n is a multimodal model designed to run locally on hardware, supporting image, text, audio, and video inputs. It features a language decoder, audio encoder, and vision encoder, and is available in two sizes: E2B and E4B. The model is optimized for memory efficiency, allowing it to run on devices with limited GPU RAM. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma models are well-suited for a variety of content understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone. Gemma 3n models are designed for efficient execution on low-resource devices. They are capable of multimodal input, handling text, image, video, and audio input, and generating text outputs, with open weights for instruction-tuned variants. These models were trained with data in over 140 spoken languages.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.501,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.414248+00:00,benchmark_result,,,,True,,,,,,678.0,,gemma-3n-e4b-it-litert-preview,,,,0.116,,,google,,,,,,,,0.116,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.414248+00:00,,,,,,False,,False,0.0,False,0.0,0.116,0.116,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.116,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,25-shot Accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.093723+00:00,benchmark_result,,,,True,,,,,,6.0,,gemma-3n-e4b-it-litert-preview,,,,0.616,,,google,,,,,,,,0.616,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.093723+00:00,,,,,,False,,False,0.0,False,0.0,0.616,0.616,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.616,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,arc-e,ARC-E,,,2025-07-19T19:56:13.196728+00:00,benchmark_result,,,,True,,,,,,1052.0,,gemma-3n-e4b-it-litert-preview,,,,0.816,,,google,,,,,,,,0.816,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.196728+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ARC-E,"['reasoning', 'general']",text,False,1.0,en,"ARC-E (AI2 Reasoning Challenge - Easy Set) is a subset of grade-school level, multiple-choice science questions that requires knowledge and reasoning capabilities. Part of the AI2 Reasoning Challenge dataset containing 5,197 questions that test scientific reasoning and factual knowledge. The Easy Set contains questions that are answerable by retrieval-based and word co-occurrence algorithms, making them more accessible than the Challenge Set.",0.816,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,few-shot Accuracy,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.228349+00:00,benchmark_result,,,,True,,,,,,1068.0,,gemma-3n-e4b-it-litert-preview,,,,0.529,,,google,,,,,,,,0.529,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.228349+00:00,,,,,,False,,False,0.0,False,0.0,0.529,0.529,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.529,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,boolq,BoolQ,,,2025-07-19T19:56:13.121696+00:00,benchmark_result,,,,True,,,,,,1018.0,,gemma-3n-e4b-it-litert-preview,,,,0.816,,,google,,,,,,,,0.816,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.121696+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.816,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot pass@1,,,codegolf-v2.2,Codegolf v2.2,,,2025-07-19T19:56:13.781222+00:00,benchmark_result,,,,True,,,,,,1324.0,,gemma-3n-e4b-it-litert-preview,,,,0.168,,,google,,,,,,,,0.168,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.781222+00:00,,,,,,False,,False,0.0,False,0.0,0.168,0.168,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Codegolf v2.2,['code'],text,False,1.0,en,Codegolf v2.2 benchmark,0.168,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,1-shot Token F1 score,,,drop,DROP,,,2025-07-19T19:56:12.991359+00:00,benchmark_result,,,,True,,,,,,943.0,,gemma-3n-e4b-it-litert-preview,,,,0.608,,,google,,,,,,,,0.608,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.991359+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.608,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot ECLeKTic score,,,eclektic,ECLeKTic,,,2025-07-19T19:56:13.565422+00:00,benchmark_result,,,,True,,,,,,1220.0,,gemma-3n-e4b-it-litert-preview,,,,0.019,,,google,,,,,,,,0.019,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.565422+00:00,,,,,,False,,False,0.0,False,0.0,0.019,0.019,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",ECLeKTic,"['language', 'reasoning']",text,True,1.0,en,"A multilingual closed-book question answering dataset that evaluates cross-lingual knowledge transfer in large language models across 12 languages, using knowledge-seeking questions based on Wikipedia articles that exist only in one language",0.019,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,global-mmlu,Global-MMLU,,,2025-07-19T19:56:13.752749+00:00,benchmark_result,,,,True,,,,,,1313.0,,gemma-3n-e4b-it-litert-preview,,,,0.603,,,google,,,,,,,,0.603,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.752749+00:00,,,,,,False,,False,0.0,False,0.0,0.603,0.603,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Global-MMLU,"['general', 'language', 'reasoning']",text,True,1.0,en,"A comprehensive multilingual benchmark covering 42 languages that addresses cultural and linguistic biases in evaluation, with improved translation quality and culturally sensitive question subsets.",0.603,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,global-mmlu-lite,Global-MMLU-Lite,,,2025-07-19T19:56:13.538643+00:00,benchmark_result,,,,True,,,,,,1206.0,,gemma-3n-e4b-it-litert-preview,,,,0.645,,,google,,,,,,,,0.645,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.538643+00:00,,,,,,False,,False,0.0,False,0.0,0.645,0.645,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Global-MMLU-Lite,"['general', 'language', 'reasoning']",text,True,1.0,en,A lightweight version of Global MMLU benchmark that evaluates language models across multiple languages while addressing cultural and linguistic biases in multilingual evaluation.,0.645,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,"Diamond, 0-shot RelaxedAccuracy/accuracy",,,gpqa,GPQA,,,2025-07-19T19:56:11.602493+00:00,benchmark_result,,,,True,,,,,,262.0,,gemma-3n-e4b-it-litert-preview,,,,0.237,,,google,,,,,,,,0.237,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.602493+00:00,,,,,,False,,False,0.0,False,0.0,0.237,0.237,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.237,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,10-shot Accuracy,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.152761+00:00,benchmark_result,,,,True,,,,,,34.0,,gemma-3n-e4b-it-litert-preview,,,,0.786,,,google,,,,,,,,0.786,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.152761+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.786,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot Accuracy,,,hiddenmath,HiddenMath,,,2025-07-19T19:56:13.429415+00:00,benchmark_result,,,,True,,,,,,1154.0,,gemma-3n-e4b-it-litert-preview,,,,0.377,,,google,,,,,,,,0.377,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.429415+00:00,,,,,,False,,False,0.0,False,0.0,0.377,0.377,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HiddenMath,"['math', 'reasoning']",text,False,1.0,en,Google DeepMind's internal mathematical reasoning benchmark that introduces novel problems not encountered during model training to evaluate true mathematical reasoning capabilities rather than memorization,0.377,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.608423+00:00,benchmark_result,,,,True,,,,,,763.0,,gemma-3n-e4b-it-litert-preview,,,,0.75,,,google,,,,,,,,0.75,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:12.608423+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.75,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,0-shot Accuracy,,,include,Include,,,2025-07-19T19:56:13.729199+00:00,benchmark_result,,,,True,,,,,,1304.0,,gemma-3n-e4b-it-litert-preview,,,,0.572,,,google,,,,,,,,0.572,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.729199+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.572,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.296281+00:00,benchmark_result,,,,True,,,,,,1102.0,,gemma-3n-e4b-it-litert-preview,,,,0.132,,,google,,,,,,,,0.132,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.296281+00:00,,,,,,False,,False,0.0,False,0.0,0.132,0.132,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.132,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot pass@1,,,livecodebench-v5,LiveCodeBench v5,,,2025-07-19T19:56:13.761673+00:00,benchmark_result,,,,True,,,,,,1317.0,,gemma-3n-e4b-it-litert-preview,,,,0.257,,,google,,,,,,,,0.257,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.761673+00:00,,,,,,False,,False,0.0,False,0.0,0.257,0.257,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",LiveCodeBench v5,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.257,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,3-shot pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.458570+00:00,benchmark_result,,,,True,,,,,,1167.0,,gemma-3n-e4b-it-litert-preview,,,,0.636,,,google,,,,,,,,0.636,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.458570+00:00,,,,,,False,,False,0.0,False,0.0,0.636,0.636,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00636,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.671283+00:00,benchmark_result,,,,True,,,,,,1273.0,,gemma-3n-e4b-it-litert-preview,,,,0.607,,,google,,,,,,,,0.607,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.671283+00:00,,,,,,False,,False,0.0,False,0.0,0.607,0.607,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.607,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.219372+00:00,benchmark_result,,,,True,,,,,,63.0,,gemma-3n-e4b-it-litert-preview,,,,0.649,,,google,,,,,,,,0.649,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.219372+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.649,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gemma
,0-shot Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.420000+00:00,benchmark_result,,,,True,,,,,,164.0,,gemma-3n-e4b-it-litert-preview,,,,0.506,,,google,,,,,,,,0.506,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.420000+00:00,,,,,,False,,False,0.0,False,0.0,0.506,0.506,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.506,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,mmlu-prox,MMLU-ProX,,,2025-07-19T19:56:13.741460+00:00,benchmark_result,,,,True,,,,,,1309.0,,gemma-3n-e4b-it-litert-preview,,,,0.199,,,google,,,,,,,,0.199,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.741460+00:00,,,,,,False,,False,0.0,False,0.0,0.199,0.199,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.199,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot Accuracy,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.183031+00:00,benchmark_result,,,,True,,,,,,1045.0,,gemma-3n-e4b-it-litert-preview,,,,0.209,,,google,,,,,,,,0.209,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.183031+00:00,,,,,,False,,False,0.0,False,0.0,0.209,0.209,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.209,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,0-shot Accuracy,,,piqa,PIQA,,,2025-07-19T19:56:13.137952+00:00,benchmark_result,,,,True,,,,,,1027.0,,gemma-3n-e4b-it-litert-preview,,,,0.81,,,google,,,,,,,,0.81,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.137952+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.81,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gemma
,0-shot Accuracy,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.161822+00:00,benchmark_result,,,,True,,,,,,1036.0,,gemma-3n-e4b-it-litert-preview,,,,0.5,,,google,,,,,,,,0.5,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.161822+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.5,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,5-shot Accuracy,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.569334+00:00,benchmark_result,,,,True,,,,,,245.0,,gemma-3n-e4b-it-litert-preview,,,,0.702,,,google,,,,,,,,0.702,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:11.569334+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.702,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,5-shot Accuracy,,,winogrande,Winogrande,,,2025-07-19T19:56:13.209229+00:00,benchmark_result,,,,True,,,,,,1058.0,,gemma-3n-e4b-it-litert-preview,,,,0.717,,,google,,,,,,,,0.717,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.209229+00:00,,,,,,False,,False,0.0,False,0.0,0.717,0.717,Unknown,,,,,Undisclosed,Good (70-79%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.717,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gemma
,"ChrF, 0-shot Character-level F-score",,,wmt24++,WMT24++,,,2025-07-19T19:56:13.580409+00:00,benchmark_result,,,,True,,,,,,1228.0,,gemma-3n-e4b-it-litert-preview,,,,0.501,,,google,,,,,,,,0.501,https://huggingface.co/google/gemma-3n-E4B-it-litert-preview,,,,,,,,2025-07-19T19:56:13.580409+00:00,,,,,,False,,False,0.0,False,0.0,0.501,0.501,Unknown,,,,,Undisclosed,Poor (<60%),Gemma 3n E4B Instructed LiteRT Preview,google,Google,1910000000.0,1910000000.0,True,0.0,False,True,gemma,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"Gemma 3n is a generative AI model optimized for use in everyday devices, such as phones, laptops, and tablets. It features innovations like Per-Layer Embedding (PLE) parameter caching and a MatFormer model architecture for reduced compute and memory. These models handle audio, text, and visual data, though this E4B preview currently supports text and vision input. Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models, and is licensed for responsible commercial use.",WMT24++,['language'],text,True,1.0,en,"WMT24++ is a comprehensive multilingual machine translation benchmark that expands the WMT24 dataset to cover 55 languages and dialects. It includes human-written references and post-edits across four domains (literary, news, social, and speech) to evaluate machine translation systems and large language models across diverse linguistic contexts.",0.501,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gemma
,Estimated,,,aa-index,AA-Index,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7013.0,,glm-4.5,,,,0.677,,,zai-org,,,,,,,,0.677,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.677,0.677,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",AA-Index,['general'],text,False,1.0,en,"No official academic documentation found for this benchmark. Extensive research through ArXiv, IEEE/ACL/NeurIPS papers, and university research sites yielded no peer-reviewed sources for an 'aa-index' benchmark. This entry requires verification from official academic sources.",0.677,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,Avg@32,,,aime-2024,AIME24,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7002.0,,glm-4.5,,,,0.91,,,zai-org,,,,,,,,0.91,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.91,0.91,Unknown,,,,,Undisclosed,Excellent (90%+),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.91,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),glm
,Full,,,bfcl-v3,BFCL-v3,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7009.0,,glm-4.5,,,,0.778,,,zai-org,,,,,,,,0.778,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.778,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,standard,,,browsecomp,BrowseComp,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7011.0,,glm-4.5,,,,0.264,,,zai-org,,,,,,,,0.264,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.264,0.264,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.264,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,Avg@8,,,gpqa,GPQA,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7005.0,,glm-4.5,,,,0.791,,,zai-org,,,,,,,,0.791,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.791,0.791,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.791,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,text-based questions only,,,hle,HLE,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7012.0,,glm-4.5,,,,0.144,,,zai-org,,,,,,,,0.144,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.144,0.144,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",HLE,"['reasoning', 'math']",multimodal,False,1.0,en,"Humanity's Last Exam (HLE) is a multi-modal academic benchmark with 2,500 questions across mathematics, humanities, and natural sciences, designed to test LLM capabilities at the frontier of human knowledge with unambiguous, verifiable solutions",0.144,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,2407-2501,,,livecodebench,LiveCodeBench,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7006.0,,glm-4.5,,,,0.729,,,zai-org,,,,,,,,0.729,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.729,0.729,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.729,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,standard,,,math-500,MATH-500,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7003.0,,glm-4.5,,,,0.982,,,zai-org,,,,,,,,0.982,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.982,0.982,Unknown,,,,,Undisclosed,Excellent (90%+),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.982,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),glm
,standard,,,mmlu-pro,MMLU-Pro,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7001.0,,glm-4.5,,,,0.846,,,zai-org,,,,,,,,0.846,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.846,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),glm
,standard,,,scicode,SciCode,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7004.0,,glm-4.5,,,,0.417,,,zai-org,,,,,,,,0.417,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.417,0.417,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",SciCode,"['reasoning', 'math', 'physics', 'chemistry', 'code']",text,False,1.0,en,"SciCode is a research coding benchmark curated by scientists that challenges language models to code solutions for scientific problems. It contains 338 subproblems decomposed from 80 challenging main problems across 16 natural science sub-fields including mathematics, physics, chemistry, biology, and materials science. Problems require knowledge recall, reasoning, and code synthesis skills.",0.417,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,OpenHands v0.34.0,,,swe-bench-verified,SWE-bench-Verified,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7007.0,,glm-4.5,,,,0.642,,,zai-org,,,,,,,,0.642,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.642,0.642,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.642,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,optimized user simulator,,,tau-bench-airline,TAU-bench-Airline,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7010.0,,glm-4.5,,,,0.604,,,zai-org,,,,,,,,0.604,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.604,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,optimized user simulator,,,tau-bench-retail,TAU-bench-Retail,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7008.0,,glm-4.5,,,,0.797,,,zai-org,,,,,,,,0.797,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.797,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,Terminus framework,,,terminal-bench,Terminal-Bench,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7014.0,,glm-4.5,,,,0.375,,,zai-org,,,,,,,,0.375,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.375,0.375,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5,zai-org,Zhipu AI,355000000000.0,355000000000.0,True,23000000000000.0,True,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5 is an Agentic, Reasoning, and Coding (ARC) foundation model designed for intelligent agents, featuring 355 billion total parameters with 32 billion active parameters using MoE architecture. Trained on 23T tokens through multi-stage training, it is a hybrid reasoning model that provides two modes: thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. The model unifies agentic, reasoning, and coding capabilities with 128K context length support. It achieves exceptional performance with a score of 63.2 across 12 industry-standard benchmarks, placing 3rd among all proprietary and open-source models. Released under MIT open-source license allowing commercial use and secondary development.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.375,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,Estimated,,,aa-index,AA-Index,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7113.0,,glm-4.5-air,,,,0.648,,,zai-org,,,,,,,,0.648,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.648,0.648,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",AA-Index,['general'],text,False,1.0,en,"No official academic documentation found for this benchmark. Extensive research through ArXiv, IEEE/ACL/NeurIPS papers, and university research sites yielded no peer-reviewed sources for an 'aa-index' benchmark. This entry requires verification from official academic sources.",0.648,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,Avg@32,,,aime-2024,AIME24,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7102.0,,glm-4.5-air,,,,0.894,,,zai-org,,,,,,,,0.894,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.894,0.894,Unknown,,,,,Undisclosed,Very Good (80-89%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.894,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),glm
,Full,,,bfcl-v3,BFCL-v3,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7109.0,,glm-4.5-air,,,,0.764,,,zai-org,,,,,,,,0.764,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.764,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,standard,,,browsecomp,BrowseComp,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7111.0,,glm-4.5-air,,,,0.213,,,zai-org,,,,,,,,0.213,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.213,0.213,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.213,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,Avg@8,,,gpqa,GPQA,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7105.0,,glm-4.5-air,,,,0.75,,,zai-org,,,,,,,,0.75,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.75,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,text-based questions only,,,hle,HLE,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7112.0,,glm-4.5-air,,,,0.106,,,zai-org,,,,,,,,0.106,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.106,0.106,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",HLE,"['reasoning', 'math']",multimodal,False,1.0,en,"Humanity's Last Exam (HLE) is a multi-modal academic benchmark with 2,500 questions across mathematics, humanities, and natural sciences, designed to test LLM capabilities at the frontier of human knowledge with unambiguous, verifiable solutions",0.106,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,2407-2501,,,livecodebench,LiveCodeBench,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7106.0,,glm-4.5-air,,,,0.707,,,zai-org,,,,,,,,0.707,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.707,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,standard,,,math-500,MATH-500,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7103.0,,glm-4.5-air,,,,0.981,,,zai-org,,,,,,,,0.981,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.981,0.981,Unknown,,,,,Undisclosed,Excellent (90%+),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.981,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),glm
,standard,,,mmlu-pro,MMLU-Pro,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7101.0,,glm-4.5-air,,,,0.814,,,zai-org,,,,,,,,0.814,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.814,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),glm
,standard,,,scicode,SciCode,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7104.0,,glm-4.5-air,,,,0.373,,,zai-org,,,,,,,,0.373,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.373,0.373,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",SciCode,"['reasoning', 'math', 'physics', 'chemistry', 'code']",text,False,1.0,en,"SciCode is a research coding benchmark curated by scientists that challenges language models to code solutions for scientific problems. It contains 338 subproblems decomposed from 80 challenging main problems across 16 natural science sub-fields including mathematics, physics, chemistry, biology, and materials science. Problems require knowledge recall, reasoning, and code synthesis skills.",0.373,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,OpenHands v0.34.0,,,swe-bench-verified,SWE-bench-Verified,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7107.0,,glm-4.5-air,,,,0.576,,,zai-org,,,,,,,,0.576,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.576,0.576,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.576,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,optimized user simulator,,,tau-bench-airline,TAU-bench-Airline,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7110.0,,glm-4.5-air,,,,0.608,,,zai-org,,,,,,,,0.608,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.608,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,optimized user simulator,,,tau-bench-retail,TAU-bench-Retail,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7108.0,,glm-4.5-air,,,,0.779,,,zai-org,,,,,,,,0.779,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.779,0.779,Unknown,,,,,Undisclosed,Good (70-79%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.779,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),glm
,Terminus framework,,,terminal-bench,Terminal-Bench,,,2025-07-28T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7114.0,,glm-4.5-air,,,,0.3,,,zai-org,,,,,,,,0.3,https://z.ai/blog/glm-4.5,,,,,,,,2025-07-28T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.3,0.3,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.5-Air,zai-org,Zhipu AI,106000000000.0,106000000000.0,True,0.0,False,False,mit,Open & Permissive,2025-07-28,2025.0,7.0,2025-07,Very Large (>70B),"GLM-4.5-Air is a more compact variant of GLM-4.5 designed for efficient Agentic, Reasoning, and Coding (ARC) applications. It features 106 billion total parameters with 12 billion active parameters using MoE architecture. Like GLM-4.5, it is a hybrid reasoning model providing thinking mode for complex reasoning and tool usage, and non-thinking mode for immediate responses. Despite its compact design, GLM-4.5-Air delivers competitive performance with a score of 59.8 across 12 industry-standard benchmarks, ranking 6th overall while maintaining superior efficiency. It supports 128K context length and is released under MIT open-source license allowing commercial use.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.3,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,standard,,,aime-2025,AIME 2025,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7002.0,,glm-4.6,,,,0.939,,,zai-org,,,,,,,,0.939,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.939,0.939,Unknown,,,,,Undisclosed,Excellent (90%+),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.939,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),glm
,standard,,,browsecomp,BrowseComp,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7011.0,,glm-4.6,,,,0.451,,,zai-org,,,,,,,,0.451,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.451,0.451,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.451,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,standard,,,gpqa,GPQA,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7005.0,,glm-4.6,,,,0.81,,,zai-org,,,,,,,,0.81,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.81,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),glm
,standard,,,hle,HLE,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7012.0,,glm-4.6,,,,0.172,,,zai-org,,,,,,,,0.172,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.172,0.172,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",HLE,"['reasoning', 'math']",multimodal,False,1.0,en,"Humanity's Last Exam (HLE) is a multi-modal academic benchmark with 2,500 questions across mathematics, humanities, and natural sciences, designed to test LLM capabilities at the frontier of human knowledge with unambiguous, verifiable solutions",0.172,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,standard,,,livecodebench-v6,LiveCodeBench v6,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7006.0,,glm-4.6,,,,0.828,,,zai-org,,,,,,,,0.828,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.828,0.828,Unknown,,,,,Undisclosed,Very Good (80-89%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.828,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),glm
,OpenHands v0.34.0,,,swe-bench-verified,SWE-bench-Verified,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7007.0,,glm-4.6,,,,0.68,,,zai-org,,,,,,,,0.68,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.68,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),glm
,standard,,,terminal-bench,Terminal-Bench,,,2025-07-30T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,7014.0,,glm-4.6,,,,0.405,,,zai-org,,,,,,,,0.405,https://z.ai/blog/glm-4.6,,,,,,,,2025-07-30T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.405,0.405,Unknown,,,,,Undisclosed,Poor (<60%),GLM-4.6,zai-org,Zhipu AI,357000000000.0,357000000000.0,True,0.0,False,True,mit,Open & Permissive,2025-09-30,2025.0,9.0,2025-09,Very Large (>70B),"GLM-4.6 is the latest version of Z.ai's flagship model, bringing significant improvements over GLM-4.5. Key features include: 200K token context window (expanded from 128K), superior coding performance with better real-world application in Claude Code/Cline/Roo Code/Kilo Code, advanced reasoning with tool use during inference, stronger agent capabilities, and refined writing aligned with human preferences. GLM-4.6 achieves competitive performance with DeepSeek-V3.2-Exp and Claude Sonnet 4, reaching near parity with Claude Sonnet 4 (48.6% win rate) on CC-Bench real-world coding tasks.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.405,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),glm
,Accuracy,,,drop,DROP,,,2025-07-19T19:56:13.025267+00:00,benchmark_result,,,,False,,,,,,963.0,,gpt-3.5-turbo-0125,,,,0.702,,,openai,,,,,,,,0.702,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:13.025267+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.702,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.770449+00:00,benchmark_result,,,,False,,,,,,359.0,,gpt-3.5-turbo-0125,,,,0.308,,,openai,,,,,,,,0.308,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.770449+00:00,,,,,,False,,False,0.0,False,0.0,0.308,0.308,Unknown,,,,,Undisclosed,Poor (<60%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.308,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,humaneval,HumanEval,,,2025-07-19T19:56:12.697970+00:00,benchmark_result,,,,False,,,,,,815.0,,gpt-3.5-turbo-0125,,,,0.68,,,openai,,,,,,,,0.68,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:12.697970+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.68,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.906977+00:00,benchmark_result,,,,False,,,,,,429.0,,gpt-3.5-turbo-0125,,,,0.431,,,openai,,,,,,,,0.431,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.906977+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.431,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.127494+00:00,benchmark_result,,,,False,,,,,,547.0,,gpt-3.5-turbo-0125,,,,0.0,,,openai,,,,,,,,0.0,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:12.127494+00:00,,,,,,False,,False,0.0,False,0.0,0.0,0.0,Unknown,,,,,Undisclosed,Poor (<60%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.0,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.717321+00:00,benchmark_result,,,,False,,,,,,1299.0,,gpt-3.5-turbo-0125,,,,0.563,,,openai,,,,,,,,0.563,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:13.717321+00:00,,,,,,False,,False,0.0,False,0.0,0.563,0.563,Unknown,,,,,Undisclosed,Poor (<60%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.563,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.331664+00:00,benchmark_result,,,,False,,,,,,126.0,,gpt-3.5-turbo-0125,,,,0.698,,,openai,,,,,,,,0.698,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:11.331664+00:00,,,,,,False,,False,0.0,False,0.0,0.698,0.698,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.698,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.230222+00:00,benchmark_result,,,,False,,,,,,597.0,,gpt-3.5-turbo-0125,,,,0.0,,,openai,,,,,,,,0.0,https://example.com/benchmark-image,,,,,,,,2025-07-19T19:56:12.230222+00:00,,,,,,False,,False,0.0,False,0.0,0.0,0.0,Unknown,,,,,Undisclosed,Poor (<60%),GPT-3.5 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2023-03-21,2023.0,3.0,2023-03,Undisclosed,The latest GPT-3.5 Turbo model with higher accuracy at responding in requested formats and a fix for a bug which caused a text encoding issue for non-English language function calls.,MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.0,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"25-shot, Grade-school multiple choice science questions (Challenge-set)",,,ai2-reasoning-challenge-(arc),AI2 Reasoning Challenge (ARC),,,2025-07-19T19:56:15.421959+00:00,benchmark_result,,,,True,,,,,,1917.0,,gpt-4-0613,,,,0.963,,,openai,,,,,,,,0.963,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:15.421959+00:00,,,,,,False,,False,0.0,False,0.0,0.963,0.963,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,AI2 Reasoning Challenge (ARC),"['reasoning', 'general']",text,False,1.0,en,"A dataset of 7,787 genuine grade-school level, multiple-choice science questions assembled to encourage research in advanced question-answering. The dataset is partitioned into a Challenge Set and Easy Set, where the Challenge Set contains only questions answered incorrectly by both retrieval-based and word co-occurrence algorithms. Covers multiple scientific domains including biology, physics, earth science, and chemistry, requiring scientific reasoning, causal understanding, and conceptual knowledge beyond simple fact retrieval. Includes a supporting corpus of over 14 million science sentences.",0.963,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,"3-shot, Reading comprehension & arithmetic (f1 score)",,,drop,DROP,,,2025-07-19T19:56:13.028099+00:00,benchmark_result,,,,True,,,,,,965.0,,gpt-4-0613,,,,0.809,,,openai,,,,,,,,0.809,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:13.028099+00:00,,,,,,False,,False,0.0,False,0.0,0.809,0.809,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.809,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,"5-shot, Commonsense reasoning",,,gpqa,GPQA,,,2025-07-19T19:56:11.775863+00:00,benchmark_result,,,,True,,,,,,362.0,,gpt-4-0613,,,,0.357,,,openai,,,,,,,,0.357,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:11.775863+00:00,,,,,,False,,False,0.0,False,0.0,0.357,0.357,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.357,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"10-shot, Commonsense reasoning around everyday events",,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.199031+00:00,benchmark_result,,,,True,,,,,,55.0,,gpt-4-0613,,,,0.953,,,openai,,,,,,,,0.953,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:11.199031+00:00,,,,,,False,,False,0.0,False,0.0,0.953,0.953,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.953,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,"0-shot, Python coding tasks",,,humaneval,HumanEval,,,2025-07-19T19:56:12.702020+00:00,benchmark_result,,,,True,,,,,,817.0,,gpt-4-0613,,,,0.67,,,openai,,,,,,,,0.67,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:12.702020+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.67,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Percentile score,,,lsat,LSAT,,,2025-07-19T19:56:15.413295+00:00,benchmark_result,,,,True,,,,,,1915.0,,gpt-4-0613,,,,0.88,,,openai,,,,,,,,0.88,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:15.413295+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,LSAT,"['reasoning', 'legal', 'general']",text,False,1.0,en,"LSAT (Law School Admission Test) benchmark evaluating complex reasoning capabilities across three challenging tasks: analytical reasoning, logical reasoning, and reading comprehension. The LSAT measures skills considered essential for success in law school including critical thinking, reading comprehension of complex texts, and analysis of arguments.",0.88,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Mathematics problem-solving,,,math,MATH,,,2025-07-19T19:56:11.913379+00:00,benchmark_result,,,,True,,,,,,432.0,,gpt-4-0613,,,,0.42,,,openai,,,,,,,,0.42,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:11.913379+00:00,,,,,,False,,False,0.0,False,0.0,0.42,0.42,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.42,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Mathematics problem-solving,,,mgsm,MGSM,,,2025-07-19T19:56:13.721873+00:00,benchmark_result,,,,True,,,,,,1302.0,,gpt-4-0613,,,,0.745,,,openai,,,,,,,,0.745,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:13.721873+00:00,,,,,,False,,False,0.0,False,0.0,0.745,0.745,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.745,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,"5-shot, Multiple-choice questions in 57 subjects (professional & academic)",,,mmlu,MMLU,,,2025-07-19T19:56:11.336601+00:00,benchmark_result,,,,True,,,,,,129.0,,gpt-4-0613,,,,0.864,,,openai,,,,,,,,0.864,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:11.336601+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.864,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Estimated from reported score of 710 out of 800,,,sat-math,SAT Math,,,2025-07-19T19:56:15.417889+00:00,benchmark_result,,,,True,,,,,,1916.0,,gpt-4-0613,,,,0.89,,,openai,,,,,,,,0.89,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:15.417889+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,SAT Math,"['math', 'reasoning']",text,False,1.0,en,"SAT Math benchmark from AGIEval containing standardized mathematics questions from the College Board SAT examination, designed to evaluate mathematical reasoning capabilities of foundation models using human-centric assessment methods.",0.89,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Percentage score,,,uniform-bar-exam,Uniform Bar Exam,,,2025-07-19T19:56:15.408427+00:00,benchmark_result,,,,True,,,,,,1914.0,,gpt-4-0613,,,,0.9,,,openai,,,,,,,,0.9,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:15.408427+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,Uniform Bar Exam,"['legal', 'reasoning']",text,False,1.0,en,"The Uniform Bar Examination (UBE) benchmark evaluates language models on the complete bar exam including multiple-choice Multistate Bar Examination (MBE), open-ended Multistate Essay Exam (MEE), and Multistate Performance Test (MPT) components. Used to assess legal reasoning capabilities across seven subject areas including Evidence, Torts, Constitutional Law, Contracts, Criminal Law and Procedure, Real Property, and Civil Procedure.",0.9,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,"5-shot, Commonsense reasoning around pronoun resolution",,,winogrande,Winogrande,,,2025-07-19T19:56:11.396099+00:00,benchmark_result,,,,True,,,,,,156.0,,gpt-4-0613,,,,0.875,,,openai,,,,,,,,0.875,https://openai.com/research/gpt-4,,,,,,,,2025-07-19T19:56:11.396099+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2023-06-13,2023.0,6.0,2023-06,Undisclosed,GPT-4 is a large multimodal model capable of processing both image and text inputs and generating human-like text outputs. It demonstrates human-level performance on various professional and academic benchmarks.,Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.875,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Reading comprehension & arithmetic (f1 score),,,drop,DROP,,,2025-07-19T19:56:13.030041+00:00,benchmark_result,,,,True,,,,,,966.0,,gpt-4-turbo-2024-04-09,,,,0.86,,,openai,,,,,,,,0.86,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:13.030041+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.86,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,General-Purpose Question Answering,,,gpqa,GPQA,,,2025-07-19T19:56:11.777899+00:00,benchmark_result,,,,True,,,,,,363.0,,gpt-4-turbo-2024-04-09,,,,0.48,,,openai,,,,,,,,0.48,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:11.777899+00:00,,,,,,False,,False,0.0,False,0.0,0.48,0.48,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.48,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Python coding tasks,,,humaneval,HumanEval,,,2025-07-19T19:56:12.703615+00:00,benchmark_result,,,,True,,,,,,818.0,,gpt-4-turbo-2024-04-09,,,,0.871,,,openai,,,,,,,,0.871,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:12.703615+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.871,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Mathematics problem-solving,,,math,MATH,,,2025-07-19T19:56:11.916360+00:00,benchmark_result,,,,True,,,,,,433.0,,gpt-4-turbo-2024-04-09,,,,0.726,,,openai,,,,,,,,0.726,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:11.916360+00:00,,,,,,False,,False,0.0,False,0.0,0.726,0.726,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.726,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Grade School Math Word Problems,,,mgsm,MGSM,,,2025-07-19T19:56:13.723556+00:00,benchmark_result,,,,True,,,,,,1303.0,,gpt-4-turbo-2024-04-09,,,,0.885,,,openai,,,,,,,,0.885,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:13.723556+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.885,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Multiple-choice questions in 57 subjects (professional & academic),,,mmlu,MMLU,,,2025-07-19T19:56:11.337995+00:00,benchmark_result,,,,True,,,,,,130.0,,gpt-4-turbo-2024-04-09,,,,0.865,,,openai,,,,,,,,0.865,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:11.337995+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4 Turbo,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-04-09,2024.0,4.0,2024-04,Undisclosed,"The latest GPT-4 model with improved performance, updated knowledge, and enhanced capabilities. It offers faster response times and more affordable pricing compared to previous versions.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.865,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.389292+00:00,benchmark_result,,,,True,,,,,,671.0,,gpt-4.1-2025-04-14,,,,0.516,,,openai,,,,,,,,0.516,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.389292+00:00,,,,,,False,,False,0.0,False,0.0,0.516,0.516,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.516,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.808732+00:00,benchmark_result,,,,True,,,,,,1335.0,,gpt-4.1-2025-04-14,,,,0.529,,,openai,,,,,,,,0.529,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:13.808732+00:00,,,,,,False,,False,0.0,False,0.0,0.529,0.529,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.529,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.019979+00:00,benchmark_result,,,,True,,,,,,486.0,,gpt-4.1-2025-04-14,,,,0.481,,,openai,,,,,,,,0.481,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.019979+00:00,,,,,,False,,False,0.0,False,0.0,0.481,0.481,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.481,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 with no tools - Competition mathematics (AIME 2025).,,,aime-2025,AIME 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10011.0,,gpt-4.1-2025-04-14,,,,0.464,,,openai,,,,,,,,0.464,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.464,0.464,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.464,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,charxiv-d,CharXiv-D,,,2025-07-19T19:56:15.330689+00:00,benchmark_result,,,,True,,,,,,1889.0,,gpt-4.1-2025-04-14,,,,0.879,,,openai,,,,,,,,0.879,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.330689+00:00,,,,,,False,,False,0.0,False,0.0,0.879,0.879,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",CharXiv-D,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",0.879,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.201588+00:00,benchmark_result,,,,True,,,,,,1837.0,,gpt-4.1-2025-04-14,,,,0.567,,,openai,,,,,,,,0.567,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.201588+00:00,,,,,,False,,False,0.0,False,0.0,0.567,0.567,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.567,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,collie,COLLIE,,,2025-07-19T19:56:15.261360+00:00,benchmark_result,,,,True,,,,,,1860.0,,gpt-4.1-2025-04-14,,,,0.658,,,openai,,,,,,,,0.658,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.261360+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.658,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.348011+00:00,benchmark_result,,,,True,,,,,,1895.0,,gpt-4.1-2025-04-14,,,,0.655,,,openai,,,,,,,,0.655,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.348011+00:00,,,,,,False,,False,0.0,False,0.0,0.655,0.655,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.655,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.761405+00:00,benchmark_result,,,,True,,,,,,353.0,,gpt-4.1-2025-04-14,,,,0.663,,,openai,,,,,,,,0.663,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:11.761405+00:00,,,,,,False,,False,0.0,False,0.0,0.663,0.663,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.663,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.294683+00:00,benchmark_result,,,,True,,,,,,1874.0,,gpt-4.1-2025-04-14,,,,0.617,,,openai,,,,,,,,0.617,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.294683+00:00,,,,,,False,,False,0.0,False,0.0,0.617,0.617,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-bfs->128k,Graphwalks BFS >128k,,,2025-07-19T19:56:15.302353+00:00,benchmark_result,,,,True,,,,,,1877.0,,gpt-4.1-2025-04-14,,,,0.19,,,openai,,,,,,,,0.19,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.302353+00:00,,,,,,False,,False,0.0,False,0.0,0.19,0.19,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Graphwalks BFS >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to perform breadth-first search (BFS) operations on graphs with context length over 128k tokens, testing long-context reasoning capabilities.",0.19,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.312231+00:00,benchmark_result,,,,True,,,,,,1881.0,,gpt-4.1-2025-04-14,,,,0.58,,,openai,,,,,,,,0.58,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.312231+00:00,,,,,,False,,False,0.0,False,0.0,0.58,0.58,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-parents->128k,Graphwalks parents >128k,,,2025-07-19T19:56:15.324002+00:00,benchmark_result,,,,True,,,,,,1886.0,,gpt-4.1-2025-04-14,,,,0.25,,,openai,,,,,,,,0.25,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.324002+00:00,,,,,,False,,False,0.0,False,0.0,0.25,0.25,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Graphwalks parents >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to find parent nodes in graphs with context length over 128k tokens, testing long-context reasoning and graph structure understanding.",0.25,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 with no tools - Harvard-MIT Mathematics Tournament.,,,hmmt-2025,HMMT 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10013.0,,gpt-4.1-2025-04-14,,,,0.289,,,openai,,,,,,,,0.289,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.289,0.289,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.289,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 with no tools - Expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10012.0,,gpt-4.1-2025-04-14,,,,0.054,,,openai,,,,,,,,0.054,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.054,0.054,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.054,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,ifeval,IFEval,,,2025-07-19T19:56:12.304284+00:00,benchmark_result,,,,True,,,,,,635.0,,gpt-4.1-2025-04-14,,,,0.874,,,openai,,,,,,,,0.874,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.304284+00:00,,,,,,False,,False,0.0,False,0.0,0.874,0.874,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.874,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Internal benchmark,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.230360+00:00,benchmark_result,,,,True,,,,,,1848.0,,gpt-4.1-2025-04-14,,,,0.491,,,openai,,,,,,,,0.491,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.230360+00:00,,,,,,False,,False,0.0,False,0.0,0.491,0.491,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.491,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,mathvista,MathVista,,,2025-07-19T19:56:12.121168+00:00,benchmark_result,,,,True,,,,,,543.0,,gpt-4.1-2025-04-14,,,,0.722,,,openai,,,,,,,,0.722,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.121168+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.722,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,mmlu,MMLU,,,2025-07-19T19:56:11.323612+00:00,benchmark_result,,,,True,,,,,,121.0,,gpt-4.1-2025-04-14,,,,0.902,,,openai,,,,,,,,0.902,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:11.323612+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.902,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Standard benchmark,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.161058+00:00,benchmark_result,,,,True,,,,,,1483.0,,gpt-4.1-2025-04-14,,,,0.873,,,openai,,,,,,,,0.873,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.161058+00:00,,,,,,False,,False,0.0,False,0.0,0.873,0.873,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.873,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,mmmu,MMMU,,,2025-07-19T19:56:12.222754+00:00,benchmark_result,,,,True,,,,,,593.0,,gpt-4.1-2025-04-14,,,,0.748,,,openai,,,,,,,,0.748,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.222754+00:00,,,,,,False,,False,0.0,False,0.0,0.748,0.748,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.748,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.648170+00:00,benchmark_result,,,,True,,,,,,1653.0,,gpt-4.1-2025-04-14,,,,0.708,,,openai,,,,,,,,0.708,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.648170+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.708,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark (GPT-4o grader),,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.561934+00:00,benchmark_result,,,,True,,,,,,743.0,,gpt-4.1-2025-04-14,,,,0.383,,,openai,,,,,,,,0.383,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.561934+00:00,,,,,,False,,False,0.0,False,0.0,0.383,0.383,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.383,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Standard benchmark (o3-mini grader, see footnote [3])",,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.244951+00:00,benchmark_result,,,,True,,,,,,1854.0,,gpt-4.1-2025-04-14,,,,0.462,,,openai,,,,,,,,0.462,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.244951+00:00,,,,,,False,,False,0.0,False,0.0,0.462,0.462,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.462,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.275855+00:00,benchmark_result,,,,True,,,,,,1866.0,,gpt-4.1-2025-04-14,,,,0.572,,,openai,,,,,,,,0.572,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.275855+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.572,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-1m,OpenAI-MRCR: 2 needle 1M,,,2025-07-19T19:56:15.286394+00:00,benchmark_result,,,,True,,,,,,1871.0,,gpt-4.1-2025-04-14,,,,0.463,,,openai,,,,,,,,0.463,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.286394+00:00,,,,,,False,,False,0.0,False,0.0,0.463,0.463,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",OpenAI-MRCR: 2 needle 1M,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-Round Co-reference Resolution benchmark that tests an LLM's ability to distinguish between multiple similar needles hidden in long conversations. Models must reproduce specific instances of content (e.g., 'Return the 2nd poem about tapirs') from multi-turn synthetic conversations, requiring reasoning about context, ordering, and subtle differences between similar outputs.",0.463,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Internal methodology, see source footnote [2]",,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.858938+00:00,benchmark_result,,,,True,,,,,,1358.0,,gpt-4.1-2025-04-14,,,,0.546,,,openai,,,,,,,,0.546,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:13.858938+00:00,,,,,,False,,False,0.0,False,0.0,0.546,0.546,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.546,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4])",,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.015514+00:00,benchmark_result,,,,True,,,,,,1780.0,,gpt-4.1-2025-04-14,,,,0.494,,,openai,,,,,,,,0.494,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.015514+00:00,,,,,,False,,False,0.0,False,0.0,0.494,0.494,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.494,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4], GPT-4o user model)",,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.986496+00:00,benchmark_result,,,,True,,,,,,1766.0,,gpt-4.1-2025-04-14,,,,0.68,,,openai,,,,,,,,0.68,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.986496+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.68,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark,,,"video-mme-(long,-no-subtitles)","Video-MME (long, no subtitles)",,,2025-07-19T19:56:15.377204+00:00,benchmark_result,,,,True,,,,,,1907.0,,gpt-4.1-2025-04-14,,,,0.72,,,openai,,,,,,,,0.72,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.377204+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 is OpenAI's latest and most advanced flagship model, significantly improving upon GPT-4 Turbo in performance across benchmarks, speed, and cost-effectiveness.","Video-MME (long, no subtitles)","['vision', 'multimodal', 'video']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark for Multi-modal Large Language Models (MLLMs) in video analysis. This variant focuses on long-term videos (30min-60min) without subtitle inputs, testing robust contextual dynamics across 6 primary visual domains with 30 subfields including knowledge, film & television, sports competition, life record, and multilingual content.",0.72,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.382631+00:00,benchmark_result,,,,True,,,,,,667.0,,gpt-4.1-mini-2025-04-14,,,,0.347,,,openai,,,,,,,,0.347,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.382631+00:00,,,,,,False,,False,0.0,False,0.0,0.347,0.347,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.347,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.801113+00:00,benchmark_result,,,,True,,,,,,1331.0,,gpt-4.1-mini-2025-04-14,,,,0.316,,,openai,,,,,,,,0.316,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:13.801113+00:00,,,,,,False,,False,0.0,False,0.0,0.316,0.316,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.316,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.013761+00:00,benchmark_result,,,,True,,,,,,482.0,,gpt-4.1-mini-2025-04-14,,,,0.496,,,openai,,,,,,,,0.496,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.013761+00:00,,,,,,False,,False,0.0,False,0.0,0.496,0.496,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.496,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 mini with no tools - Competition mathematics (AIME 2025).,,,aime-2025,AIME 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10014.0,,gpt-4.1-mini-2025-04-14,,,,0.402,,,openai,,,,,,,,0.402,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.402,0.402,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.402,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,charxiv-d,CharXiv-D,,,2025-07-19T19:56:15.327509+00:00,benchmark_result,,,,True,,,,,,1887.0,,gpt-4.1-mini-2025-04-14,,,,0.884,,,openai,,,,,,,,0.884,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.327509+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",CharXiv-D,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",0.884,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.195563+00:00,benchmark_result,,,,True,,,,,,1834.0,,gpt-4.1-mini-2025-04-14,,,,0.568,,,openai,,,,,,,,0.568,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.195563+00:00,,,,,,False,,False,0.0,False,0.0,0.568,0.568,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.568,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,collie,COLLIE,,,2025-07-19T19:56:15.255006+00:00,benchmark_result,,,,True,,,,,,1857.0,,gpt-4.1-mini-2025-04-14,,,,0.546,,,openai,,,,,,,,0.546,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.255006+00:00,,,,,,False,,False,0.0,False,0.0,0.546,0.546,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.546,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.339307+00:00,benchmark_result,,,,True,,,,,,1892.0,,gpt-4.1-mini-2025-04-14,,,,0.493,,,openai,,,,,,,,0.493,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.339307+00:00,,,,,,False,,False,0.0,False,0.0,0.493,0.493,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.493,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.752534+00:00,benchmark_result,,,,True,,,,,,348.0,,gpt-4.1-mini-2025-04-14,,,,0.65,,,openai,,,,,,,,0.65,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:11.752534+00:00,,,,,,False,,False,0.0,False,0.0,0.65,0.65,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.65,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.289789+00:00,benchmark_result,,,,True,,,,,,1872.0,,gpt-4.1-mini-2025-04-14,,,,0.617,,,openai,,,,,,,,0.617,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.289789+00:00,,,,,,False,,False,0.0,False,0.0,0.617,0.617,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-bfs->128k,Graphwalks BFS >128k,,,2025-07-19T19:56:15.298708+00:00,benchmark_result,,,,True,,,,,,1875.0,,gpt-4.1-mini-2025-04-14,,,,0.15,,,openai,,,,,,,,0.15,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.298708+00:00,,,,,,False,,False,0.0,False,0.0,0.15,0.15,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Graphwalks BFS >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to perform breadth-first search (BFS) operations on graphs with context length over 128k tokens, testing long-context reasoning capabilities.",0.15,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.306151+00:00,benchmark_result,,,,True,,,,,,1878.0,,gpt-4.1-mini-2025-04-14,,,,0.605,,,openai,,,,,,,,0.605,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.306151+00:00,,,,,,False,,False,0.0,False,0.0,0.605,0.605,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-parents->128k,Graphwalks parents >128k,,,2025-07-19T19:56:15.319823+00:00,benchmark_result,,,,True,,,,,,1884.0,,gpt-4.1-mini-2025-04-14,,,,0.11,,,openai,,,,,,,,0.11,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.319823+00:00,,,,,,False,,False,0.0,False,0.0,0.11,0.11,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Graphwalks parents >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to find parent nodes in graphs with context length over 128k tokens, testing long-context reasoning and graph structure understanding.",0.11,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 mini with no tools - Harvard-MIT Mathematics Tournament.,,,hmmt-2025,HMMT 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10016.0,,gpt-4.1-mini-2025-04-14,,,,0.35,,,openai,,,,,,,,0.35,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.35,0.35,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.35,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4.1 mini with no tools - Expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10015.0,,gpt-4.1-mini-2025-04-14,,,,0.037,,,openai,,,,,,,,0.037,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.037,0.037,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.037,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,ifeval,IFEval,,,2025-07-19T19:56:12.299050+00:00,benchmark_result,,,,True,,,,,,632.0,,gpt-4.1-mini-2025-04-14,,,,0.841,,,openai,,,,,,,,0.841,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.299050+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.841,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Internal benchmark,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.225405+00:00,benchmark_result,,,,True,,,,,,1845.0,,gpt-4.1-mini-2025-04-14,,,,0.451,,,openai,,,,,,,,0.451,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.225405+00:00,,,,,,False,,False,0.0,False,0.0,0.451,0.451,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.451,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,mathvista,MathVista,,,2025-07-19T19:56:12.114367+00:00,benchmark_result,,,,True,,,,,,539.0,,gpt-4.1-mini-2025-04-14,,,,0.731,,,openai,,,,,,,,0.731,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.114367+00:00,,,,,,False,,False,0.0,False,0.0,0.731,0.731,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.731,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,mmlu,MMLU,,,2025-07-19T19:56:11.317652+00:00,benchmark_result,,,,True,,,,,,117.0,,gpt-4.1-mini-2025-04-14,,,,0.875,,,openai,,,,,,,,0.875,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:11.317652+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.875,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.157799+00:00,benchmark_result,,,,True,,,,,,1481.0,,gpt-4.1-mini-2025-04-14,,,,0.785,,,openai,,,,,,,,0.785,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.157799+00:00,,,,,,False,,False,0.0,False,0.0,0.785,0.785,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.785,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,mmmu,MMMU,,,2025-07-19T19:56:12.217019+00:00,benchmark_result,,,,True,,,,,,590.0,,gpt-4.1-mini-2025-04-14,,,,0.727,,,openai,,,,,,,,0.727,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.217019+00:00,,,,,,False,,False,0.0,False,0.0,0.727,0.727,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.727,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.643303+00:00,benchmark_result,,,,True,,,,,,1650.0,,gpt-4.1-mini-2025-04-14,,,,0.67,,,openai,,,,,,,,0.67,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.643303+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.67,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark (GPT-4o grader),,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.555824+00:00,benchmark_result,,,,True,,,,,,740.0,,gpt-4.1-mini-2025-04-14,,,,0.358,,,openai,,,,,,,,0.358,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:12.555824+00:00,,,,,,False,,False,0.0,False,0.0,0.358,0.358,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.358,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Standard benchmark (o3-mini grader, see footnote [3])",,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.239021+00:00,benchmark_result,,,,True,,,,,,1851.0,,gpt-4.1-mini-2025-04-14,,,,0.422,,,openai,,,,,,,,0.422,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.239021+00:00,,,,,,False,,False,0.0,False,0.0,0.422,0.422,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.422,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.270008+00:00,benchmark_result,,,,True,,,,,,1863.0,,gpt-4.1-mini-2025-04-14,,,,0.472,,,openai,,,,,,,,0.472,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.270008+00:00,,,,,,False,,False,0.0,False,0.0,0.472,0.472,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.472,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-1m,OpenAI-MRCR: 2 needle 1M,,,2025-07-19T19:56:15.282718+00:00,benchmark_result,,,,True,,,,,,1869.0,,gpt-4.1-mini-2025-04-14,,,,0.333,,,openai,,,,,,,,0.333,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.282718+00:00,,,,,,False,,False,0.0,False,0.0,0.333,0.333,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",OpenAI-MRCR: 2 needle 1M,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-Round Co-reference Resolution benchmark that tests an LLM's ability to distinguish between multiple similar needles hidden in long conversations. Models must reproduce specific instances of content (e.g., 'Return the 2nd poem about tapirs') from multi-turn synthetic conversations, requiring reasoning about context, ordering, and subtle differences between similar outputs.",0.333,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Internal methodology, see source footnote [2]",,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.852737+00:00,benchmark_result,,,,True,,,,,,1355.0,,gpt-4.1-mini-2025-04-14,,,,0.236,,,openai,,,,,,,,0.236,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:13.852737+00:00,,,,,,False,,False,0.0,False,0.0,0.236,0.236,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.236,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4])",,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.007636+00:00,benchmark_result,,,,True,,,,,,1776.0,,gpt-4.1-mini-2025-04-14,,,,0.36,,,openai,,,,,,,,0.36,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:15.007636+00:00,,,,,,False,,False,0.0,False,0.0,0.36,0.36,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.36,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4], GPT-4o user model)",,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.978528+00:00,benchmark_result,,,,True,,,,,,1762.0,,gpt-4.1-mini-2025-04-14,,,,0.558,,,openai,,,,,,,,0.558,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-19T19:56:14.978528+00:00,,,,,,False,,False,0.0,False,0.0,0.558,0.558,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,"GPT-4.1 mini provides a balance between intelligence, speed, and cost. It's a significant leap in small model performance, even beating GPT-4o in many benchmarks while reducing latency and cost.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.558,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.385924+00:00,benchmark_result,,,,True,,,,,,669.0,,gpt-4.1-nano-2025-04-14,,,,0.098,,,openai,,,,,,,,0.098,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.385924+00:00,,,,,,False,,False,0.0,False,0.0,0.098,0.098,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.098,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.804864+00:00,benchmark_result,,,,True,,,,,,1333.0,,gpt-4.1-nano-2025-04-14,,,,0.062,,,openai,,,,,,,,0.062,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.804864+00:00,,,,,,False,,False,0.0,False,0.0,0.062,0.062,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.062,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.016856+00:00,benchmark_result,,,,True,,,,,,484.0,,gpt-4.1-nano-2025-04-14,,,,0.294,,,openai,,,,,,,,0.294,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.016856+00:00,,,,,,False,,False,0.0,False,0.0,0.294,0.294,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.294,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,charxiv-d,CharXiv-D,,,2025-07-19T19:56:15.329021+00:00,benchmark_result,,,,True,,,,,,1888.0,,gpt-4.1-nano-2025-04-14,,,,0.739,,,openai,,,,,,,,0.739,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.329021+00:00,,,,,,False,,False,0.0,False,0.0,0.739,0.739,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,CharXiv-D,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",0.739,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,Standard benchmark,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.199274+00:00,benchmark_result,,,,True,,,,,,1836.0,,gpt-4.1-nano-2025-04-14,,,,0.405,,,openai,,,,,,,,0.405,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.199274+00:00,,,,,,False,,False,0.0,False,0.0,0.405,0.405,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.405,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,collie,COLLIE,,,2025-07-19T19:56:15.257208+00:00,benchmark_result,,,,True,,,,,,1858.0,,gpt-4.1-nano-2025-04-14,,,,0.425,,,openai,,,,,,,,0.425,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.257208+00:00,,,,,,False,,False,0.0,False,0.0,0.425,0.425,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.425,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.341699+00:00,benchmark_result,,,,True,,,,,,1893.0,,gpt-4.1-nano-2025-04-14,,,,0.057,,,openai,,,,,,,,0.057,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.341699+00:00,,,,,,False,,False,0.0,False,0.0,0.057,0.057,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.057,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.756178+00:00,benchmark_result,,,,True,,,,,,350.0,,gpt-4.1-nano-2025-04-14,,,,0.503,,,openai,,,,,,,,0.503,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.756178+00:00,,,,,,False,,False,0.0,False,0.0,0.503,0.503,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.503,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.291775+00:00,benchmark_result,,,,True,,,,,,1873.0,,gpt-4.1-nano-2025-04-14,,,,0.25,,,openai,,,,,,,,0.25,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.291775+00:00,,,,,,False,,False,0.0,False,0.0,0.25,0.25,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-bfs->128k,Graphwalks BFS >128k,,,2025-07-19T19:56:15.300453+00:00,benchmark_result,,,,True,,,,,,1876.0,,gpt-4.1-nano-2025-04-14,,,,0.029,,,openai,,,,,,,,0.029,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.300453+00:00,,,,,,False,,False,0.0,False,0.0,0.029,0.029,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Graphwalks BFS >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to perform breadth-first search (BFS) operations on graphs with context length over 128k tokens, testing long-context reasoning capabilities.",0.029,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.308330+00:00,benchmark_result,,,,True,,,,,,1879.0,,gpt-4.1-nano-2025-04-14,,,,0.094,,,openai,,,,,,,,0.094,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.308330+00:00,,,,,,False,,False,0.0,False,0.0,0.094,0.094,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Internal benchmark,,,graphwalks-parents->128k,Graphwalks parents >128k,,,2025-07-19T19:56:15.322097+00:00,benchmark_result,,,,True,,,,,,1885.0,,gpt-4.1-nano-2025-04-14,,,,0.056,,,openai,,,,,,,,0.056,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.322097+00:00,,,,,,False,,False,0.0,False,0.0,0.056,0.056,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Graphwalks parents >128k,"['reasoning', 'spatial_reasoning', 'long_context']",text,False,1.0,en,"A graph reasoning benchmark that evaluates language models' ability to find parent nodes in graphs with context length over 128k tokens, testing long-context reasoning and graph structure understanding.",0.056,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,ifeval,IFEval,,,2025-07-19T19:56:12.300562+00:00,benchmark_result,,,,True,,,,,,633.0,,gpt-4.1-nano-2025-04-14,,,,0.745,,,openai,,,,,,,,0.745,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.300562+00:00,,,,,,False,,False,0.0,False,0.0,0.745,0.745,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.745,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Internal benchmark,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.227248+00:00,benchmark_result,,,,True,,,,,,1846.0,,gpt-4.1-nano-2025-04-14,,,,0.316,,,openai,,,,,,,,0.316,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.227248+00:00,,,,,,False,,False,0.0,False,0.0,0.316,0.316,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.316,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,mathvista,MathVista,,,2025-07-19T19:56:12.117553+00:00,benchmark_result,,,,True,,,,,,541.0,,gpt-4.1-nano-2025-04-14,,,,0.562,,,openai,,,,,,,,0.562,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.117553+00:00,,,,,,False,,False,0.0,False,0.0,0.562,0.562,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.562,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,mmlu,MMLU,,,2025-07-19T19:56:11.319012+00:00,benchmark_result,,,,True,,,,,,118.0,,gpt-4.1-nano-2025-04-14,,,,0.801,,,openai,,,,,,,,0.801,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.319012+00:00,,,,,,False,,False,0.0,False,0.0,0.801,0.801,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.801,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.159419+00:00,benchmark_result,,,,True,,,,,,1482.0,,gpt-4.1-nano-2025-04-14,,,,0.669,,,openai,,,,,,,,0.669,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.159419+00:00,,,,,,False,,False,0.0,False,0.0,0.669,0.669,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.669,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Standard benchmark,,,mmmu,MMMU,,,2025-07-19T19:56:12.220951+00:00,benchmark_result,,,,True,,,,,,592.0,,gpt-4.1-nano-2025-04-14,,,,0.554,,,openai,,,,,,,,0.554,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.220951+00:00,,,,,,False,,False,0.0,False,0.0,0.554,0.554,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.554,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.645047+00:00,benchmark_result,,,,True,,,,,,1651.0,,gpt-4.1-nano-2025-04-14,,,,0.572,,,openai,,,,,,,,0.572,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.645047+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.572,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Standard benchmark (GPT-4o grader),,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.557571+00:00,benchmark_result,,,,True,,,,,,741.0,,gpt-4.1-nano-2025-04-14,,,,0.15,,,openai,,,,,,,,0.15,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.557571+00:00,,,,,,False,,False,0.0,False,0.0,0.15,0.15,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.15,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Standard benchmark (o3-mini grader, see footnote [3])",,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.241054+00:00,benchmark_result,,,,True,,,,,,1852.0,,gpt-4.1-nano-2025-04-14,,,,0.311,,,openai,,,,,,,,0.311,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.241054+00:00,,,,,,False,,False,0.0,False,0.0,0.311,0.311,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.311,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.272341+00:00,benchmark_result,,,,True,,,,,,1864.0,,gpt-4.1-nano-2025-04-14,,,,0.366,,,openai,,,,,,,,0.366,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.272341+00:00,,,,,,False,,False,0.0,False,0.0,0.366,0.366,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.366,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Internal benchmark,,,openai-mrcr:-2-needle-1m,OpenAI-MRCR: 2 needle 1M,,,2025-07-19T19:56:15.284545+00:00,benchmark_result,,,,True,,,,,,1870.0,,gpt-4.1-nano-2025-04-14,,,,0.12,,,openai,,,,,,,,0.12,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.284545+00:00,,,,,,False,,False,0.0,False,0.0,0.12,0.12,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,OpenAI-MRCR: 2 needle 1M,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-Round Co-reference Resolution benchmark that tests an LLM's ability to distinguish between multiple similar needles hidden in long conversations. Models must reproduce specific instances of content (e.g., 'Return the 2nd poem about tapirs') from multi-turn synthetic conversations, requiring reasoning about context, ordering, and subtle differences between similar outputs.",0.12,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4])",,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.011934+00:00,benchmark_result,,,,True,,,,,,1778.0,,gpt-4.1-nano-2025-04-14,,,,0.14,,,openai,,,,,,,,0.14,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.011934+00:00,,,,,,False,,False,0.0,False,0.0,0.14,0.14,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.14,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,"Avg 5 runs, no custom tools/prompting (footnote [4], GPT-4o user model)",,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.982239+00:00,benchmark_result,,,,True,,,,,,1764.0,,gpt-4.1-nano-2025-04-14,,,,0.226,,,openai,,,,,,,,0.226,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.982239+00:00,,,,,,False,,False,0.0,False,0.0,0.226,0.226,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.1 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-14,2025.0,4.0,2025-04,Undisclosed,GPT-4.1 nano is OpenAI's fastest and cheapest model available in the GPT-4.1 family. It delivers exceptional performance at a small size with its 1 million token context window. Ideal for tasks like classification or autocompletion.,TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.226,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.811839+00:00,benchmark_result,,,,True,,,,,,1337.0,,gpt-4.5,,,,0.449,,,openai,,,,,,,,0.449,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.811839+00:00,,,,,,False,,False,0.0,False,0.0,0.449,0.449,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.449,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.024273+00:00,benchmark_result,,,,True,,,,,,489.0,,gpt-4.5,,,,0.367,,,openai,,,,,,,,0.367,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.024273+00:00,,,,,,False,,False,0.0,False,0.0,0.367,0.367,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.367,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,charxiv-d,CharXiv-D,,,2025-07-19T19:56:15.335527+00:00,benchmark_result,,,,True,,,,,,1891.0,,gpt-4.5,,,,0.9,,,openai,,,,,,,,0.9,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.335527+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",CharXiv-D,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",0.9,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),gpt
,Accuracy,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.204875+00:00,benchmark_result,,,,True,,,,,,1839.0,,gpt-4.5,,,,0.554,,,openai,,,,,,,,0.554,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.204875+00:00,,,,,,False,,False,0.0,False,0.0,0.554,0.554,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.554,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,collie,COLLIE,,,2025-07-19T19:56:15.265565+00:00,benchmark_result,,,,True,,,,,,1862.0,,gpt-4.5,,,,0.723,,,openai,,,,,,,,0.723,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.265565+00:00,,,,,,False,,False,0.0,False,0.0,0.723,0.723,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.723,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.351430+00:00,benchmark_result,,,,True,,,,,,1897.0,,gpt-4.5,,,,0.63,,,openai,,,,,,,,0.63,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.351430+00:00,,,,,,False,,False,0.0,False,0.0,0.63,0.63,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.63,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy (Diamond),,,gpqa,GPQA,,,2025-07-19T19:56:11.767414+00:00,benchmark_result,,,,True,,,,,,357.0,,gpt-4.5,,,,0.695,,,openai,,,,,,,,0.695,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.767414+00:00,,,,,,False,,False,0.0,False,0.0,0.695,0.695,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.695,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.372855+00:00,benchmark_result,,,,True,,,,,,1906.0,,gpt-4.5,,,,0.723,,,openai,,,,,,,,0.723,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.372855+00:00,,,,,,False,,False,0.0,False,0.0,0.723,0.723,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Accuracy,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.315697+00:00,benchmark_result,,,,True,,,,,,1883.0,,gpt-4.5,,,,0.726,,,openai,,,,,,,,0.726,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.315697+00:00,,,,,,False,,False,0.0,False,0.0,0.726,0.726,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Answer accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.114869+00:00,benchmark_result,,,,True,,,,,,1015.0,,gpt-4.5,,,,0.97,,,openai,,,,,,,,0.97,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:13.114869+00:00,,,,,,False,,False,0.0,False,0.0,0.97,0.97,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.97,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.694244+00:00,benchmark_result,,,,True,,,,,,813.0,,gpt-4.5,,,,0.88,,,openai,,,,,,,,0.88,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:12.694244+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.88,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,ifeval,IFEval,,,2025-07-19T19:56:12.307682+00:00,benchmark_result,,,,True,,,,,,637.0,,gpt-4.5,,,,0.882,,,openai,,,,,,,,0.882,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.307682+00:00,,,,,,False,,False,0.0,False,0.0,0.882,0.882,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.882,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.234022+00:00,benchmark_result,,,,True,,,,,,1850.0,,gpt-4.5,,,,0.54,,,openai,,,,,,,,0.54,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.234022+00:00,,,,,,False,,False,0.0,False,0.0,0.54,0.54,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.54,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.124115+00:00,benchmark_result,,,,True,,,,,,545.0,,gpt-4.5,,,,0.723,,,openai,,,,,,,,0.723,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.124115+00:00,,,,,,False,,False,0.0,False,0.0,0.723,0.723,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.723,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,Multiple-choice accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.328688+00:00,benchmark_result,,,,True,,,,,,124.0,,gpt-4.5,,,,0.908,,,openai,,,,,,,,0.908,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.328688+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.908,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Accuracy,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.164320+00:00,benchmark_result,,,,True,,,,,,1485.0,,gpt-4.5,,,,0.851,,,openai,,,,,,,,0.851,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.164320+00:00,,,,,,False,,False,0.0,False,0.0,0.851,0.851,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.851,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.226731+00:00,benchmark_result,,,,True,,,,,,595.0,,gpt-4.5,,,,0.752,,,openai,,,,,,,,0.752,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.226731+00:00,,,,,,False,,False,0.0,False,0.0,0.752,0.752,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.752,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.652033+00:00,benchmark_result,,,,True,,,,,,1655.0,,gpt-4.5,,,,0.708,,,openai,,,,,,,,0.708,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.652033+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.708,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.563438+00:00,benchmark_result,,,,True,,,,,,744.0,,gpt-4.5,,,,0.438,,,openai,,,,,,,,0.438,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.563438+00:00,,,,,,False,,False,0.0,False,0.0,0.438,0.438,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.438,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.249385+00:00,benchmark_result,,,,True,,,,,,1856.0,,gpt-4.5,,,,0.501,,,openai,,,,,,,,0.501,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.249385+00:00,,,,,,False,,False,0.0,False,0.0,0.501,0.501,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.501,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.279311+00:00,benchmark_result,,,,True,,,,,,1868.0,,gpt-4.5,,,,0.385,,,openai,,,,,,,,0.385,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.279311+00:00,,,,,,False,,False,0.0,False,0.0,0.385,0.385,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.385,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.559622+00:00,benchmark_result,,,,True,,,,,,240.0,,gpt-4.5,,,,0.625,,,openai,,,,,,,,0.625,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:11.559622+00:00,,,,,,False,,False,0.0,False,0.0,0.625,0.625,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.625,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Success rate,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.863719+00:00,benchmark_result,,,,True,,,,,,1360.0,,gpt-4.5,,,,0.38,,,openai,,,,,,,,0.38,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.863719+00:00,,,,,,False,,False,0.0,False,0.0,0.38,0.38,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.38,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Success rate ($186K equivalent),,,swe-lancer,SWE-Lancer,,,2025-07-19T19:56:15.358579+00:00,benchmark_result,,,,True,,,,,,1900.0,,gpt-4.5,,,,0.373,,,openai,,,,,,,,0.373,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.358579+00:00,,,,,,False,,False,0.0,False,0.0,0.373,0.373,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",SWE-Lancer,"['reasoning', 'code']",text,False,1.0,en,"A benchmark for evaluating large language models on real-world freelance software engineering tasks from Upwork. Contains over 1,400 tasks valued at $1 million USD total, ranging from $50 bug fixes to $32,000 feature implementations. Includes both independent engineering tasks graded via end-to-end tests and managerial tasks assessed against original engineering managers' choices.",0.373,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Success rate ($41K equivalent),,,swe-lancer-(ic-diamond-subset),SWE-Lancer (IC-Diamond subset),,,2025-07-19T19:56:15.365353+00:00,benchmark_result,,,,True,,,,,,1903.0,,gpt-4.5,,,,0.174,,,openai,,,,,,,,0.174,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.365353+00:00,,,,,,False,,False,0.0,False,0.0,0.174,0.174,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",SWE-Lancer (IC-Diamond subset),"['reasoning', 'code']",text,False,1.0,en,"SWE-Lancer (IC-Diamond subset) is a benchmark of real-world freelance software engineering tasks from Upwork, ranging from $50 bug fixes to $32,000 feature implementations. It evaluates AI models on independent engineering tasks using end-to-end tests triple-verified by experienced software engineers, and includes managerial tasks where models choose between technical implementation proposals.",0.174,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.020093+00:00,benchmark_result,,,,True,,,,,,1782.0,,gpt-4.5,,,,0.5,,,openai,,,,,,,,0.5,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.020093+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.5,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.989887+00:00,benchmark_result,,,,True,,,,,,1768.0,,gpt-4.5,,,,0.684,,,openai,,,,,,,,0.684,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.989887+00:00,,,,,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4.5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-27,2025.0,2.0,2025-02,Undisclosed,"GPT-4.5 is OpenAI's most advanced model, offering improved reasoning, coding, and creative capabilities with faster performance and longer context handling than GPT-4. It features enhanced instruction following, reduced hallucinations, and better factual accuracy.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.684,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,F1 Score,,,drop,DROP,,,2025-07-19T19:56:13.023727+00:00,benchmark_result,,,,True,,,,,,962.0,,gpt-4o-2024-05-13,,,,0.834,,,openai,,,,,,,,0.834,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:13.023727+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.834,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.759539+00:00,benchmark_result,,,,True,,,,,,352.0,,gpt-4o-2024-05-13,,,,0.536,,,openai,,,,,,,,0.536,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:11.759539+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.536,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.689969+00:00,benchmark_result,,,,True,,,,,,811.0,,gpt-4o-2024-05-13,,,,0.902,,,openai,,,,,,,,0.902,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:12.689969+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.902,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.903446+00:00,benchmark_result,,,,True,,,,,,427.0,,gpt-4o-2024-05-13,,,,0.766,,,openai,,,,,,,,0.766,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:11.903446+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.766,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.119289+00:00,benchmark_result,,,,True,,,,,,542.0,,gpt-4o-2024-05-13,,,,0.638,,,openai,,,,,,,,0.638,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:12.119289+00:00,,,,,,False,,False,0.0,False,0.0,0.638,0.638,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.638,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.714155+00:00,benchmark_result,,,,True,,,,,,1297.0,,gpt-4o-2024-05-13,,,,0.905,,,openai,,,,,,,,0.905,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:13.714155+00:00,,,,,,False,,False,0.0,False,0.0,0.905,0.905,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.905,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.322163+00:00,benchmark_result,,,,True,,,,,,120.0,,gpt-4o-2024-05-13,,,,0.887,,,openai,,,,,,,,0.887,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:11.322163+00:00,,,,,,False,,False,0.0,False,0.0,0.887,0.887,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.887,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.515262+00:00,benchmark_result,,,,True,,,,,,219.0,,gpt-4o-2024-05-13,,,,0.726,,,openai,,,,,,,,0.726,https://openai.com/blog/gpt-4o,,,,,,,,2025-07-19T19:56:11.515262+00:00,,,,,,False,,False,0.0,False,0.0,0.726,0.726,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-05-13,2024.0,5.0,2024-05,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.726,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,test set evaluation,,,activitynet,ActivityNet,,,2025-07-19T19:56:15.381219+00:00,benchmark_result,,,,True,,,,,,1908.0,,gpt-4o-2024-08-06,,,,0.619,,,openai,,,,,,,,0.619,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:15.381219+00:00,,,,,,False,,False,0.0,False,0.0,0.619,0.619,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",ActivityNet,"['vision', 'video']",video,False,1.0,en,"A large-scale video benchmark for human activity understanding. Provides samples from 203 activity classes with an average of 137 untrimmed videos per class and 1.41 activity instances per video, for a total of 849 video hours. The benchmark covers a wide range of complex human activities that are of interest to people in their daily living and can be used to compare algorithms for three scenarios: untrimmed video classification, trimmed activity classification, and activity detection.",0.619,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gpt
,test set evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.646808+00:00,benchmark_result,,,,True,,,,,,1262.0,,gpt-4o-2024-08-06,,,,0.942,,,openai,,,,,,,,0.942,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:13.646808+00:00,,,,,,False,,False,0.0,False,0.0,0.942,0.942,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.942,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),gpt
,Accuracy,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.391433+00:00,benchmark_result,,,,True,,,,,,672.0,,gpt-4o-2024-08-06,,,,0.307,,,openai,,,,,,,,0.307,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.391433+00:00,,,,,,False,,False,0.0,False,0.0,0.307,0.307,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.307,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.810263+00:00,benchmark_result,,,,True,,,,,,1336.0,,gpt-4o-2024-08-06,,,,0.182,,,openai,,,,,,,,0.182,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.810263+00:00,,,,,,False,,False,0.0,False,0.0,0.182,0.182,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.182,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.022775+00:00,benchmark_result,,,,True,,,,,,488.0,,gpt-4o-2024-08-06,,,,0.131,,,openai,,,,,,,,0.131,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.022775+00:00,,,,,,False,,False,0.0,False,0.0,0.131,0.131,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.131,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,test set evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.824155+00:00,benchmark_result,,,,True,,,,,,875.0,,gpt-4o-2024-08-06,,,,0.857,,,openai,,,,,,,,0.857,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:12.824155+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.857,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,charxiv-d,CharXiv-D,,,2025-07-19T19:56:15.333294+00:00,benchmark_result,,,,True,,,,,,1890.0,,gpt-4o-2024-08-06,,,,0.853,,,openai,,,,,,,,0.853,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.333294+00:00,,,,,,False,,False,0.0,False,0.0,0.853,0.853,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",CharXiv-D,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-D is the descriptive questions subset of the CharXiv benchmark, designed to assess multimodal large language models' ability to extract basic information from scientific charts. It contains descriptive questions covering information extraction, enumeration, pattern recognition, and counting across 2,323 diverse charts from arXiv papers, all curated and verified by human experts.",0.853,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-4o without thinking mode - Scientific figure reasoning and interpretation.,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.203285+00:00,benchmark_result,,,,True,,,,,,1838.0,,gpt-4o-2024-08-06,,,,0.588,,,openai,,,,,,,,0.588,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:15.203285+00:00,,,,,,False,,False,0.0,False,0.0,0.588,0.588,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.588,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,GPT-4o without thinking mode - Instruction-following in freeform writing.,,,collie,COLLIE,,,2025-07-19T19:56:15.262884+00:00,benchmark_result,,,,True,,,,,,1861.0,,gpt-4o-2024-08-06,,,,0.61,,,openai,,,,,,,,0.61,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:15.262884+00:00,,,,,,False,,False,0.0,False,0.0,0.61,0.61,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.61,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.349679+00:00,benchmark_result,,,,True,,,,,,1896.0,,gpt-4o-2024-08-06,,,,0.665,,,openai,,,,,,,,0.665,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.349679+00:00,,,,,,False,,False,0.0,False,0.0,0.665,0.665,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.665,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,test set evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.873722+00:00,benchmark_result,,,,True,,,,,,900.0,,gpt-4o-2024-08-06,,,,0.928,,,openai,,,,,,,,0.928,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:12.873722+00:00,,,,,,False,,False,0.0,False,0.0,0.928,0.928,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.928,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),gpt
,test set evaluation,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.935728+00:00,benchmark_result,,,,True,,,,,,926.0,,gpt-4o-2024-08-06,,,,0.722,,,openai,,,,,,,,0.722,https://openai.com/index/hello-gpt-4o/,,,,,,,,2025-07-19T19:56:12.935728+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.722,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,GPT-4o without thinking mode - Multimodal spatial reasoning.,,,erqa,ERQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1872.0,,gpt-4o-2024-08-06,,,,0.352,,,openai,,,,,,,,0.352,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.352,0.352,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",ERQA,"['vision', 'reasoning', 'spatial_reasoning']",multimodal,False,1.0,en,"Embodied Reasoning Question Answering benchmark consisting of 400 multiple-choice visual questions across spatial reasoning, trajectory reasoning, action reasoning, state estimation, and multi-view reasoning for evaluating AI capabilities in physical world interactions",0.352,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,GPT-4o - Diamond no thinking no tools,,,gpqa,GPQA,,,2025-07-19T19:56:11.764329+00:00,benchmark_result,,,,True,,,,,,355.0,,gpt-4o-2024-08-06,,,,0.701,,,openai,,,,,,,,0.701,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:11.764329+00:00,,,,,,False,,False,0.0,False,0.0,0.701,0.701,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.701,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.370259+00:00,benchmark_result,,,,True,,,,,,1905.0,,gpt-4o-2024-08-06,,,,0.417,,,openai,,,,,,,,0.417,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.370259+00:00,,,,,,False,,False,0.0,False,0.0,0.417,0.417,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Accuracy,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.314044+00:00,benchmark_result,,,,True,,,,,,1882.0,,gpt-4o-2024-08-06,,,,0.354,,,openai,,,,,,,,0.354,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.314044+00:00,,,,,,False,,False,0.0,False,0.0,0.354,0.354,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,GPT-4o without thinking mode (no tools) - Full set of expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2003.0,,gpt-4o-2024-08-06,,,,0.053,,,openai,,,,,,,,0.053,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.053,0.053,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.053,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,ifeval,IFEval,,,2025-07-19T19:56:12.306083+00:00,benchmark_result,,,,True,,,,,,636.0,,gpt-4o-2024-08-06,,,,0.81,,,openai,,,,,,,,0.81,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.306083+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.81,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.232334+00:00,benchmark_result,,,,True,,,,,,1849.0,,gpt-4o-2024-08-06,,,,0.292,,,openai,,,,,,,,0.292,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.232334+00:00,,,,,,False,,False,0.0,False,0.0,0.292,0.292,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.292,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.122558+00:00,benchmark_result,,,,True,,,,,,544.0,,gpt-4o-2024-08-06,,,,0.614,,,openai,,,,,,,,0.614,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.122558+00:00,,,,,,False,,False,0.0,False,0.0,0.614,0.614,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.614,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.325082+00:00,benchmark_result,,,,True,,,,,,122.0,,gpt-4o-2024-08-06,,,,0.857,,,openai,,,,,,,,0.857,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.325082+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.857,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.517058+00:00,benchmark_result,,,,True,,,,,,220.0,,gpt-4o-2024-08-06,,,,0.747,,,openai,,,,,,,,0.747,https://huggingface.co/spaces/TIGER-Lab/MMLU-Pro,,,,,,,,2025-07-19T19:56:11.517058+00:00,,,,,,False,,False,0.0,False,0.0,0.747,0.747,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.747,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.162717+00:00,benchmark_result,,,,True,,,,,,1484.0,,gpt-4o-2024-08-06,,,,0.814,,,openai,,,,,,,,0.814,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.162717+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.814,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-4o without thinking mode - College-level visual problem-solving with multimodal reasoning.,,,mmmu,MMMU,,,2025-07-19T19:56:12.224513+00:00,benchmark_result,,,,True,,,,,,594.0,,gpt-4o-2024-08-06,,,,0.722,,,openai,,,,,,,,0.722,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:12.224513+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.722,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,GPT-4o without thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.,,,mmmu-pro,MMMU-Pro,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1870.0,,gpt-4o-2024-08-06,,,,0.599,,,openai,,,,,,,,0.599,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.599,0.599,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.599,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.650416+00:00,benchmark_result,,,,True,,,,,,1654.0,,gpt-4o-2024-08-06,,,,0.609,,,openai,,,,,,,,0.609,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.650416+00:00,,,,,,False,,False,0.0,False,0.0,0.609,0.609,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.609,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Accuracy,,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.246431+00:00,benchmark_result,,,,True,,,,,,1855.0,,gpt-4o-2024-08-06,,,,0.399,,,openai,,,,,,,,0.399,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.246431+00:00,,,,,,False,,False,0.0,False,0.0,0.399,0.399,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.399,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.277538+00:00,benchmark_result,,,,True,,,,,,1867.0,,gpt-4o-2024-08-06,,,,0.319,,,openai,,,,,,,,0.319,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.277538+00:00,,,,,,False,,False,0.0,False,0.0,0.319,0.319,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.319,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4o without thinking mode - Multi-turn instruction following benchmark.,,,scale-multichallenge,Scale MultiChallenge,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2005.0,,gpt-4o-2024-08-06,,,,0.403,,,openai,,,,,,,,0.403,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.403,0.403,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Scale MultiChallenge,"['reasoning', 'communication', 'general']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark developed by Scale AI that evaluates large language models on four challenging conversation categories: instruction retention, inference memory of user information, reliable versioned editing, and self-coherence. Each challenge requires accurate instruction-following, context allocation, and in-context reasoning. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge.",0.403,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.557852+00:00,benchmark_result,,,,True,,,,,,239.0,,gpt-4o-2024-08-06,,,,0.382,,,openai,,,,,,,,0.382,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:11.557852+00:00,,,,,,False,,False,0.0,False,0.0,0.382,0.382,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.382,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.861280+00:00,benchmark_result,,,,True,,,,,,1359.0,,gpt-4o-2024-08-06,,,,0.332,,,openai,,,,,,,,0.332,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.861280+00:00,,,,,,False,,False,0.0,False,0.0,0.332,0.332,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.332,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,percentage score,,,swe-lancer,SWE-Lancer,,,2025-07-19T19:56:15.356738+00:00,benchmark_result,,,,True,,,,,,1899.0,,gpt-4o-2024-08-06,,,,0.326,,,openai,,,,,,,,0.326,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.356738+00:00,,,,,,False,,False,0.0,False,0.0,0.326,0.326,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",SWE-Lancer,"['reasoning', 'code']",text,False,1.0,en,"A benchmark for evaluating large language models on real-world freelance software engineering tasks from Upwork. Contains over 1,400 tasks valued at $1 million USD total, ranging from $50 bug fixes to $32,000 feature implementations. Includes both independent engineering tasks graded via end-to-end tests and managerial tasks assessed against original engineering managers' choices.",0.326,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,percentage score,,,swe-lancer-(ic-diamond-subset),SWE-Lancer (IC-Diamond subset),,,2025-07-19T19:56:15.363614+00:00,benchmark_result,,,,True,,,,,,1902.0,,gpt-4o-2024-08-06,,,,0.124,,,openai,,,,,,,,0.124,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.363614+00:00,,,,,,False,,False,0.0,False,0.0,0.124,0.124,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",SWE-Lancer (IC-Diamond subset),"['reasoning', 'code']",text,False,1.0,en,"SWE-Lancer (IC-Diamond subset) is a benchmark of real-world freelance software engineering tasks from Upwork, ranging from $50 bug fixes to $32,000 feature implementations. It evaluates AI models on independent engineering tasks using end-to-end tests triple-verified by experienced software engineers, and includes managerial tasks where models choose between technical implementation proposals.",0.124,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.017725+00:00,benchmark_result,,,,True,,,,,,1781.0,,gpt-4o-2024-08-06,,,,0.428,,,openai,,,,,,,,0.428,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.017725+00:00,,,,,,False,,False,0.0,False,0.0,0.428,0.428,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.428,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.988086+00:00,benchmark_result,,,,True,,,,,,1767.0,,gpt-4o-2024-08-06,,,,0.603,,,openai,,,,,,,,0.603,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.988086+00:00,,,,,,False,,False,0.0,False,0.0,0.603,0.603,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.603,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,GPT-4o without thinking mode - Function calling benchmark (airline domain).,,,tau2-airline,Tau2 airline,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1867.0,,gpt-4o-2024-08-06,,,,0.455,,,openai,,,,,,,,0.455,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.455,0.455,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.455,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4o without thinking mode - Function calling benchmark (retail domain).,,,tau2-retail,Tau2 retail,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1868.0,,gpt-4o-2024-08-06,,,,0.634,,,openai,,,,,,,,0.634,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.634,0.634,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.634,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,GPT-4o without thinking mode - Function calling benchmark (telecom domain).,,,tau2-telecom,Tau2 telecom,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1869.0,,gpt-4o-2024-08-06,,,,0.235,,,openai,,,,,,,,0.235,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.235,0.235,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.235,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-4o without thinking mode - Video-based multimodal reasoning (max frame 256).,,,videommmu,VideoMMMU,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,1871.0,,gpt-4o-2024-08-06,,,,0.612,,,openai,,,,,,,,0.612,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.612,0.612,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-4o,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-06,2024.0,8.0,2024-08,Undisclosed,"GPT-4o ('o' for 'omni') is a multimodal AI model that accepts text, audio, image, and video inputs, and generates text, audio, and image outputs. It matches GPT-4 Turbo performance on text and code, with improvements in non-English languages, vision, and audio understanding.",VideoMMMU,"['multimodal', 'vision', 'reasoning']",multimodal,False,1.0,en,"Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",0.612,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gpt
,F1 Score,,,drop,DROP,,,2025-07-19T19:56:13.026741+00:00,benchmark_result,,,,True,,,,,,964.0,,gpt-4o-mini-2024-07-18,,,,0.797,,,openai,,,,,,,,0.797,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:13.026741+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.797,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.774361+00:00,benchmark_result,,,,True,,,,,,361.0,,gpt-4o-mini-2024-07-18,,,,0.402,,,openai,,,,,,,,0.402,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:11.774361+00:00,,,,,,False,,False,0.0,False,0.0,0.402,0.402,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.402,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.700095+00:00,benchmark_result,,,,True,,,,,,816.0,,gpt-4o-mini-2024-07-18,,,,0.872,,,openai,,,,,,,,0.872,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:12.700095+00:00,,,,,,False,,False,0.0,False,0.0,0.872,0.872,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.872,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.911917+00:00,benchmark_result,,,,True,,,,,,431.0,,gpt-4o-mini-2024-07-18,,,,0.702,,,openai,,,,,,,,0.702,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:11.911917+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.702,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.128984+00:00,benchmark_result,,,,True,,,,,,548.0,,gpt-4o-mini-2024-07-18,,,,0.567,,,openai,,,,,,,,0.567,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:12.128984+00:00,,,,,,False,,False,0.0,False,0.0,0.567,0.567,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.567,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),gpt
,Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.720445+00:00,benchmark_result,,,,True,,,,,,1301.0,,gpt-4o-mini-2024-07-18,,,,0.87,,,openai,,,,,,,,0.87,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:13.720445+00:00,,,,,,False,,False,0.0,False,0.0,0.87,0.87,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.87,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.335061+00:00,benchmark_result,,,,True,,,,,,128.0,,gpt-4o-mini-2024-07-18,,,,0.82,,,openai,,,,,,,,0.82,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:11.335061+00:00,,,,,,False,,False,0.0,False,0.0,0.82,0.82,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.82,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.232157+00:00,benchmark_result,,,,True,,,,,,598.0,,gpt-4o-mini-2024-07-18,,,,0.594,,,openai,,,,,,,,0.594,https://openai.com/blog/gpt-4o-mini-announcement,,,,,,,,2025-07-19T19:56:12.232157+00:00,,,,,,False,,False,0.0,False,0.0,0.594,0.594,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.594,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Pass Rate,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.870038+00:00,benchmark_result,,,,True,,,,,,1363.0,,gpt-4o-mini-2024-07-18,,,,0.087,,,openai,,,,,,,,0.087,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.870038+00:00,,,,,,False,,False,0.0,False,0.0,0.087,0.087,Unknown,,,,,Undisclosed,Poor (<60%),GPT-4o mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-07-18,2024.0,7.0,2024-07,Undisclosed,"GPT-4o mini is OpenAI's latest cost-efficient small model, designed to make AI intelligence more accessible and affordable. It excels in textual intelligence and multimodal reasoning, outperforming previous models like GPT-3.5 Turbo. With a context window of 128K tokens and support for text and vision, it offers low-cost, real-time applications such as customer support chatbots. Priced at 15 cents per million input tokens and 60 cents per million output tokens, it is significantly cheaper than its predecessors. Safety is prioritized with built-in measures and improved resistance to security threats.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.087,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Thinking mode enabled (up to 128K tokens) with step-by-step reasoning and multi-language code understanding.,,,aider-polyglot,Aider-Polyglot,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9004.0,,gpt-5-2025-08-07,,,,0.88,,,openai,,,,,,,,0.88,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.88,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 standard with thinking mode enabled (no tools) - competition mathematics.,,,aime-2025,AIME 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9020.0,,gpt-5-2025-08-07,,,,0.946,,,openai,,,,,,,,0.946,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.946,0.946,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.946,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,GPT-5 with thinking mode enabled - Agentic search & browsing benchmark.,,,browsecomp,BrowseComp,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9043.0,,gpt-5-2025-08-07,,,,0.549,,,openai,,,,,,,,0.549,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.549,0.549,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.549,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,BrowseComp long-context 128k variant.,,,browsecomp-long-128k,BrowseComp Long Context 128k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10052.0,,gpt-5-2025-08-07,,,,0.9,,,openai,,,,,,,,0.9,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",BrowseComp Long Context 128k,"['reasoning', 'search']",text,False,1.0,en,"A challenging benchmark for evaluating web browsing agents' ability to persistently navigate the internet and find hard-to-locate, entangled information. Comprises 1,266 questions requiring strategic reasoning, creative search, and interpretation of retrieved content, with short and easily verifiable answers.",0.9,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,BrowseComp long-context 256k variant.,,,browsecomp-long-256k,BrowseComp Long Context 256k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10053.0,,gpt-5-2025-08-07,,,,0.888,,,openai,,,,,,,,0.888,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.888,0.888,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",BrowseComp Long Context 256k,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark for measuring the ability of agents to browse the web, comprising 1,266 questions that require persistently navigating the internet in search of hard-to-find, entangled information. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers. The benchmark focuses on questions where answers are obscure, time-invariant, and well-supported by evidence scattered across the open web.",0.888,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode - Scientific figure reasoning and interpretation.,,,charxiv-r,CharXiv-R,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9057.0,,gpt-5-2025-08-07,,,,0.811,,,openai,,,,,,,,0.811,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.811,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode enabled - Instruction-following in freeform writing.,,,collie,COLLIE,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9045.0,,gpt-5-2025-08-07,,,,0.99,,,openai,,,,,,,,0.99,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.99,0.99,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.99,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,GPT-5 with thinking mode - Multimodal spatial reasoning.,,,erqa,ERQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9059.0,,gpt-5-2025-08-07,,,,0.657,,,openai,,,,,,,,0.657,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.657,0.657,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",ERQA,"['vision', 'reasoning', 'spatial_reasoning']",multimodal,False,1.0,en,"Embodied Reasoning Question Answering benchmark consisting of 400 multiple-choice visual questions across spatial reasoning, trajectory reasoning, action reasoning, state estimation, and multi-view reasoning for evaluating AI capabilities in physical world interactions",0.657,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),gpt
,Thinking mode enabled for factual accuracy assessment. Measured hallucination rate on open-source prompts.,,,factscore,FactScore,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10071.0,,gpt-5-2025-08-07,,,,0.01,,,openai,,,,,,,,0.01,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.01,0.01,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",FActScore,['reasoning'],text,False,1.0,en,"A fine-grained atomic evaluation metric for factual precision in long-form text generation that breaks generated text into atomic facts and computes the percentage supported by reliable knowledge sources, with automated assessment using retrieval and language models",0.01,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 standard with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.,,,frontiermath,FrontierMath,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9024.0,,gpt-5-2025-08-07,,,,0.263,,,openai,,,,,,,,0.263,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.263,0.263,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.263,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 - Diamond thinking no tools,,,gpqa,GPQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9032.0,,gpt-5-2025-08-07,,,,0.857,,,openai,,,,,,,,0.857,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.857,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Graphwalks BFS (<128k) long-context reasoning.,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10050.0,,gpt-5-2025-08-07,,,,0.783,,,openai,,,,,,,,0.783,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.783,0.783,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Graphwalks parents (<128k) long-context reasoning.,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10051.0,,gpt-5-2025-08-07,,,,0.733,,,openai,,,,,,,,0.733,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,gpt
,Thinking mode enabled for medical hallucination detection. Measured inaccuracies on challenging healthcare conversations.,,,healthbench-hard,HealthBench Hard,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9013.0,,gpt-5-2025-08-07,,,,0.016,,,openai,,,,,,,,0.016,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.016,0.016,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",HealthBench Hard,['healthcare'],text,False,1.0,en,"A challenging variation of HealthBench that evaluates large language models' performance and safety in healthcare through 5,000 multi-turn conversations with particularly rigorous evaluation criteria validated by 262 physicians from 60 countries",0.016,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 standard with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.,,,hmmt-2025,HMMT 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9028.0,,gpt-5-2025-08-07,,,,0.933,,,openai,,,,,,,,0.933,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.933,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Code generation benchmark with function completion tasks in Python.,,,humaneval,HumanEval,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9007.0,,gpt-5-2025-08-07,,,,0.934,,,openai,,,,,,,,0.934,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.934,0.934,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.934,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,GPT-5 standard with thinking mode (no tools) - Full set of expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9037.0,,gpt-5-2025-08-07,,,,0.248,,,openai,,,,,,,,0.248,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.248,0.248,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.248,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 - Internal API instruction following evaluation (hard difficulty).,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10035.0,,gpt-5-2025-08-07,,,,0.64,,,openai,,,,,,,,0.64,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.64,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Thinking mode enabled for hallucination detection. Measured on open-source prompts for concept-based factual queries.,,,longfact-concepts,LongFact-Concepts,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10069.0,,gpt-5-2025-08-07,,,,0.007,,,openai,,,,,,,,0.007,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.007,0.007,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",LongFact Concepts,"['general', 'reasoning']",text,False,1.0,en,"LongFact is a benchmark for evaluating long-form factuality in large language models. It comprises 2,280 fact-seeking prompts spanning 38 topics, designed to test a model's ability to generate accurate, long-form responses. The benchmark uses SAFE (Search-Augmented Factuality Evaluator) to evaluate factual accuracy.",0.007,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Thinking mode enabled for hallucination detection. Measured on open-source prompts for object-based factual queries.,,,longfact-objects,LongFact-Objects,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10070.0,,gpt-5-2025-08-07,,,,0.008,,,openai,,,,,,,,0.008,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.008,0.008,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",LongFact Objects,"['general', 'reasoning']",text,False,1.0,en,"LongFact is a benchmark for evaluating long-form factuality in large language models. It comprises 2,280 fact-seeking prompts spanning 38 topics, designed to test a model's ability to generate accurate, long-form responses. The benchmark uses SAFE (Search-Augmented Factuality Evaluator) to evaluate factual accuracy.",0.008,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Thinking mode enabled with step-by-step mathematical problem solving and verification.,,,math,MATH,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9008.0,,gpt-5-2025-08-07,,,,0.847,,,openai,,,,,,,,0.847,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.847,0.847,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.847,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Standard benchmark across multiple academic subjects with comprehensive knowledge evaluation.,,,mmlu,MMLU,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9006.0,,gpt-5-2025-08-07,,,,0.925,,,openai,,,,,,,,0.925,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.925,0.925,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.925,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,GPT-5 with thinking mode - College-level visual problem-solving with multimodal reasoning.,,,mmmu,MMMU,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9009.0,,gpt-5-2025-08-07,,,,0.842,,,openai,,,,,,,,0.842,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.842,0.842,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.842,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.,,,mmmu-pro,MMMU-Pro,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9053.0,,gpt-5-2025-08-07,,,,0.784,,,openai,,,,,,,,0.784,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.784,0.784,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.784,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),gpt
,GPT-5 with o3-mini grader - Multi-turn instruction following benchmark with improved grading accuracy.,,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10034.0,,gpt-5-2025-08-07,,,,0.696,,,openai,,,,,,,,0.696,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.696,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,OpenAI-MRCR 2-needle retrieval at 128k tokens.,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10048.0,,gpt-5-2025-08-07,,,,0.952,,,openai,,,,,,,,0.952,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.952,0.952,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.952,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,OpenAI-MRCR 2-needle retrieval at 256k tokens.,,,openai-mrcr:-2-needle-256k,OpenAI-MRCR: 2 needle 256k,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10049.0,,gpt-5-2025-08-07,,,,0.868,,,openai,,,,,,,,0.868,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",OpenAI-MRCR: 2 needle 256k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-Round Co-reference Resolution (MRCR) benchmark that tests long-context reasoning by evaluating a model's ability to distinguish between similar outputs, reason about ordering, and reproduce specific content from multi-turn conversations containing multiple writing requests on overlapping topics at 256k tokens.",0.868,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode enabled - Multi-turn instruction following benchmark.,,,scale-multichallenge,Scale MultiChallenge,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9041.0,,gpt-5-2025-08-07,,,,0.696,,,openai,,,,,,,,0.696,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Scale MultiChallenge,"['reasoning', 'communication', 'general']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark developed by Scale AI that evaluates large language models on four challenging conversation categories: instruction retention, inference memory of user information, reliable versioned editing, and self-coherence. Each challenge requires accurate instruction-following, context allocation, and in-context reasoning. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge.",0.696,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Thinking mode enabled (up to 128K tokens) with enhanced reasoning capabilities and iterative problem-solving approach.,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9002.0,,gpt-5-2025-08-07,,,,0.749,,,openai,,,,,,,,0.749,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.749,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,GPT-5 - IC SWE Diamond Freelance Coding Tasks (earnings-based evaluation).,,,swe-lancer-(ic-diamond-subset),SWE-Lancer (IC-Diamond subset),,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10027.0,,gpt-5-2025-08-07,,,,1.0,,,openai,,,,,,,,1.0,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,1.0,1.0,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",SWE-Lancer (IC-Diamond subset),"['reasoning', 'code']",text,False,1.0,en,"SWE-Lancer (IC-Diamond subset) is a benchmark of real-world freelance software engineering tasks from Upwork, ranging from $50 bug fixes to $32,000 feature implementations. It evaluates AI models on independent engineering tasks using end-to-end tests triple-verified by experienced software engineers, and includes managerial tasks where models choose between technical implementation proposals.",1.0,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,GPT-5 - Function calling benchmark (airline domain).,,,tau2-airline,Tau2 airline,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9047.0,,gpt-5-2025-08-07,,,,0.626,,,openai,,,,,,,,0.626,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.626,0.626,Unknown,,,,,Undisclosed,Fair (60-69%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.626,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,GPT-5 with thinking mode - Function calling benchmark (retail domain).,,,tau2-retail,Tau2 retail,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9049.0,,gpt-5-2025-08-07,,,,0.811,,,openai,,,,,,,,0.811,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.811,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode - Function calling benchmark (telecom domain).,,,tau2-telecom,Tau2 telecom,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9051.0,,gpt-5-2025-08-07,,,,0.967,,,openai,,,,,,,,0.967,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.967,0.967,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.967,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),gpt
,VideoMME (long) with subtitles category.,,,videomme-w-sub.,VideoMME w sub.,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10054.0,,gpt-5-2025-08-07,,,,0.867,,,openai,,,,,,,,0.867,https://openai.com/index/introducing-gpt-5-for-developers/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.867,0.867,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",VideoMME w sub.,"['vision', 'multimodal', 'video']",multimodal,False,1.0,en,"The first-ever comprehensive evaluation benchmark of Multi-modal LLMs in Video analysis. Features 900 videos (254 hours) with 2,700 question-answer pairs covering 6 primary visual domains and 30 subfields. Evaluates temporal understanding across short (11 seconds) to long (1 hour) videos with multi-modal inputs including video frames, subtitles, and audio.",0.867,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 with thinking mode - Video-based multimodal reasoning (max frame 256).,,,videommmu,VideoMMMU,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9055.0,,gpt-5-2025-08-07,,,,0.846,,,openai,,,,,,,,0.846,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 is our flagship model for coding, reasoning, and agentic tasks across domains. The best model for coding and agentic tasks with higher reasoning capabilities and medium speed.",VideoMMMU,"['multimodal', 'vision', 'reasoning']",multimodal,False,1.0,en,"Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",0.846,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 Codex specialized for code review and critical flaw detection with enhanced agentic coding capabilities.,,,swe-bench-verified,SWE-Bench Verified,,,2025-09-18T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10100.0,,gpt-5-codex-2025-09-15,,,,0.745,,,openai,,,,,,,,0.745,https://openai.com/index/introducing-upgrades-to-codex/,,,,,,,,2025-09-18T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.745,0.745,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5 Codex,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-09-15,2025.0,9.0,2025-09,Undisclosed,"GPT-5 Codex has been trained specifically for conducting code reviews and finding critical flaws. When reviewing, it navigates your codebase and analyzes code patterns to identify potential security vulnerabilities, performance issues, and bugs.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.745,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,GPT-5 mini with thinking mode enabled (no tools) - competition mathematics.,,,aime-2025,AIME 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9021.0,,gpt-5-mini-2025-08-07,,,,0.911,,,openai,,,,,,,,0.911,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.911,0.911,Unknown,,,,,Undisclosed,Excellent (90%+),GPT-5 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.911,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,GPT-5 mini with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.,,,frontiermath,FrontierMath,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9025.0,,gpt-5-mini-2025-08-07,,,,0.221,,,openai,,,,,,,,0.221,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.221,0.221,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.221,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 mini - Diamond thinking no tools,,,gpqa,GPQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9033.0,,gpt-5-mini-2025-08-07,,,,0.823,,,openai,,,,,,,,0.823,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.823,0.823,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.823,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 mini with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.,,,hmmt-2025,HMMT 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9029.0,,gpt-5-mini-2025-08-07,,,,0.878,,,openai,,,,,,,,0.878,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.878,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 mini with thinking mode (no tools) - Full set of expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9038.0,,gpt-5-mini-2025-08-07,,,,0.167,,,openai,,,,,,,,0.167,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.167,0.167,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5 mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"A faster, more cost-efficient version of GPT-5 for well-defined tasks. Great for well-defined tasks and precise prompts with high reasoning capabilities at reduced cost.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.167,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 nano with thinking mode enabled (no tools) - competition mathematics.,,,aime-2025,AIME 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9022.0,,gpt-5-nano-2025-08-07,,,,0.852,,,openai,,,,,,,,0.852,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.852,0.852,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT-5 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.852,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,GPT-5 nano with thinking mode enabled (with python tool only) - FrontierMath Tier 1-3 expert-level mathematics.,,,frontiermath,FrontierMath,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9026.0,,gpt-5-nano-2025-08-07,,,,0.096,,,openai,,,,,,,,0.096,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.096,0.096,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.096,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,GPT-5 nano - Diamond thinking no tools,,,gpqa,GPQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9034.0,,gpt-5-nano-2025-08-07,,,,0.712,,,openai,,,,,,,,0.712,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.712,0.712,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.712,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,GPT-5 nano with thinking mode enabled (no tools) - Harvard-MIT Mathematics Tournament.,,,hmmt-2025,HMMT 2025,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9030.0,,gpt-5-nano-2025-08-07,,,,0.756,,,openai,,,,,,,,0.756,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.756,0.756,Unknown,,,,,Undisclosed,Good (70-79%),GPT-5 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.756,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,GPT-5 nano with thinking mode (no tools) - Full set of expert-level questions across subjects.,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9039.0,,gpt-5-nano-2025-08-07,,,,0.087,,,openai,,,,,,,,0.087,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.087,0.087,Unknown,,,,,Undisclosed,Poor (<60%),GPT-5 nano,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-08-07,2025.0,8.0,2025-08,Undisclosed,"GPT-5 nano is our fastest, cheapest version of GPT-5. It's great for summarization and classification tasks with average reasoning capabilities and very fast speed.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.087,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Elo (with tools),,,codeforces,Codeforces Competition code,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-120b,,,,0.874,,,openai,,,,,,,,0.874,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.874,0.874,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.00029133333333333333,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Without tools,,,gpqa,GPQA,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,2226.0,,gpt-oss-120b,,,,0.801,,,openai,,,,,,,,0.801,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.801,0.801,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.801,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Score,,,healthbench,HealthBench - Realistic health conversations,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-120b,,,,0.576,,,openai,,,,,,,,0.576,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.576,0.576,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",HealthBench,['healthcare'],text,False,1.0,en,"An open-source benchmark for measuring performance and safety of large language models in healthcare, consisting of 5,000 multi-turn conversations evaluated by 262 physicians using 48,562 unique rubric criteria across health contexts and behavioral dimensions",0.576,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Score,,,healthbench-hard,HealthBench Hard - Challenging health conversations,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,225.0,,gpt-oss-120b,,,,0.3,,,openai,,,,,,,,0.3,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.3,0.3,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",HealthBench Hard,['healthcare'],text,False,1.0,en,"A challenging variation of HealthBench that evaluates large language models' performance and safety in healthcare through 5,000 multi-turn conversations with particularly rigorous evaluation criteria validated by 262 physicians from 60 countries",0.3,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy (with tools),,,humanity's-last-exam,Humanity's Last Exam,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-120b,,,,0.19,,,openai,,,,,,,,0.19,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.19,0.19,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.19,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Without tools,,,mmlu,MMLU benchmark,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,22226.0,,gpt-oss-120b,,,,0.9,,,openai,,,,,,,,0.9,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.9,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),gpt
,Function calling,,,tau-bench-retail,TAU-bench Retail benchmark,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,22226.0,,gpt-oss-120b,,,,0.678,,,openai,,,,,,,,0.678,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.678,0.678,Unknown,,,,,Undisclosed,Fair (60-69%),GPT OSS 120B,openai,OpenAI,116800000000.0,116800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"GPT-OSS-120B is an open-weight, 116.8B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use, including function calling, browsing, and structured output generation. It achieves near-parity with OpenAI o4-mini on core reasoning benchmarks. Note: While referred to as '120b' for simplicity, it technically has 116.8B parameters.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.678,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),gpt
,Elo (with tools),,,codeforces,Codeforces Competition code,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-20b,,,,0.8387,,,openai,,,,,,,,0.8387,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.8387,0.8387,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.00027956666666666667,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Diamond (without tools),,,gpqa,GPQA,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,2226.0,,gpt-oss-20b,,,,0.715,,,openai,,,,,,,,0.715,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.715,0.715,Unknown,,,,,Undisclosed,Good (70-79%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.715,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),gpt
,Score,,,healthbench,HealthBench - Realistic health conversations,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-20b,,,,0.425,,,openai,,,,,,,,0.425,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.425,0.425,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",HealthBench,['healthcare'],text,False,1.0,en,"An open-source benchmark for measuring performance and safety of large language models in healthcare, consisting of 5,000 multi-turn conversations evaluated by 262 physicians using 48,562 unique rubric criteria across health contexts and behavioral dimensions",0.425,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Score,,,healthbench-hard,HealthBench Hard - Challenging health conversations,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,225.0,,gpt-oss-20b,,,,0.108,,,openai,,,,,,,,0.108,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.108,0.108,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",HealthBench Hard,['healthcare'],text,False,1.0,en,"A challenging variation of HealthBench that evaluates large language models' performance and safety in healthcare through 5,000 multi-turn conversations with particularly rigorous evaluation criteria validated by 262 physicians from 60 countries",0.108,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Accuracy (with tools),,,humanity's-last-exam,Humanity's Last Exam,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,224.0,,gpt-oss-20b,,,,0.173,,,openai,,,,,,,,0.173,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.173,0.173,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.173,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Without tools,,,mmlu,MMLU benchmark,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,22226.0,,gpt-oss-20b,,,,0.853,,,openai,,,,,,,,0.853,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.853,0.853,Unknown,,,,,Undisclosed,Very Good (80-89%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.853,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),gpt
,Function calling,,,tau-bench-retail,TAU-bench Retail benchmark,,,2025-08-05T19:49:05.852855+00:00,benchmark_result,,,,True,,,,,,22226.0,,gpt-oss-20b,,,,0.548,,,openai,,,,,,,,0.548,https://openai.com/index/introducing-gpt-oss/,,,,,,,,2025-08-05T19:49:05.852855+00:00,,,,,,False,,False,0.0,False,0.0,0.548,0.548,Unknown,,,,,Undisclosed,Poor (<60%),GPT OSS 20B,openai,OpenAI,20900000000.0,20900000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-08-05,2025.0,8.0,2025-08,Very Large (>70B),"The gpt-oss-20b model (technically 20.9B parameters) achieves near-parity with OpenAI o4-mini on core reasoning benchmarks, while running efficiently on a single 80 GB GPU. The gpt-oss-20b model delivers similar results to OpenAI o3‑mini on common benchmarks and can run on edge devices with just 16 GB of memory, making it ideal for on-device use cases, local inference, or rapid iteration without costly infrastructure. Both models also perform strongly on tool use, few-shot function calling, CoT reasoning (as seen in results on the Tau-Bench agentic evaluation suite) and HealthBench (even outperforming proprietary models like OpenAI o1 and GPT‑4o). Note: While referred to as '20b' for simplicity, it technically has 20.9B parameters.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.548,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),gpt
,Score,,,agieval,AGIEval,,,2025-07-19T19:56:13.976963+00:00,benchmark_result,,,,True,,,,,,1409.0,,granite-3.3-8b-base,,,,0.493,,,ibm,,,,,,,,0.493,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:13.976963+00:00,,,,,,False,,False,0.0,False,0.0,0.493,0.493,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",AGIEval,"['reasoning', 'general', 'math']",text,False,1.0,en,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",0.493,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Not specified,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.006332+00:00,benchmark_result,,,,True,,,,,,477.0,,granite-3.3-8b-base,,,,0.812,,,ibm,,,,,,,,0.812,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:12.006332+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.812,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,Score,,,alpacaeval-2.0,AlpacaEval 2.0,,,2025-07-19T19:56:15.048676+00:00,benchmark_result,,,,True,,,,,,1794.0,,granite-3.3-8b-base,,,,0.6268,,,ibm,,,,,,,,0.6268,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:15.048676+00:00,,,,,,False,,False,0.0,False,0.0,0.6268,0.6268,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",AlpacaEval 2.0,"['general', 'creativity', 'reasoning']",text,False,1.0,en,"AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",0.6268,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,arc-c,ARC-C,,,2025-07-19T19:56:11.131347+00:00,benchmark_result,,,,True,,,,,,23.0,,granite-3.3-8b-base,,,,0.5084,,,ibm,,,,,,,,0.5084,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.131347+00:00,,,,,,False,,False,0.0,False,0.0,0.5084,0.5084,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.5084,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Arena Hard,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.111734+00:00,benchmark_result,,,,True,,,,,,1460.0,,granite-3.3-8b-base,,,,0.5756,,,ibm,,,,,,,,0.5756,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:14.111734+00:00,,,,,,False,,False,0.0,False,0.0,0.5756,0.5756,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.5756,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Not specified (OLMES),,,attaq,AttaQ,,,2025-07-19T19:56:15.087212+00:00,benchmark_result,,,,True,,,,,,1807.0,,granite-3.3-8b-base,,,,0.885,,,ibm,,,,,,,,0.885,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:15.087212+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",AttaQ,['safety'],text,False,1.0,en,"AttaQ is a unique dataset containing adversarial examples in the form of questions designed to provoke harmful or inappropriate responses from large language models. The benchmark evaluates safety vulnerabilities by using specialized clustering techniques that analyze both the semantic similarity of input attacks and the harmfulness of model responses, facilitating targeted improvements to model safety mechanisms.",0.885,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES (Added regex for more efficient answer extraction),,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.251020+00:00,benchmark_result,,,,True,,,,,,1081.0,,granite-3.3-8b-base,,,,0.6913,,,ibm,,,,,,,,0.6913,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:13.251020+00:00,,,,,,False,,False,0.0,False,0.0,0.6913,0.6913,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.6913,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,drop,DROP,,,2025-07-19T19:56:13.012196+00:00,benchmark_result,,,,True,,,,,,955.0,,granite-3.3-8b-base,,,,0.3614,,,ibm,,,,,,,,0.3614,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:13.012196+00:00,,,,,,False,,False,0.0,False,0.0,0.3614,0.3614,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.3614,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.098078+00:00,benchmark_result,,,,True,,,,,,1004.0,,granite-3.3-8b-base,,,,0.59,,,ibm,,,,,,,,0.59,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:13.098078+00:00,,,,,,False,,False,0.0,False,0.0,0.59,0.59,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.59,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.186799+00:00,benchmark_result,,,,True,,,,,,49.0,,granite-3.3-8b-base,,,,0.801,,,ibm,,,,,,,,0.801,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.186799+00:00,,,,,,False,,False,0.0,False,0.0,0.801,0.801,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.801,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,humaneval,HumanEval,,,2025-07-19T19:56:12.666882+00:00,benchmark_result,,,,True,,,,,,798.0,,granite-3.3-8b-base,,,,0.8973,,,ibm,,,,,,,,0.8973,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:12.666882+00:00,,,,,,False,,False,0.0,False,0.0,0.8973,0.8973,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.8973,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.078662+00:00,benchmark_result,,,,True,,,,,,1444.0,,granite-3.3-8b-base,,,,0.8609,,,ibm,,,,,,,,0.8609,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:14.078662+00:00,,,,,,False,,False,0.0,False,0.0,0.8609,0.8609,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.8609,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,ifeval,IFEval,,,2025-07-19T19:56:12.288064+00:00,benchmark_result,,,,True,,,,,,626.0,,granite-3.3-8b-base,,,,0.7482,,,ibm,,,,,,,,0.7482,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:12.288064+00:00,,,,,,False,,False,0.0,False,0.0,0.7482,0.7482,Unknown,,,,,Undisclosed,Good (70-79%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.7482,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Not specified,,,math-500,MATH-500,,,2025-07-19T19:56:12.056690+00:00,benchmark_result,,,,True,,,,,,508.0,,granite-3.3-8b-base,,,,0.6902,,,ibm,,,,,,,,0.6902,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:12.056690+00:00,,,,,,False,,False,0.0,False,0.0,0.6902,0.6902,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.6902,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,mmlu,MMLU,,,2025-07-19T19:56:11.290899+00:00,benchmark_result,,,,True,,,,,,101.0,,granite-3.3-8b-base,,,,0.6389,,,ibm,,,,,,,,0.6389,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.290899+00:00,,,,,,False,,False,0.0,False,0.0,0.6389,0.6389,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.6389,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,nq,NQ,,,2025-07-19T19:56:15.090844+00:00,benchmark_result,,,,True,,,,,,1808.0,,granite-3.3-8b-base,,,,0.365,,,ibm,,,,,,,,0.365,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:15.090844+00:00,,,,,,False,,False,0.0,False,0.0,0.365,0.365,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",NQ,"['reasoning', 'general']",text,False,1.0,en,"Natural Questions (NQ) benchmark containing real user questions issued to Google search with answers found from Wikipedia, designed for training and evaluation of automatic question answering systems",0.365,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,popqa,PopQA,,,2025-07-19T19:56:15.078883+00:00,benchmark_result,,,,True,,,,,,1804.0,,granite-3.3-8b-base,,,,0.2617,,,ibm,,,,,,,,0.2617,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:15.078883+00:00,,,,,,False,,False,0.0,False,0.0,0.2617,0.2617,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",PopQA,"['general', 'reasoning']",text,False,1.0,en,"PopQA is an entity-centric open-domain question-answering dataset consisting of 14,000 QA pairs designed to evaluate language models' ability to memorize and recall factual knowledge across entities with varying popularity levels. The dataset probes both parametric memory (stored in model parameters) and non-parametric memory effectiveness, with questions covering 16 diverse relationship types from Wikidata converted to natural language using templates. Created by sampling knowledge triples from Wikidata and converting them to natural language questions, focusing on long-tail entities to understand LMs' strengths and limitations in memorizing factual knowledge.",0.2617,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.577753+00:00,benchmark_result,,,,True,,,,,,250.0,,granite-3.3-8b-base,,,,0.7818,,,ibm,,,,,,,,0.7818,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.577753+00:00,,,,,,False,,False,0.0,False,0.0,0.7818,0.7818,Unknown,,,,,Undisclosed,Good (70-79%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.7818,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Score,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.362380+00:00,benchmark_result,,,,True,,,,,,142.0,,granite-3.3-8b-base,,,,0.5215,,,ibm,,,,,,,,0.5215,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.362380+00:00,,,,,,False,,False,0.0,False,0.0,0.5215,0.5215,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.5215,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,winogrande,Winogrande,,,2025-07-19T19:56:11.387990+00:00,benchmark_result,,,,True,,,,,,152.0,,granite-3.3-8b-base,,,,0.744,,,ibm,,,,,,,,0.744,https://huggingface.co/ibm-granite/granite-3.3-8b-base,,,,,,,,2025-07-19T19:56:11.387990+00:00,,,,,,False,,False,0.0,False,0.0,0.744,0.744,Unknown,,,,,Undisclosed,Good (70-79%),Granite 3.3 8B Base,ibm,IBM,8170000000.0,8170000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite-3.3-8B-Base is a decoder-only language model with a 128K token context window. It improves upon Granite-3.1-8B-Base by adding support for Fill-in-the-Middle (FIM) using specialized tokens, enabling the model to generate content conditioned on both prefix and suffix. This makes it well-suited for code completion tasks",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.744,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Not specified,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.004852+00:00,benchmark_result,,,,True,,,,,,476.0,,granite-3.3-8b-instruct,,,,0.812,,,ibm,,,,,,,,0.812,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:12.004852+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.812,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,Score,,,alpacaeval-2.0,AlpacaEval 2.0,,,2025-07-19T19:56:15.046908+00:00,benchmark_result,,,,True,,,,,,1793.0,,granite-3.3-8b-instruct,,,,0.6268,,,ibm,,,,,,,,0.6268,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:15.046908+00:00,,,,,,False,,False,0.0,False,0.0,0.6268,0.6268,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",AlpacaEval 2.0,"['general', 'creativity', 'reasoning']",text,False,1.0,en,"AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",0.6268,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Arena Hard benchmark evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.110277+00:00,benchmark_result,,,,True,,,,,,1459.0,,granite-3.3-8b-instruct,,,,0.5756,,,ibm,,,,,,,,0.5756,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:14.110277+00:00,,,,,,False,,False,0.0,False,0.0,0.5756,0.5756,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.5756,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Not specified (OLMES),,,attaq,AttaQ,,,2025-07-19T19:56:15.085492+00:00,benchmark_result,,,,True,,,,,,1806.0,,granite-3.3-8b-instruct,,,,0.885,,,ibm,,,,,,,,0.885,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:15.085492+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",AttaQ,['safety'],text,False,1.0,en,"AttaQ is a unique dataset containing adversarial examples in the form of questions designed to provoke harmful or inappropriate responses from large language models. The benchmark evaluates safety vulnerabilities by using specialized clustering techniques that analyze both the semantic similarity of input attacks and the harmfulness of model responses, facilitating targeted improvements to model safety mechanisms.",0.885,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES (Added regex for more efficient answer extraction),,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.249459+00:00,benchmark_result,,,,True,,,,,,1080.0,,granite-3.3-8b-instruct,,,,0.6913,,,ibm,,,,,,,,0.6913,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:13.249459+00:00,,,,,,False,,False,0.0,False,0.0,0.6913,0.6913,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.6913,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,OLMES (Modified implementation),,,drop,DROP,,,2025-07-19T19:56:13.010691+00:00,benchmark_result,,,,True,,,,,,954.0,,granite-3.3-8b-instruct,,,,0.5936,,,ibm,,,,,,,,0.5936,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:13.010691+00:00,,,,,,False,,False,0.0,False,0.0,0.5936,0.5936,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.5936,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,OLMES,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.095998+00:00,benchmark_result,,,,True,,,,,,1003.0,,granite-3.3-8b-instruct,,,,0.8089,,,ibm,,,,,,,,0.8089,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:13.095998+00:00,,,,,,False,,False,0.0,False,0.0,0.8089,0.8089,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.8089,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,humaneval,HumanEval,,,2025-07-19T19:56:12.665403+00:00,benchmark_result,,,,True,,,,,,797.0,,granite-3.3-8b-instruct,,,,0.8973,,,ibm,,,,,,,,0.8973,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:12.665403+00:00,,,,,,False,,False,0.0,False,0.0,0.8973,0.8973,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.8973,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.076877+00:00,benchmark_result,,,,True,,,,,,1443.0,,granite-3.3-8b-instruct,,,,0.8609,,,ibm,,,,,,,,0.8609,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:14.076877+00:00,,,,,,False,,False,0.0,False,0.0,0.8609,0.8609,Unknown,,,,,Undisclosed,Very Good (80-89%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.8609,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,OLMES,,,ifeval,IFEval,,,2025-07-19T19:56:12.286600+00:00,benchmark_result,,,,True,,,,,,625.0,,granite-3.3-8b-instruct,,,,0.7482,,,ibm,,,,,,,,0.7482,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:12.286600+00:00,,,,,,False,,False,0.0,False,0.0,0.7482,0.7482,Unknown,,,,,Undisclosed,Good (70-79%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.7482,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Not specified,,,math-500,MATH-500,,,2025-07-19T19:56:12.054762+00:00,benchmark_result,,,,True,,,,,,507.0,,granite-3.3-8b-instruct,,,,0.6902,,,ibm,,,,,,,,0.6902,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:12.054762+00:00,,,,,,False,,False,0.0,False,0.0,0.6902,0.6902,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.6902,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,mmlu,MMLU,,,2025-07-19T19:56:11.288937+00:00,benchmark_result,,,,True,,,,,,100.0,,granite-3.3-8b-instruct,,,,0.6554,,,ibm,,,,,,,,0.6554,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:11.288937+00:00,,,,,,False,,False,0.0,False,0.0,0.6554,0.6554,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.6554,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,popqa,PopQA,,,2025-07-19T19:56:15.077308+00:00,benchmark_result,,,,True,,,,,,1803.0,,granite-3.3-8b-instruct,,,,0.2617,,,ibm,,,,,,,,0.2617,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:15.077308+00:00,,,,,,False,,False,0.0,False,0.0,0.2617,0.2617,Unknown,,,,,Undisclosed,Poor (<60%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",PopQA,"['general', 'reasoning']",text,False,1.0,en,"PopQA is an entity-centric open-domain question-answering dataset consisting of 14,000 QA pairs designed to evaluate language models' ability to memorize and recall factual knowledge across entities with varying popularity levels. The dataset probes both parametric memory (stored in model parameters) and non-parametric memory effectiveness, with questions covering 16 diverse relationship types from Wikidata converted to natural language using templates. Created by sampling knowledge triples from Wikidata and converting them to natural language questions, focusing on long-tail entities to understand LMs' strengths and limitations in memorizing factual knowledge.",0.2617,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.360858+00:00,benchmark_result,,,,True,,,,,,141.0,,granite-3.3-8b-instruct,,,,0.6686,,,ibm,,,,,,,,0.6686,https://huggingface.co/ibm-granite/granite-3.3-8b-instruct,,,,,,,,2025-07-19T19:56:11.360858+00:00,,,,,,False,,False,0.0,False,0.0,0.6686,0.6686,Unknown,,,,,Undisclosed,Fair (60-69%),Granite 3.3 8B Instruct,ibm,IBM,8000000000.0,8000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-04-16,2025.0,4.0,2025-04,Very Large (>70B),"Granite 3.3 models feature enhanced reasoning capabilities and support for Fill-in-the-Middle (FIM) code completion. They are built on a foundation of open-source instruction datasets with permissive licenses, alongside internally curated synthetic datasets tailored for long-context problem-solving. These models preserve the key strengths of previous Granite versions, including support for a 128K context length, strong performance in retrieval-augmented generation (RAG) and function calling, and controls for response length and originality. Granite 3.3 also delivers competitive results across general, enterprise, and safety benchmarks. Released as open source, the models are available under the Apache 2.0 license.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.6686,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,alpacaeval-2.0,AlpacaEval 2.0,,,2025-07-19T19:56:15.045290+00:00,benchmark_result,,,,True,,,,,,1792.0,,granite-4.0-tiny-preview,,,,0.3516,,,ibm,,,,,,,,0.3516,https://huggingface.co/ibm-granite/granite-4.0-tiny-preview,,,,,,,,2025-07-19T19:56:15.045290+00:00,,,,,,False,,False,0.0,False,0.0,0.3516,0.3516,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",AlpacaEval 2.0,"['general', 'creativity', 'reasoning']",text,False,1.0,en,"AlpacaEval 2.0 is a length-controlled automatic evaluator for instruction-following language models that uses GPT-4 Turbo to assess model responses against a baseline. It evaluates models on 805 diverse instruction-following tasks including creative writing, classification, programming, and general knowledge questions. The benchmark achieves 0.98 Spearman correlation with ChatBot Arena while being fast (< 3 minutes) and affordable (< $10 in OpenAI credits). It addresses length bias in automatic evaluation through length-controlled win-rates and uses weighted scoring based on response quality.",0.3516,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.108397+00:00,benchmark_result,,,,True,,,,,,1458.0,,granite-4.0-tiny-preview,,,,0.267,,,ibm,,,,,,,,0.267,https://huggingface.co/ibm-granite/granite-4.0-tiny-preview,,,,,,,,2025-07-19T19:56:14.108397+00:00,,,,,,False,,False,0.0,False,0.0,0.267,0.267,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.267,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,attaq,AttaQ,,,2025-07-19T19:56:15.083480+00:00,benchmark_result,,,,True,,,,,,1805.0,,granite-4.0-tiny-preview,,,,0.861,,,ibm,,,,,,,,0.861,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:15.083480+00:00,,,,,,False,,False,0.0,False,0.0,0.861,0.861,Unknown,,,,,Undisclosed,Very Good (80-89%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",AttaQ,['safety'],text,False,1.0,en,"AttaQ is a unique dataset containing adversarial examples in the form of questions designed to provoke harmful or inappropriate responses from large language models. The benchmark evaluates safety vulnerabilities by using specialized clustering techniques that analyze both the semantic similarity of input attacks and the harmfulness of model responses, facilitating targeted improvements to model safety mechanisms.",0.861,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,Score,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.247228+00:00,benchmark_result,,,,True,,,,,,1079.0,,granite-4.0-tiny-preview,,,,0.557,,,ibm,,,,,,,,0.557,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:13.247228+00:00,,,,,,False,,False,0.0,False,0.0,0.557,0.557,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.557,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,drop,DROP,,,2025-07-19T19:56:13.009229+00:00,benchmark_result,,,,True,,,,,,953.0,,granite-4.0-tiny-preview,,,,0.462,,,ibm,,,,,,,,0.462,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:13.009229+00:00,,,,,,False,,False,0.0,False,0.0,0.462,0.462,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.462,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.094422+00:00,benchmark_result,,,,True,,,,,,1002.0,,granite-4.0-tiny-preview,,,,0.701,,,ibm,,,,,,,,0.701,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:13.094422+00:00,,,,,,False,,False,0.0,False,0.0,0.701,0.701,Unknown,,,,,Undisclosed,Good (70-79%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.701,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Score,,,humaneval,HumanEval,,,2025-07-19T19:56:12.663900+00:00,benchmark_result,,,,True,,,,,,796.0,,granite-4.0-tiny-preview,,,,0.824,,,ibm,,,,,,,,0.824,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:12.663900+00:00,,,,,,False,,False,0.0,False,0.0,0.824,0.824,Unknown,,,,,Undisclosed,Very Good (80-89%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.824,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),granite
,Score,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.074105+00:00,benchmark_result,,,,True,,,,,,1442.0,,granite-4.0-tiny-preview,,,,0.783,,,ibm,,,,,,,,0.783,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:14.074105+00:00,,,,,,False,,False,0.0,False,0.0,0.783,0.783,Unknown,,,,,Undisclosed,Good (70-79%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.783,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),granite
,Score,,,ifeval,IFEval,,,2025-07-19T19:56:12.285068+00:00,benchmark_result,,,,True,,,,,,624.0,,granite-4.0-tiny-preview,,,,0.63,,,ibm,,,,,,,,0.63,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:12.285068+00:00,,,,,,False,,False,0.0,False,0.0,0.63,0.63,Unknown,,,,,Undisclosed,Fair (60-69%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.63,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,mmlu,MMLU,,,2025-07-19T19:56:11.287184+00:00,benchmark_result,,,,True,,,,,,99.0,,granite-4.0-tiny-preview,,,,0.604,,,ibm,,,,,,,,0.604,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:11.287184+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.604,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),granite
,Score,,,popqa,PopQA,,,2025-07-19T19:56:15.075622+00:00,benchmark_result,,,,True,,,,,,1802.0,,granite-4.0-tiny-preview,,,,0.229,,,ibm,,,,,,,,0.229,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:15.075622+00:00,,,,,,False,,False,0.0,False,0.0,0.229,0.229,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",PopQA,"['general', 'reasoning']",text,False,1.0,en,"PopQA is an entity-centric open-domain question-answering dataset consisting of 14,000 QA pairs designed to evaluate language models' ability to memorize and recall factual knowledge across entities with varying popularity levels. The dataset probes both parametric memory (stored in model parameters) and non-parametric memory effectiveness, with questions covering 16 diverse relationship types from Wikidata converted to natural language using templates. Created by sampling knowledge triples from Wikidata and converting them to natural language questions, focusing on long-tail entities to understand LMs' strengths and limitations in memorizing factual knowledge.",0.229,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,Score,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.358910+00:00,benchmark_result,,,,True,,,,,,140.0,,granite-4.0-tiny-preview,,,,0.581,,,ibm,,,,,,,,0.581,https://www.ibm.com/new/announcements/ibm-granite-4-0-tiny-preview-sneak-peek,,,,,,,,2025-07-19T19:56:11.358910+00:00,,,,,,False,,False,0.0,False,0.0,0.581,0.581,Unknown,,,,,Undisclosed,Poor (<60%),IBM Granite 4.0 Tiny Preview,ibm,IBM,7000000000.0,7000000000.0,True,2500000000000.0,True,False,apache_2_0,Open & Permissive,2025-05-02,2025.0,5.0,2025-05,Very Large (>70B),"A preliminary version of the smallest model in the upcoming Granite 4.0 family, released May 2025. It utilizes a novel hybrid Mamba-2/Transformer, fine-grained mixture of experts (MoE) architecture (7B total parameters, 1B active at inference). This preview version is partially trained (2.5T tokens) but demonstrates significant memory efficiency and performance potential, validated for at least 128K context length without positional encoding.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.581,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),granite
,0-shot,,,docvqa,DocVQA,,,2025-07-19T19:56:12.861804+00:00,benchmark_result,,,,True,,,,,,894.0,,grok-1.5,,,,0.856,,,xai,,,,,,,,0.856,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.861804+00:00,,,,,,False,,False,0.0,False,0.0,0.856,0.856,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.856,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),grok
,0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.711788+00:00,benchmark_result,,,,True,,,,,,322.0,,grok-1.5,,,,0.359,,,xai,,,,,,,,0.359,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.711788+00:00,,,,,,False,,False,0.0,False,0.0,0.359,0.359,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.359,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,8-shot,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.092882+00:00,benchmark_result,,,,True,,,,,,1001.0,,grok-1.5,,,,0.9,,,xai,,,,,,,,0.9,https://x.ai/blog/grok-1.5,,,,,,,,2025-07-19T19:56:13.092882+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.9,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.660557+00:00,benchmark_result,,,,True,,,,,,794.0,,grok-1.5,,,,0.741,,,xai,,,,,,,,0.741,https://x.ai/blog/grok-1.5,,,,,,,,2025-07-19T19:56:12.660557+00:00,,,,,,False,,False,0.0,False,0.0,0.741,0.741,Unknown,,,,,Undisclosed,Good (70-79%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.741,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,4-shot,,,math,MATH,,,2025-07-19T19:56:11.878054+00:00,benchmark_result,,,,True,,,,,,413.0,,grok-1.5,,,,0.506,,,xai,,,,,,,,0.506,https://x.ai/blog/grok-1.5,,,,,,,,2025-07-19T19:56:11.878054+00:00,,,,,,False,,False,0.0,False,0.0,0.506,0.506,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.506,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,0-shot,,,mathvista,MathVista,,,2025-07-19T19:56:12.103226+00:00,benchmark_result,,,,True,,,,,,532.0,,grok-1.5,,,,0.528,,,xai,,,,,,,,0.528,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.103226+00:00,,,,,,False,,False,0.0,False,0.0,0.528,0.528,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.528,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),grok
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.283997+00:00,benchmark_result,,,,True,,,,,,97.0,,grok-1.5,,,,0.813,,,xai,,,,,,,,0.813,https://x.ai/blog/grok-1.5,,,,,,,,2025-07-19T19:56:11.283997+00:00,,,,,,False,,False,0.0,False,0.0,0.813,0.813,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.813,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,0-shot,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.492470+00:00,benchmark_result,,,,True,,,,,,206.0,,grok-1.5,,,,0.51,,,xai,,,,,,,,0.51,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.492470+00:00,,,,,,False,,False,0.0,False,0.0,0.51,0.51,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.51,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,0-shot,,,mmmu,MMMU,,,2025-07-19T19:56:12.189264+00:00,benchmark_result,,,,True,,,,,,578.0,,grok-1.5,,,,0.536,,,xai,,,,,,,,0.536,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.189264+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5,xai,xAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-03-28,2024.0,3.0,2024-03,Undisclosed,"An advanced language model with improved reasoning capabilities, particularly excelling in coding and mathematical tasks. Features a 128K token context window and enhanced problem-solving abilities compared to its predecessor.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.536,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,zero-shot evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.641849+00:00,benchmark_result,,,,True,,,,,,1259.0,,grok-1.5v,,,,0.883,,,xai,,,,,,,,0.883,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:13.641849+00:00,,,,,,False,,False,0.0,False,0.0,0.883,0.883,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.883,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),grok
,zero-shot evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.817786+00:00,benchmark_result,,,,True,,,,,,871.0,,grok-1.5v,,,,0.761,,,xai,,,,,,,,0.761,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:12.817786+00:00,,,,,,False,,False,0.0,False,0.0,0.761,0.761,Unknown,,,,,Undisclosed,Good (70-79%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.761,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),grok
,zero-shot evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.865566+00:00,benchmark_result,,,,True,,,,,,896.0,,grok-1.5v,,,,0.856,,,xai,,,,,,,,0.856,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:12.865566+00:00,,,,,,False,,False,0.0,False,0.0,0.856,0.856,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.856,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),grok
,zero-shot evaluation,,,mathvista,MathVista,,,2025-07-19T19:56:12.106344+00:00,benchmark_result,,,,True,,,,,,534.0,,grok-1.5v,,,,0.528,,,xai,,,,,,,,0.528,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:12.106344+00:00,,,,,,False,,False,0.0,False,0.0,0.528,0.528,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.528,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),grok
,zero-shot evaluation,,,mmmu,MMMU,,,2025-07-19T19:56:12.195047+00:00,benchmark_result,,,,True,,,,,,581.0,,grok-1.5v,,,,0.536,,,xai,,,,,,,,0.536,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:12.195047+00:00,,,,,,False,,False,0.0,False,0.0,0.536,0.536,Unknown,,,,,Undisclosed,Poor (<60%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.536,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,zero-shot evaluation,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.606610+00:00,benchmark_result,,,,True,,,,,,1638.0,,grok-1.5v,,,,0.687,,,xai,,,,,,,,0.687,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:14.606610+00:00,,,,,,False,,False,0.0,False,0.0,0.687,0.687,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.687,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),grok
,zero-shot evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.908800+00:00,benchmark_result,,,,True,,,,,,915.0,,grok-1.5v,,,,0.781,,,xai,,,,,,,,0.781,https://x.ai/blog/grok-1.5v,,,,,,,,2025-07-19T19:56:12.908800+00:00,,,,,,False,,False,0.0,False,0.0,0.781,0.781,Unknown,,,,,Undisclosed,Good (70-79%),Grok-1.5V,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-04-12,2024.0,4.0,2024-04,Undisclosed,"A multimodal model capable of processing text and visual information, including documents, diagrams, charts, screenshots, and photographs. Notable for strong real-world spatial understanding capabilities.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.781,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,docvqa,DocVQA,,,2025-07-19T19:56:12.863462+00:00,benchmark_result,,,,True,,,,,,895.0,,grok-2,,,,0.936,,,xai,,,,,,,,0.936,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.863462+00:00,,,,,,False,,False,0.0,False,0.0,0.936,0.936,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.936,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.716230+00:00,benchmark_result,,,,True,,,,,,325.0,,grok-2,,,,0.56,,,xai,,,,,,,,0.56,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.716230+00:00,,,,,,False,,False,0.0,False,0.0,0.56,0.56,Unknown,,,,,Undisclosed,Poor (<60%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.56,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.662404+00:00,benchmark_result,,,,True,,,,,,795.0,,grok-2,,,,0.884,,,xai,,,,,,,,0.884,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.662404+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.884,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,maj@1,,,math,MATH,,,2025-07-19T19:56:11.880368+00:00,benchmark_result,,,,True,,,,,,414.0,,grok-2,,,,0.761,,,xai,,,,,,,,0.761,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.880368+00:00,,,,,,False,,False,0.0,False,0.0,0.761,0.761,Unknown,,,,,Undisclosed,Good (70-79%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.761,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.104885+00:00,benchmark_result,,,,True,,,,,,533.0,,grok-2,,,,0.69,,,xai,,,,,,,,0.69,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.104885+00:00,,,,,,False,,False,0.0,False,0.0,0.69,0.69,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.69,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),grok
,accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.285517+00:00,benchmark_result,,,,True,,,,,,98.0,,grok-2,,,,0.875,,,xai,,,,,,,,0.875,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.285517+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.875,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.494333+00:00,benchmark_result,,,,True,,,,,,207.0,,grok-2,,,,0.755,,,xai,,,,,,,,0.755,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.494333+00:00,,,,,,False,,False,0.0,False,0.0,0.755,0.755,Unknown,,,,,Undisclosed,Good (70-79%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.755,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.193698+00:00,benchmark_result,,,,True,,,,,,580.0,,grok-2,,,,0.661,,,xai,,,,,,,,0.661,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.193698+00:00,,,,,,False,,False,0.0,False,0.0,0.661,0.661,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-2,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 is a frontier language model with state-of-the-art reasoning capabilities, featuring advanced abilities in chat, coding, and reasoning. It demonstrates superior performance in visual math reasoning, document-based question answering, and excels across various academic benchmarks including reasoning, reading comprehension, math, and science.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.661,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),grok
,accuracy,,,docvqa,DocVQA,,,2025-07-19T19:56:12.860093+00:00,benchmark_result,,,,True,,,,,,893.0,,grok-2-mini,,,,0.932,,,xai,,,,,,,,0.932,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.860093+00:00,,,,,,False,,False,0.0,False,0.0,0.932,0.932,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.932,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.710285+00:00,benchmark_result,,,,True,,,,,,321.0,,grok-2-mini,,,,0.51,,,xai,,,,,,,,0.51,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.710285+00:00,,,,,,False,,False,0.0,False,0.0,0.51,0.51,Unknown,,,,,Undisclosed,Poor (<60%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.51,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.658802+00:00,benchmark_result,,,,True,,,,,,793.0,,grok-2-mini,,,,0.857,,,xai,,,,,,,,0.857,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.658802+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.857,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,maj@1,,,math,MATH,,,2025-07-19T19:56:11.876593+00:00,benchmark_result,,,,True,,,,,,412.0,,grok-2-mini,,,,0.73,,,xai,,,,,,,,0.73,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.876593+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.73,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.101817+00:00,benchmark_result,,,,True,,,,,,531.0,,grok-2-mini,,,,0.681,,,xai,,,,,,,,0.681,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.101817+00:00,,,,,,False,,False,0.0,False,0.0,0.681,0.681,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.681,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),grok
,accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.281643+00:00,benchmark_result,,,,True,,,,,,96.0,,grok-2-mini,,,,0.862,,,xai,,,,,,,,0.862,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.281643+00:00,,,,,,False,,False,0.0,False,0.0,0.862,0.862,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.862,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.490630+00:00,benchmark_result,,,,True,,,,,,205.0,,grok-2-mini,,,,0.72,,,xai,,,,,,,,0.72,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:11.490630+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.72,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.186961+00:00,benchmark_result,,,,True,,,,,,577.0,,grok-2-mini,,,,0.632,,,xai,,,,,,,,0.632,https://x.ai/blog/grok-2,,,,,,,,2025-07-19T19:56:12.186961+00:00,,,,,,False,,False,0.0,False,0.0,0.632,0.632,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-2 mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-08-13,2024.0,8.0,2024-08,Undisclosed,"Grok-2 mini is a smaller, faster variant of Grok-2 that offers a balance between speed and answer quality. While more compact than its larger sibling, it maintains strong capabilities across various tasks including reasoning, coding, and chat interactions.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.632,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),grok
,accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.003392+00:00,benchmark_result,,,,True,,,,,,475.0,,grok-3,,,,0.933,,,xai,,,,,,,,0.933,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:12.003392+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-3,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.933,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.457788+00:00,benchmark_result,,,,True,,,,,,696.0,,grok-3,,,,0.933,,,xai,,,,,,,,0.933,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:12.457788+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-3,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.933,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.714708+00:00,benchmark_result,,,,True,,,,,,324.0,,grok-3,,,,0.846,,,xai,,,,,,,,0.846,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:11.714708+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-3,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.846,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.402422+00:00,benchmark_result,,,,True,,,,,,1142.0,,grok-3,,,,0.794,,,xai,,,,,,,,0.794,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:13.402422+00:00,,,,,,False,,False,0.0,False,0.0,0.794,0.794,Unknown,,,,,Undisclosed,Good (70-79%),Grok-3,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.794,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.191844+00:00,benchmark_result,,,,True,,,,,,579.0,,grok-3,,,,0.78,,,xai,,,,,,,,0.78,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:12.191844+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Grok-3,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3, launched by xAI on February 17, 2025, is an advanced AI model with significantly enhanced capabilities compared to Grok 2, boasting an order of magnitude increase in performance. Trained on a vast dataset that includes legal documents among others, and utilizing a massive compute infrastructure with around 200,000 GPUs in a Memphis data center, Grok 3's training used ten times more compute than its predecessor. It features specialized models like Grok 3 Reasoning and Grok 3 Mini Reasoning for complex problem-solving, and it excels in benchmarks like AIME for mathematics and GPQA for PhD-level science.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.78,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.001587+00:00,benchmark_result,,,,True,,,,,,474.0,,grok-3-mini,,,,0.958,,,xai,,,,,,,,0.958,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:12.001587+00:00,,,,,,False,,False,0.0,False,0.0,0.958,0.958,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-3 Mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.958,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),grok
,accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.452930+00:00,benchmark_result,,,,True,,,,,,693.0,,grok-3-mini,,,,0.908,,,xai,,,,,,,,0.908,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:12.452930+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-3 Mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.908,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.707259+00:00,benchmark_result,,,,True,,,,,,319.0,,grok-3-mini,,,,0.84,,,xai,,,,,,,,0.84,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:11.707259+00:00,,,,,,False,,False,0.0,False,0.0,0.84,0.84,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-3 Mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.84,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.394024+00:00,benchmark_result,,,,True,,,,,,1139.0,,grok-3-mini,,,,0.804,,,xai,,,,,,,,0.804,https://x.ai/blog/grok-3,,,,,,,,2025-07-19T19:56:13.394024+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-3 Mini,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-02-17,2025.0,2.0,2025-02,Undisclosed,"Grok 3 Mini is a streamlined version of xAI's Grok 3 AI model, designed for quicker response times while maintaining utility. It's tailored for users who require speed over the comprehensive capabilities of the full Grok 3 model, making it suitable for tasks where rapid information retrieval is key. Grok 3 Mini still leverages the advanced training and data that Grok 3 was built on but offers a lighter, more efficient version for everyday use.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.804,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.456102+00:00,benchmark_result,,,,True,,,,,,695.0,,grok-4,,,,0.917,,,xai,,,,,,,,0.917,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:12.456102+00:00,,,,,,False,,False,0.0,False,0.0,0.917,0.917,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.917,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,arc-agi-v2,ARC-AGI v2,,,2025-07-19T19:56:13.922021+00:00,benchmark_result,,,,True,,,,,,1387.0,,grok-4,,,,0.159,,,xai,,,,,,,,0.159,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.922021+00:00,,,,,,False,,False,0.0,False,0.0,0.159,0.159,Unknown,,,,,Undisclosed,Poor (<60%),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",ARC-AGI v2,"['reasoning', 'vision', 'spatial_reasoning']",multimodal,False,1.0,en,"ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",0.159,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.713248+00:00,benchmark_result,,,,True,,,,,,323.0,,grok-4,,,,0.875,,,xai,,,,,,,,0.875,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:11.713248+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.875,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,hmmt25,HMMT25,,,2025-07-19T19:56:15.065811+00:00,benchmark_result,,,,True,,,,,,1799.0,,grok-4,,,,0.9,,,xai,,,,,,,,0.9,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:15.065811+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.9,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),grok
,accuracy,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.523105+00:00,benchmark_result,,,,True,,,,,,723.0,,grok-4,,,,0.4,,,xai,,,,,,,,0.4,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:12.523105+00:00,,,,,,False,,False,0.0,False,0.0,0.4,0.4,Unknown,,,,,Undisclosed,Poor (<60%),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.4,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.399716+00:00,benchmark_result,,,,True,,,,,,1141.0,,grok-4,,,,0.79,,,xai,,,,,,,,0.79,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.399716+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.79,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,usamo25,USAMO25,,,2025-07-19T19:56:15.071894+00:00,benchmark_result,,,,True,,,,,,1801.0,,grok-4,,,,0.375,,,xai,,,,,,,,0.375,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:15.071894+00:00,,,,,,False,,False,0.0,False,0.0,0.375,0.375,Unknown,,,,,Undisclosed,Poor (<60%),Grok-4,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4, announced by xAI in summer 2025, represents a major leap in AI capabilities, described as 'the smartest AI in the world.' Built on version 6 of xAI's foundation model, it uses 100x more training compute than Grok 2 and 10x more reinforcement learning compute than Grok 3. The model achieves PhD-level performance across all academic disciplines simultaneously, scoring perfect on standardized tests like the SAT and near-perfect on graduate exams like the GRE. Unlike Grok 3, tool usage is built into the training process rather than relying on generalization. Trained using 200,000 GPUs, Grok 4 excels at complex reasoning, mathematical problem-solving, and coding tasks, though it has acknowledged weaknesses in multimodal capabilities that are being addressed in the next version.",USAMO25,"['math', 'reasoning']",text,False,1.0,en,"The 2025 United States of America Mathematical Olympiad (USAMO) benchmark consists of six challenging mathematical problems requiring rigorous proof-based reasoning. USAMO is the most prestigious high school mathematics competition in the United States, serving as the final round of the American Mathematics Competitions series. This benchmark evaluates models on mathematical problem-solving capabilities beyond simple numerical computation, focusing on formal mathematical reasoning and proof generation.",0.375,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.454500+00:00,benchmark_result,,,,True,,,,,,694.0,,grok-4-heavy,,,,1.0,,,xai,,,,,,,,1.0,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:12.454500+00:00,,,,,,False,,False,0.0,False,0.0,1.0,1.0,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",1.0,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),grok
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.708827+00:00,benchmark_result,,,,True,,,,,,320.0,,grok-4-heavy,,,,0.884,,,xai,,,,,,,,0.884,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:11.708827+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.884,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),grok
,accuracy,,,hmmt25,HMMT25,,,2025-07-19T19:56:15.063588+00:00,benchmark_result,,,,True,,,,,,1798.0,,grok-4-heavy,,,,0.967,,,xai,,,,,,,,0.967,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:15.063588+00:00,,,,,,False,,False,0.0,False,0.0,0.967,0.967,Unknown,,,,,Undisclosed,Excellent (90%+),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.967,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),grok
,accuracy,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.521361+00:00,benchmark_result,,,,True,,,,,,722.0,,grok-4-heavy,,,,0.507,,,xai,,,,,,,,0.507,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:12.521361+00:00,,,,,,False,,False,0.0,False,0.0,0.507,0.507,Unknown,,,,,Undisclosed,Poor (<60%),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.507,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),grok
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.396669+00:00,benchmark_result,,,,True,,,,,,1140.0,,grok-4-heavy,,,,0.794,,,xai,,,,,,,,0.794,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.396669+00:00,,,,,,False,,False,0.0,False,0.0,0.794,0.794,Unknown,,,,,Undisclosed,Good (70-79%),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.794,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),grok
,accuracy,,,usamo25,USAMO25,,,2025-07-19T19:56:15.070427+00:00,benchmark_result,,,,True,,,,,,1800.0,,grok-4-heavy,,,,0.619,,,xai,,,,,,,,0.619,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:15.070427+00:00,,,,,,False,,False,0.0,False,0.0,0.619,0.619,Unknown,,,,,Undisclosed,Fair (60-69%),Grok-4 Heavy,xai,xAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-07-09,2025.0,7.0,2025-07,Undisclosed,"Grok 4 Heavy is the multi-agent version of Grok 4, released alongside the standard model in summer 2025. This system spawns multiple Grok 4 agents in parallel that work independently on problems and then collaborate by comparing their solutions, similar to a study group. The agents share insights and tricks they discover, with the system intelligently combining their work rather than simply using majority voting. Grok 4 Heavy uses approximately 10x more test-time compute than regular Grok 4, enabling it to solve significantly more complex problems. On the Humanities Last Exam, it achieves over 50% accuracy on text-only problems, and it scored a perfect result on the AIME 2025 mathematics competition. The system represents a major advancement in multi-agent AI collaboration and reasoning capabilities.",USAMO25,"['math', 'reasoning']",text,False,1.0,en,"The 2025 United States of America Mathematical Olympiad (USAMO) benchmark consists of six challenging mathematical problems requiring rigorous proof-based reasoning. USAMO is the most prestigious high school mathematics competition in the United States, serving as the final round of the American Mathematics Competitions series. This benchmark evaluates models on mathematical problem-solving capabilities beyond simple numerical computation, focusing on formal mathematical reasoning and proof generation.",0.619,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),grok
,Accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.139664+00:00,benchmark_result,,,,True,,,,,,28.0,,jamba-1.5-large,,,,0.93,,,ai21,,,,,,,,0.93,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:11.139664+00:00,,,,,,False,,False,0.0,False,0.0,0.93,0.93,Unknown,,,,,Undisclosed,Excellent (90%+),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.93,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),jamba
,Accuracy,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.114965+00:00,benchmark_result,,,,True,,,,,,1462.0,,jamba-1.5-large,,,,0.654,,,ai21,,,,,,,,0.654,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:14.114965+00:00,,,,,,False,,False,0.0,False,0.0,0.654,0.654,Unknown,,,,,Undisclosed,Fair (60-69%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.654,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),jamba
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.736664+00:00,benchmark_result,,,,True,,,,,,338.0,,jamba-1.5-large,,,,0.369,,,ai21,,,,,,,,0.369,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:11.736664+00:00,,,,,,False,,False,0.0,False,0.0,0.369,0.369,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.369,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.109009+00:00,benchmark_result,,,,True,,,,,,1011.0,,jamba-1.5-large,,,,0.87,,,ai21,,,,,,,,0.87,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:13.109009+00:00,,,,,,False,,False,0.0,False,0.0,0.87,0.87,Unknown,,,,,Undisclosed,Very Good (80-89%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.87,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),jamba
,Chain-of-Thought accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.302578+00:00,benchmark_result,,,,True,,,,,,108.0,,jamba-1.5-large,,,,0.812,,,ai21,,,,,,,,0.812,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:11.302578+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.812,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),jamba
,Chain-of-Thought accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.505024+00:00,benchmark_result,,,,True,,,,,,213.0,,jamba-1.5-large,,,,0.535,,,ai21,,,,,,,,0.535,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:11.505024+00:00,,,,,,False,,False,0.0,False,0.0,0.535,0.535,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.535,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.365684+00:00,benchmark_result,,,,True,,,,,,144.0,,jamba-1.5-large,,,,0.583,,,ai21,,,,,,,,0.583,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:11.365684+00:00,,,,,,False,,False,0.0,False,0.0,0.583,0.583,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.583,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,wild-bench,Wild Bench,,,2025-07-19T19:56:15.125090+00:00,benchmark_result,,,,True,,,,,,1816.0,,jamba-1.5-large,,,,0.485,,,ai21,,,,,,,,0.485,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large,,,,,,,,2025-07-19T19:56:15.125090+00:00,,,,,,False,,False,0.0,False,0.0,0.485,0.485,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Large,ai21,AI21 Labs,398000000000.0,398000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"State-of-the-art hybrid SSM-Transformer instruction following foundation model, offering superior long context handling, speed, and quality.",Wild Bench,"['general', 'reasoning']",text,False,1.0,en,"WildBench is an automated evaluation framework that benchmarks large language models using 1,024 challenging, real-world tasks selected from over one million human-chatbot conversation logs. It introduces two evaluation metrics (WB-Reward and WB-Score) that achieve high correlation with human preferences and uses task-specific checklists for systematic evaluation.",0.485,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.141043+00:00,benchmark_result,,,,True,,,,,,29.0,,jamba-1.5-mini,,,,0.857,,,ai21,,,,,,,,0.857,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:11.141043+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.857,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),jamba
,Accuracy,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.117178+00:00,benchmark_result,,,,True,,,,,,1463.0,,jamba-1.5-mini,,,,0.461,,,ai21,,,,,,,,0.461,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:14.117178+00:00,,,,,,False,,False,0.0,False,0.0,0.461,0.461,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.461,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.739037+00:00,benchmark_result,,,,True,,,,,,339.0,,jamba-1.5-mini,,,,0.323,,,ai21,,,,,,,,0.323,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:11.739037+00:00,,,,,,False,,False,0.0,False,0.0,0.323,0.323,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.323,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.110443+00:00,benchmark_result,,,,True,,,,,,1012.0,,jamba-1.5-mini,,,,0.758,,,ai21,,,,,,,,0.758,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:13.110443+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.758,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),jamba
,Chain-of-Thought accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.304017+00:00,benchmark_result,,,,True,,,,,,109.0,,jamba-1.5-mini,,,,0.697,,,ai21,,,,,,,,0.697,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:11.304017+00:00,,,,,,False,,False,0.0,False,0.0,0.697,0.697,Unknown,,,,,Undisclosed,Fair (60-69%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.697,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),jamba
,Chain-of-Thought accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.506893+00:00,benchmark_result,,,,True,,,,,,214.0,,jamba-1.5-mini,,,,0.425,,,ai21,,,,,,,,0.425,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:11.506893+00:00,,,,,,False,,False,0.0,False,0.0,0.425,0.425,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.425,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.367476+00:00,benchmark_result,,,,True,,,,,,145.0,,jamba-1.5-mini,,,,0.541,,,ai21,,,,,,,,0.541,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:11.367476+00:00,,,,,,False,,False,0.0,False,0.0,0.541,0.541,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.541,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Accuracy,,,wild-bench,Wild Bench,,,2025-07-19T19:56:15.127075+00:00,benchmark_result,,,,True,,,,,,1817.0,,jamba-1.5-mini,,,,0.424,,,ai21,,,,,,,,0.424,https://huggingface.co/ai21labs/AI21-Jamba-1.5-Mini,,,,,,,,2025-07-19T19:56:15.127075+00:00,,,,,,False,,False,0.0,False,0.0,0.424,0.424,Unknown,,,,,Undisclosed,Poor (<60%),Jamba 1.5 Mini,ai21,AI21 Labs,52000000000.0,52000000000.0,True,0.0,False,False,jamba_open_model_license,Restricted/Community,2024-08-22,2024.0,8.0,2024-08,Very Large (>70B),"Part of the Jamba 1.5 family, a state-of-the-art hybrid SSM-Transformer instruction following foundation model offering superior long context handling, speed, and quality.",Wild Bench,"['general', 'reasoning']",text,False,1.0,en,"WildBench is an automated evaluation framework that benchmarks large language models using 1,024 challenging, real-world tasks selected from over one million human-chatbot conversation logs. It introduces two evaluation metrics (WB-Reward and WB-Score) that achieve high correlation with human preferences and uses task-specific checklists for systematic evaluation.",0.424,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),jamba
,Pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.945090+00:00,benchmark_result,,,,True,,,,,,444.0,,kimi-k1.5,,,,0.775,,,moonshotai,,,,,,,,0.775,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:11.945090+00:00,,,,,,False,,False,0.0,False,0.0,0.775,0.775,Unknown,,,,,Undisclosed,Good (70-79%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.775,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Exact Match,,,c-eval,C-Eval,,,2025-07-19T19:56:11.922484+00:00,benchmark_result,,,,True,,,,,,435.0,,kimi-k1.5,,,,0.883,,,moonshotai,,,,,,,,0.883,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:11.922484+00:00,,,,,,False,,False,0.0,False,0.0,0.883,0.883,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",C-Eval,"['general', 'reasoning']",text,True,1.0,en,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",0.883,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Exact Match,,,cluewsc,CLUEWSC,,,2025-07-19T19:56:12.236097+00:00,benchmark_result,,,,True,,,,,,599.0,,kimi-k1.5,,,,0.914,,,moonshotai,,,,,,,,0.914,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.236097+00:00,,,,,,False,,False,0.0,False,0.0,0.914,0.914,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",CLUEWSC,"['language', 'reasoning']",text,True,1.0,en,"CLUEWSC2020 is the Chinese version of the Winograd Schema Challenge, part of the CLUE benchmark. It focuses on pronoun disambiguation and coreference resolution, requiring models to determine which noun a pronoun refers to in a sentence. The dataset contains 1,244 training samples and 304 development samples extracted from contemporary Chinese literature.",0.914,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Exact Match,,,ifeval,IFEval,,,2025-07-19T19:56:12.244895+00:00,benchmark_result,,,,True,,,,,,602.0,,kimi-k1.5,,,,0.872,,,moonshotai,,,,,,,,0.872,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.244895+00:00,,,,,,False,,False,0.0,False,0.0,0.872,0.872,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.872,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Pass@1,,,livecodebench-v5-24.12-25.2,LiveCodeBench v5 24.12-25.2,,,2025-07-19T19:56:12.068737+00:00,benchmark_result,,,,True,,,,,,514.0,,kimi-k1.5,,,,0.625,,,moonshotai,,,,,,,,0.625,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.068737+00:00,,,,,,False,,False,0.0,False,0.0,0.625,0.625,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",LiveCodeBench v5 24.12-25.2,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.625,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Exact Match,,,math-500,MATH-500,,,2025-07-19T19:56:12.029931+00:00,benchmark_result,,,,True,,,,,,492.0,,kimi-k1.5,,,,0.962,,,moonshotai,,,,,,,,0.962,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.029931+00:00,,,,,,False,,False,0.0,False,0.0,0.962,0.962,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.962,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),kimi
,Pass@1,,,mathvista,MathVista,,,2025-07-19T19:56:12.071814+00:00,benchmark_result,,,,True,,,,,,515.0,,kimi-k1.5,,,,0.749,,,moonshotai,,,,,,,,0.749,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.071814+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.749,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),kimi
,Exact Match,,,mmlu,MMLU,,,2025-07-19T19:56:11.207582+00:00,benchmark_result,,,,True,,,,,,58.0,,kimi-k1.5,,,,0.874,,,moonshotai,,,,,,,,0.874,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:11.207582+00:00,,,,,,False,,False,0.0,False,0.0,0.874,0.874,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.874,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Pass@1,,,mmmu,MMMU,,,2025-07-19T19:56:12.132422+00:00,benchmark_result,,,,True,,,,,,549.0,,kimi-k1.5,,,,0.7,,,moonshotai,,,,,,,,0.7,https://github.com/MoonshotAI/Kimi-k1.5,,,,,,,,2025-07-19T19:56:12.132422+00:00,,,,,,False,,False,0.0,False,0.0,0.7,0.7,Unknown,,,,,Undisclosed,Good (70-79%),Kimi-k1.5,moonshotai,Moonshot AI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-01-20,2025.0,1.0,2025-01,Undisclosed,"Kimi 1.5 is a next-generation multimodal large language model developed by Moonshot AI. It incorporates advanced reinforcement learning (RL) and scalable multimodal reasoning, delivering state-of-the-art performance in math, code, vision, and long-context reasoning tasks.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.7,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,,,,aime-2024,AIME 2024,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9006.0,,kimi-k2-0905,,,,0.72,,,moonshotai,,,,,,,,0.72,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.72,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,,,,gpqa,GPQA,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9001.0,,kimi-k2-0905,,,,0.758,,,moonshotai,,,,,,,,0.758,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.758,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,,,,humaneval,HumanEval,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9004.0,,kimi-k2-0905,,,,0.945,,,moonshotai,,,,,,,,0.945,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.945,0.945,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.945,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,,,,math,MATH,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9003.0,,kimi-k2-0905,,,,0.891,,,moonshotai,,,,,,,,0.891,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.891,0.891,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.891,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,,,,mmlu,MMLU,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9002.0,,kimi-k2-0905,,,,0.902,,,moonshotai,,,,,,,,0.902,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.902,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,,,,mmlu-pro,MMLU-Pro,,,2024-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9005.0,,kimi-k2-0905,,,,0.825,,,moonshotai,,,,,,,,0.825,https://moonshot.cn/blog/kimi-k2-0905,,,,,,,,2024-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.825,0.825,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,0.0,False,False,proprietary,Proprietary,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2 0905 is the September update of Kimi K2 0711. It is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It supports long-context inference up to 256k tokens, extended from the previous 128k. This update improves agentic coding with higher accuracy and better generalization across scaffolds, and enhances frontend coding with more aesthetic and functional outputs for web, 3D, and related tasks. The model is trained with a novel stack incorporating the MuonClip optimizer for stable large-scale MoE training.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.825,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,c-eval,C-Eval,,,2025-07-19T19:56:11.920573+00:00,benchmark_result,,,,True,,,,,,434.0,,kimi-k2-base,,,,0.925,,,moonshotai,,,,,,,,0.925,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.920573+00:00,,,,,,False,,False,0.0,False,0.0,0.925,0.925,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",C-Eval,"['general', 'reasoning']",text,True,1.0,en,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",0.925,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Correct,,,csimpleqa,CSimpleQA,,,2025-07-19T19:56:11.934566+00:00,benchmark_result,,,,True,,,,,,440.0,,kimi-k2-base,,,,0.776,,,moonshotai,,,,,,,,0.776,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.934566+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",CSimpleQA,"['general', 'language']",text,True,1.0,en,"Chinese SimpleQA is the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions. It contains 3,000 high-quality questions spanning 6 major topics with 99 diverse subtopics, designed to assess Chinese factual knowledge across humanities, science, engineering, culture, and society.",0.776,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Pass@1,,,evalplus,EvalPlus,,,2025-07-19T19:56:11.796250+00:00,benchmark_result,,,,True,,,,,,369.0,,kimi-k2-base,,,,0.803,,,moonshotai,,,,,,,,0.803,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.796250+00:00,,,,,,False,,False,0.0,False,0.0,0.803,0.803,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",EvalPlus,"['reasoning', 'code']",text,False,100.0,en,"A rigorous code synthesis evaluation framework that augments existing datasets with extensive test cases generated by LLM and mutation-based strategies to better assess functional correctness of generated code, including HumanEval+ with 80x more test cases",0.00803,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Diamond Avg@8,,,gpqa,GPQA,,,2025-07-19T19:56:11.591508+00:00,benchmark_result,,,,True,,,,,,256.0,,kimi-k2-base,,,,0.481,,,moonshotai,,,,,,,,0.481,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.591508+00:00,,,,,,False,,False,0.0,False,0.0,0.481,0.481,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.481,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,EM,,,gsm8k,GSM8k,,,2025-07-19T19:56:11.403308+00:00,benchmark_result,,,,True,,,,,,158.0,,kimi-k2-base,,,,0.921,,,moonshotai,,,,,,,,0.921,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.403308+00:00,,,,,,False,,False,0.0,False,0.0,0.921,0.921,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.921,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Pass@1,,,livecodebench-v6,LiveCodeBench v6,,,2025-07-19T19:56:11.789592+00:00,benchmark_result,,,,True,,,,,,367.0,,kimi-k2-base,,,,0.263,,,moonshotai,,,,,,,,0.263,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.789592+00:00,,,,,,False,,False,0.0,False,0.0,0.263,0.263,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.263,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,EM,,,math,MATH,,,2025-07-19T19:56:11.808795+00:00,benchmark_result,,,,True,,,,,,373.0,,kimi-k2-base,,,,0.702,,,moonshotai,,,,,,,,0.702,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.808795+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.702,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,EM,,,mmlu,MMLU,,,2025-07-19T19:56:11.205746+00:00,benchmark_result,,,,True,,,,,,57.0,,kimi-k2-base,,,,0.878,,,moonshotai,,,,,,,,0.878,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.205746+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.878,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.410852+00:00,benchmark_result,,,,True,,,,,,161.0,,kimi-k2-base,,,,0.692,,,moonshotai,,,,,,,,0.692,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.410852+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.692,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,EM,,,mmlu-redux-2.0,MMLU-redux-2.0,,,2025-07-19T19:56:11.520883+00:00,benchmark_result,,,,True,,,,,,221.0,,kimi-k2-base,,,,0.902,,,moonshotai,,,,,,,,0.902,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.520883+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",MMLU-redux-2.0,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A curated version of the MMLU benchmark featuring manually re-annotated 5,700 questions across 57 subjects to identify and correct errors in the original dataset. Addresses the 6.49% error rate found in MMLU and provides more reliable evaluation metrics for language models.",0.902,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Correct,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.524097+00:00,benchmark_result,,,,True,,,,,,222.0,,kimi-k2-base,,,,0.353,,,moonshotai,,,,,,,,0.353,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.524097+00:00,,,,,,False,,False,0.0,False,0.0,0.353,0.353,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.353,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,EM,,,supergpqa,SuperGPQA,,,2025-07-19T19:56:11.781413+00:00,benchmark_result,,,,True,,,,,,364.0,,kimi-k2-base,,,,0.447,,,moonshotai,,,,,,,,0.447,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.781413+00:00,,,,,,False,,False,0.0,False,0.0,0.447,0.447,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.447,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,EM,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.566226+00:00,benchmark_result,,,,True,,,,,,243.0,,kimi-k2-base,,,,0.851,,,moonshotai,,,,,,,,0.851,https://github.com/MoonshotAI/Kimi-K2,,,,,,,,2025-07-19T19:56:11.566226+00:00,,,,,,False,,False,0.0,False,0.0,0.851,0.851,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Base,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 base model is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained on 15.5 trillion tokens with the MuonClip optimizer, this is the foundation model before instruction tuning. It demonstrates strong performance on knowledge, reasoning, and coding benchmarks while being optimized for agentic capabilities.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.851,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Accuracy,,,acebench,AceBench,,,2025-07-19T19:56:12.408910+00:00,benchmark_result,,,,True,,,,,,676.0,,kimi-k2-instruct,,,,0.765,,,moonshotai,,,,,,,,0.765,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.408910+00:00,,,,,,False,,False,0.0,False,0.0,0.765,0.765,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",ACEBench,"['general', 'reasoning']",text,False,1.0,en,"ACEBench is a comprehensive benchmark for evaluating Large Language Models' tool usage capabilities across three primary evaluation types: Normal (basic tool usage scenarios), Special (tool usage with ambiguous or incomplete instructions), and Agent (multi-agent interactions simulating real-world dialogues). The benchmark covers 4,538 APIs across 8 major domains and 68 sub-domains including technology, finance, entertainment, society, health, culture, and environment, supporting both English and Chinese languages.",0.765,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Accuracy,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.362819+00:00,benchmark_result,,,,True,,,,,,657.0,,kimi-k2-instruct,,,,0.6,,,moonshotai,,,,,,,,0.6,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.362819+00:00,,,,,,False,,False,0.0,False,0.0,0.6,0.6,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.6,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Avg@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.946639+00:00,benchmark_result,,,,True,,,,,,445.0,,kimi-k2-instruct,,,,0.696,,,moonshotai,,,,,,,,0.696,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.946639+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.696,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Avg@64,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.412395+00:00,benchmark_result,,,,True,,,,,,677.0,,kimi-k2-instruct,,,,0.495,,,moonshotai,,,,,,,,0.495,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.412395+00:00,,,,,,False,,False,0.0,False,0.0,0.495,0.495,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.495,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,autologi,AutoLogi,,,2025-07-19T19:56:12.506457+00:00,benchmark_result,,,,True,,,,,,715.0,,kimi-k2-instruct,,,,0.895,,,moonshotai,,,,,,,,0.895,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.506457+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",AutoLogi,['reasoning'],text,True,1.0,en,"AutoLogi is an automated method for synthesizing open-ended logic puzzles to evaluate reasoning abilities of Large Language Models. The benchmark addresses limitations of existing multiple-choice reasoning evaluations by featuring program-based verification and controllable difficulty levels. It includes 1,575 English and 883 Chinese puzzles, enabling more reliable evaluation that better distinguishes models' reasoning capabilities across languages.",0.895,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Accuracy,,,cbnsl,CBNSL,,,2025-07-19T19:56:12.594017+00:00,benchmark_result,,,,True,,,,,,757.0,,kimi-k2-instruct,,,,0.956,,,moonshotai,,,,,,,,0.956,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.594017+00:00,,,,,,False,,False,0.0,False,0.0,0.956,0.956,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",CBNSL,"['math', 'reasoning']",text,False,1.0,en,Curriculum Learning of Bayesian Network Structures (CBNSL) benchmark for evaluating algorithms that learn Bayesian network structures from data using curriculum learning techniques. The benchmark uses networks from the bnlearn repository and evaluates structure learning performance using BDeu scoring metrics.,0.956,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),kimi
,Avg@16,,,cnmo-2024,CNMO 2024,,,2025-07-19T19:56:12.489469+00:00,benchmark_result,,,,True,,,,,,709.0,,kimi-k2-instruct,,,,0.743,,,moonshotai,,,,,,,,0.743,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.489469+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",CNMO 2024,['math'],text,False,1.0,en,China Mathematical Olympiad 2024 - A challenging mathematics competition.,0.743,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Correct,,,csimpleqa,CSimpleQA,,,2025-07-19T19:56:11.936097+00:00,benchmark_result,,,,True,,,,,,441.0,,kimi-k2-instruct,,,,0.784,,,moonshotai,,,,,,,,0.784,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.936097+00:00,,,,,,False,,False,0.0,False,0.0,0.784,0.784,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",CSimpleQA,"['general', 'language']",text,True,1.0,en,"Chinese SimpleQA is the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions. It contains 3,000 high-quality questions spanning 6 major topics with 99 diverse subtopics, designed to assess Chinese factual knowledge across humanities, science, engineering, culture, and society.",0.784,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Diamond Avg@8,,,gpqa,GPQA,,,2025-07-19T19:56:11.593256+00:00,benchmark_result,,,,True,,,,,,257.0,,kimi-k2-instruct,,,,0.751,,,moonshotai,,,,,,,,0.751,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.593256+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.751,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:11.405113+00:00,benchmark_result,,,,True,,,,,,159.0,,kimi-k2-instruct,,,,0.973,,,moonshotai,,,,,,,,0.973,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.405113+00:00,,,,,,False,,False,0.0,False,0.0,0.973,0.973,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.973,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),kimi
,Avg@32,,,hmmt-2025,HMMT 2025,,,2025-07-19T19:56:12.482540+00:00,benchmark_result,,,,True,,,,,,707.0,,kimi-k2-instruct,,,,0.388,,,moonshotai,,,,,,,,0.388,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.482540+00:00,,,,,,False,,False,0.0,False,0.0,0.388,0.388,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.388,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.598519+00:00,benchmark_result,,,,True,,,,,,758.0,,kimi-k2-instruct,,,,0.933,,,moonshotai,,,,,,,,0.933,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.598519+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.933,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Pass@1,,,humaneval-er,HumanEval-ER,,,2025-07-19T19:56:12.707650+00:00,benchmark_result,,,,True,,,,,,819.0,,kimi-k2-instruct,,,,0.811,,,moonshotai,,,,,,,,0.811,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.707650+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",HumanEval-ER,['reasoning'],text,False,1.0,en,"A variant of the HumanEval benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.811,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Accuracy (Text Only),,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.510122+00:00,benchmark_result,,,,True,,,,,,716.0,,kimi-k2-instruct,,,,0.047,,,moonshotai,,,,,,,,0.047,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.510122+00:00,,,,,,False,,False,0.0,False,0.0,0.047,0.047,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.047,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Prompt Strict,,,ifeval,IFEval,,,2025-07-19T19:56:12.247003+00:00,benchmark_result,,,,True,,,,,,603.0,,kimi-k2-instruct,,,,0.898,,,moonshotai,,,,,,,,0.898,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.247003+00:00,,,,,,False,,False,0.0,False,0.0,0.898,0.898,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.898,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Pass@1,,,livebench,LiveBench,,,2025-07-19T19:56:12.567525+00:00,benchmark_result,,,,True,,,,,,745.0,,kimi-k2-instruct,,,,0.764,,,moonshotai,,,,,,,,0.764,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.567525+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.764,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Pass@1,,,livecodebench-v6,LiveCodeBench v6,,,2025-07-19T19:56:11.791826+00:00,benchmark_result,,,,True,,,,,,368.0,,kimi-k2-instruct,,,,0.537,,,moonshotai,,,,,,,,0.537,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.791826+00:00,,,,,,False,,False,0.0,False,0.0,0.537,0.537,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.537,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,math-500,MATH-500,,,2025-07-19T19:56:12.031465+00:00,benchmark_result,,,,True,,,,,,493.0,,kimi-k2-instruct,,,,0.974,,,moonshotai,,,,,,,,0.974,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.031465+00:00,,,,,,False,,False,0.0,False,0.0,0.974,0.974,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.974,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),kimi
,EM,,,mmlu,MMLU,,,2025-07-19T19:56:11.209924+00:00,benchmark_result,,,,True,,,,,,59.0,,kimi-k2-instruct,,,,0.895,,,moonshotai,,,,,,,,0.895,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.209924+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.895,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.412849+00:00,benchmark_result,,,,True,,,,,,162.0,,kimi-k2-instruct,,,,0.811,,,moonshotai,,,,,,,,0.811,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.412849+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.811,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.531649+00:00,benchmark_result,,,,True,,,,,,727.0,,kimi-k2-instruct,,,,0.927,,,moonshotai,,,,,,,,0.927,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.531649+00:00,,,,,,False,,False,0.0,False,0.0,0.927,0.927,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.927,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Accuracy,,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.554319+00:00,benchmark_result,,,,True,,,,,,739.0,,kimi-k2-instruct,,,,0.541,,,moonshotai,,,,,,,,0.541,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.554319+00:00,,,,,,False,,False,0.0,False,0.0,0.541,0.541,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.541,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Pass@1,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.314432+00:00,benchmark_result,,,,True,,,,,,639.0,,kimi-k2-instruct,,,,0.857,,,moonshotai,,,,,,,,0.857,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.314432+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.857,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Pass@1,,,musr,MuSR,,,2025-07-19T19:56:12.711252+00:00,benchmark_result,,,,True,,,,,,820.0,,kimi-k2-instruct,,,,0.764,,,moonshotai,,,,,,,,0.764,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.711252+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",MuSR,['reasoning'],text,False,1.0,en,"MuSR (Multistep Soft Reasoning) is a benchmark for evaluating language models on multistep soft reasoning tasks specified in natural language narratives. Created through a neurosymbolic synthetic-to-natural generation algorithm, it generates complex reasoning scenarios like murder mysteries roughly 1000 words in length that challenge current LLMs including GPT-4. The benchmark tests chain-of-thought reasoning capabilities across domains involving commonsense reasoning about physical and social situations.",0.764,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Pass@1,,,ojbench,OJBench,,,2025-07-19T19:56:12.310963+00:00,benchmark_result,,,,True,,,,,,638.0,,kimi-k2-instruct,,,,0.271,,,moonshotai,,,,,,,,0.271,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.310963+00:00,,,,,,False,,False,0.0,False,0.0,0.271,0.271,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",OJBench,['reasoning'],text,False,1.0,en,"OJBench is a competition-level code benchmark designed to assess the competitive-level code reasoning abilities of large language models. It comprises 232 programming competition problems from NOI and ICPC, categorized into Easy, Medium, and Hard difficulty levels. The benchmark evaluates models' ability to solve complex competitive programming challenges using Python and C++.",0.271,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Avg@4,,,polymath-en,PolyMath-en,,,2025-07-19T19:56:12.499339+00:00,benchmark_result,,,,True,,,,,,713.0,,kimi-k2-instruct,,,,0.651,,,moonshotai,,,,,,,,0.651,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.499339+00:00,,,,,,False,,False,0.0,False,0.0,0.651,0.651,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",PolyMath-en,"['math', 'reasoning']",text,True,1.0,en,"PolyMath is a multilingual mathematical reasoning benchmark covering 18 languages and 4 difficulty levels from easy to hard, ensuring difficulty comprehensiveness, language diversity, and high-quality translation. The benchmark evaluates mathematical reasoning capabilities of large language models across diverse linguistic contexts, making it a highly discriminative multilingual mathematical benchmark.",0.651,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Correct,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.526736+00:00,benchmark_result,,,,True,,,,,,223.0,,kimi-k2-instruct,,,,0.31,,,moonshotai,,,,,,,,0.31,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.526736+00:00,,,,,,False,,False,0.0,False,0.0,0.31,0.31,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.31,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,supergpqa,SuperGPQA,,,2025-07-19T19:56:11.782850+00:00,benchmark_result,,,,True,,,,,,365.0,,kimi-k2-instruct,,,,0.572,,,moonshotai,,,,,,,,0.572,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:11.782850+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.572,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Single Attempt,,,swe-bench-multilingual,SWE-bench Multilingual,,,2025-07-19T19:56:12.343981+00:00,benchmark_result,,,,True,,,,,,651.0,,kimi-k2-instruct,,,,0.473,,,moonshotai,,,,,,,,0.473,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.343981+00:00,,,,,,False,,False,0.0,False,0.0,0.473,0.473,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SWE-bench Multilingual,"['reasoning', 'code']",text,True,1.0,en,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",0.473,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Single Attempt,,,swe-bench-verified-(agentic-coding),SWE-bench Verified (Agentic Coding),,,2025-07-19T19:56:12.333761+00:00,benchmark_result,,,,True,,,,,,649.0,,kimi-k2-instruct,,,,0.658,,,moonshotai,,,,,,,,0.658,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.333761+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SWE-bench Verified (Agentic Coding),"['reasoning', 'code']",text,False,1.0,en,"SWE-bench Verified is a human-filtered subset of 500 software engineering problems drawn from real GitHub issues across 12 popular Python repositories. Given a codebase and an issue description, language models are tasked with generating patches that resolve the described problems. This benchmark evaluates AI's real-world agentic coding skills by requiring models to navigate complex codebases, understand software engineering problems, and coordinate changes across multiple functions, classes, and files to fix well-defined issues with clear descriptions.",0.658,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Single Patch without Test,,,swe-bench-verified-(agentless),SWE-bench Verified (Agentless),,,2025-07-19T19:56:12.330548+00:00,benchmark_result,,,,True,,,,,,648.0,,kimi-k2-instruct,,,,0.518,,,moonshotai,,,,,,,,0.518,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.330548+00:00,,,,,,False,,False,0.0,False,0.0,0.518,0.518,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SWE-bench Verified (Agentless),"['general', 'reasoning']",text,False,1.0,en,"A human-validated subset of SWE-bench that evaluates language models' ability to resolve real-world GitHub issues using an agentless approach. The benchmark tests models on software engineering problems requiring understanding and coordinating changes across multiple functions, classes, and files simultaneously.",0.518,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Multiple Attempts with parallel test-time compute,,,swe-bench-verified-(multiple-attempts),SWE-bench Verified (Multiple Attempts),,,2025-07-19T19:56:12.339305+00:00,benchmark_result,,,,True,,,,,,650.0,,kimi-k2-instruct,,,,0.716,,,moonshotai,,,,,,,,0.716,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.339305+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",SWE-bench Verified (Multiple Attempts),['reasoning'],text,False,1.0,en,"SWE-bench Verified is a human-validated subset of 500 test samples from the original SWE-bench dataset that evaluates AI systems' ability to automatically resolve real GitHub issues in Python repositories. Given a codebase and issue description, models must edit the code to successfully resolve the problem, requiring understanding and coordination of changes across multiple functions, classes, and files. The Verified version provides more reliable evaluation through manual validation of test samples.",0.716,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Avg@4,,,tau2-airline,Tau2 airline,,,2025-07-19T19:56:12.401229+00:00,benchmark_result,,,,True,,,,,,674.0,,kimi-k2-instruct,,,,0.565,,,moonshotai,,,,,,,,0.565,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.401229+00:00,,,,,,False,,False,0.0,False,0.0,0.565,0.565,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.565,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Avg@4,,,tau2-retail,Tau2 retail,,,2025-07-19T19:56:12.395604+00:00,benchmark_result,,,,True,,,,,,673.0,,kimi-k2-instruct,,,,0.706,,,moonshotai,,,,,,,,0.706,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.395604+00:00,,,,,,False,,False,0.0,False,0.0,0.706,0.706,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.706,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Avg@4,,,tau2-telecom,Tau2 telecom,,,2025-07-19T19:56:12.405145+00:00,benchmark_result,,,,True,,,,,,675.0,,kimi-k2-instruct,,,,0.658,,,moonshotai,,,,,,,,0.658,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.405145+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.658,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Inhouse Framework,,,terminal-bench,Terminal-bench,,,2025-07-19T19:56:12.348003+00:00,benchmark_result,,,,True,,,,,,652.0,,kimi-k2-instruct,,,,0.3,,,moonshotai,,,,,,,,0.3,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.348003+00:00,,,,,,False,,False,0.0,False,0.0,0.3,0.3,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.3,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,terminus,Terminus,,,2025-07-19T19:56:12.358921+00:00,benchmark_result,,,,True,,,,,,656.0,,kimi-k2-instruct,,,,0.25,,,moonshotai,,,,,,,,0.25,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.358921+00:00,,,,,,False,,False,0.0,False,0.0,0.25,0.25,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",Terminus,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments, evaluating how well agents can handle real-world, end-to-end tasks autonomously. The benchmark includes tasks spanning coding, system administration, security, data science, model training, file operations, version control, and web development. Terminus is the neutral test-bed agent designed to work with Terminal-Bench, operating purely through tmux sessions without dedicated tools.",0.25,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,zebralogic,ZebraLogic,,,2025-07-19T19:56:12.502879+00:00,benchmark_result,,,,True,,,,,,714.0,,kimi-k2-instruct,,,,0.89,,,moonshotai,,,,,,,,0.89,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-07-19T19:56:12.502879+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2 Instruct,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-07-11,2025.0,7.0,2025-07,Very Large (>70B),"Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the MuonClip optimizer, it achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities. The instruct variant is post-trained for drop-in, general-purpose chat and agentic experiences without long thinking.",ZebraLogic,['reasoning'],text,False,1.0,en,"ZebraLogic is an evaluation framework for assessing large language models' logical reasoning capabilities through logic grid puzzles derived from constraint satisfaction problems (CSPs). The benchmark consists of 1,000 programmatically generated puzzles with controllable and quantifiable complexity, revealing a 'curse of complexity' where model accuracy declines significantly as problem complexity grows.",0.89,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Accuracy,,,acebench,Acebench,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10011.0,,kimi-k2-instruct-0905,,,,0.765,,,moonshotai,,,,,,,,0.765,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.765,0.765,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",ACEBench,"['general', 'reasoning']",text,False,1.0,en,"ACEBench is a comprehensive benchmark for evaluating Large Language Models' tool usage capabilities across three primary evaluation types: Normal (basic tool usage scenarios), Special (tool usage with ambiguous or incomplete instructions), and Agent (multi-agent interactions simulating real-world dialogues). The benchmark covers 4,538 APIs across 8 major domains and 68 sub-domains including technology, finance, entertainment, society, health, culture, and environment, supporting both English and Chinese languages.",0.765,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Accuracy,,,aider-polyglot,Aider Polyglot,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10007.0,,kimi-k2-instruct-0905,,,,0.6,,,moonshotai,,,,,,,,0.6,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.6,0.6,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.6,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Avg@64,,,aime-2024,Aime 2024,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10012.0,,kimi-k2-instruct-0905,,,,0.696,,,moonshotai,,,,,,,,0.696,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.696,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Avg@64,,,aime-2025,Aime 2025,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10013.0,,kimi-k2-instruct-0905,,,,0.495,,,moonshotai,,,,,,,,0.495,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.495,0.495,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.495,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,autologi,Autologi,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10019.0,,kimi-k2-instruct-0905,,,,0.895,,,moonshotai,,,,,,,,0.895,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",AutoLogi,['reasoning'],text,True,1.0,en,"AutoLogi is an automated method for synthesizing open-ended logic puzzles to evaluate reasoning abilities of Large Language Models. The benchmark addresses limitations of existing multiple-choice reasoning evaluations by featuring program-based verification and controllable difficulty levels. It includes 1,575 English and 883 Chinese puzzles, enabling more reliable evaluation that better distinguishes models' reasoning capabilities across languages.",0.895,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Avg@16,,,cnmo-2024,Cnmo 2024,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10016.0,,kimi-k2-instruct-0905,,,,0.743,,,moonshotai,,,,,,,,0.743,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",CNMO 2024,['math'],text,False,1.0,en,China Mathematical Olympiad 2024 - A challenging mathematics competition.,0.743,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Diamond - Avg@8,,,gpqa,Gpqa,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10020.0,,kimi-k2-instruct-0905,,,,0.751,,,moonshotai,,,,,,,,0.751,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.751,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Text Only,,,hle,Hle,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10022.0,,kimi-k2-instruct-0905,,,,0.047,,,moonshotai,,,,,,,,0.047,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.047,0.047,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",HLE,"['reasoning', 'math']",multimodal,False,1.0,en,"Humanity's Last Exam (HLE) is a multi-modal academic benchmark with 2,500 questions across mathematics, humanities, and natural sciences, designed to test LLM capabilities at the frontier of human knowledge with unambiguous, verifiable solutions",0.047,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Avg@32,,,hmmt-2025,Hmmt 2025,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10015.0,,kimi-k2-instruct-0905,,,,0.388,,,moonshotai,,,,,,,,0.388,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.388,0.388,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",HMMT 2025,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.388,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Prompt Strict,,,ifeval,Ifeval,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10026.0,,kimi-k2-instruct-0905,,,,0.898,,,moonshotai,,,,,,,,0.898,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.898,0.898,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.898,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,2024/11/25 Pass@1,,,livebench,Livebench,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10029.0,,kimi-k2-instruct-0905,,,,0.764,,,moonshotai,,,,,,,,0.764,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.764,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,v6 (Aug 24-May 25) Pass@1,,,livecodebench,Livecodebench,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10004.0,,kimi-k2-instruct-0905,,,,0.537,,,moonshotai,,,,,,,,0.537,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.537,0.537,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.537,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,math-500,Math 500,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10014.0,,kimi-k2-instruct-0905,,,,0.974,,,moonshotai,,,,,,,,0.974,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.974,0.974,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.974,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),kimi
,EM,,,mmlu,Mmlu,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10023.0,,kimi-k2-instruct-0905,,,,0.895,,,moonshotai,,,,,,,,0.895,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.895,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,mmlu-pro,Mmlu Pro,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10025.0,,kimi-k2-instruct-0905,,,,0.811,,,moonshotai,,,,,,,,0.811,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.811,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,EM,,,mmlu-redux,Mmlu Redux,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10024.0,,kimi-k2-instruct-0905,,,,0.927,,,moonshotai,,,,,,,,0.927,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.927,0.927,Unknown,,,,,Undisclosed,Excellent (90%+),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.927,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),kimi
,Accuracy,,,multichallenge,Multichallenge,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10027.0,,kimi-k2-instruct-0905,,,,0.541,,,moonshotai,,,,,,,,0.541,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.541,0.541,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.541,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Pass@1,,,multipl-e,Multiple,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10006.0,,kimi-k2-instruct-0905,,,,0.857,,,moonshotai,,,,,,,,0.857,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.857,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,Pass@1,,,ojbench,Ojbench,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10005.0,,kimi-k2-instruct-0905,,,,0.271,,,moonshotai,,,,,,,,0.271,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.271,0.271,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",OJBench,['reasoning'],text,False,1.0,en,"OJBench is a competition-level code benchmark designed to assess the competitive-level code reasoning abilities of large language models. It comprises 232 programming competition problems from NOI and ICPC, categorized into Easy, Medium, and Hard difficulty levels. The benchmark evaluates models' ability to solve complex competitive programming challenges using Python and C++.",0.271,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Avg@4,,,polymath-en,Polymath En,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10017.0,,kimi-k2-instruct-0905,,,,0.651,,,moonshotai,,,,,,,,0.651,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.651,0.651,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",PolyMath-en,"['math', 'reasoning']",text,True,1.0,en,"PolyMath is a multilingual mathematical reasoning benchmark covering 18 languages and 4 difficulty levels from easy to hard, ensuring difficulty comprehensiveness, language diversity, and high-quality translation. The benchmark evaluates mathematical reasoning capabilities of large language models across diverse linguistic contexts, making it a highly discriminative multilingual mathematical benchmark.",0.651,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Correct,,,simpleqa,Simpleqa,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10028.0,,kimi-k2-instruct-0905,,,,0.31,,,moonshotai,,,,,,,,0.31,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.31,0.31,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.31,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,supergpqa,Supergpqa,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10021.0,,kimi-k2-instruct-0905,,,,0.572,,,moonshotai,,,,,,,,0.572,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.572,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Agentic Coding - Single Attempt,,,swe-bench-multilingual,Swe Bench Multilingual,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10002.0,,kimi-k2-instruct-0905,,,,0.473,,,moonshotai,,,,,,,,0.473,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.473,0.473,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",SWE-bench Multilingual,"['reasoning', 'code']",text,True,1.0,en,"A multilingual benchmark for issue resolving in software engineering that covers Java, TypeScript, JavaScript, Go, Rust, C, and C++. Contains 1,632 high-quality instances carefully annotated from 2,456 candidates by 68 expert annotators, designed to evaluate Large Language Models across diverse software ecosystems beyond Python.",0.473,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Agentic Coding - Single Attempt,,,swe-bench-verified,Swe Bench Verified,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10001.0,,kimi-k2-instruct-0905,,,,0.658,,,moonshotai,,,,,,,,0.658,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,"65.8% single attempt, 71.6% multiple",,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.658,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Avg@4,,,tau2-airline,Tau2 Airline,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10009.0,,kimi-k2-instruct-0905,,,,0.565,,,moonshotai,,,,,,,,0.565,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.565,0.565,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.565,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Avg@4,,,tau2-retail,Tau2 Retail,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10008.0,,kimi-k2-instruct-0905,,,,0.706,,,moonshotai,,,,,,,,0.706,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.706,0.706,Unknown,,,,,Undisclosed,Good (70-79%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.706,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),kimi
,Avg@4,,,tau2-telecom,Tau2 Telecom,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10010.0,,kimi-k2-instruct-0905,,,,0.658,,,moonshotai,,,,,,,,0.658,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.658,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),kimi
,Terminus,,,terminal-bench,Terminal Bench,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10003.0,,kimi-k2-instruct-0905,,,,0.25,,,moonshotai,,,,,,,,0.25,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.25,0.25,Unknown,,,,,Undisclosed,Poor (<60%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",Terminal-Bench,"['reasoning', 'code']",text,False,1.0,en,"Terminal-Bench is a benchmark for testing AI agents in real terminal environments. It evaluates how well agents can handle real-world, end-to-end tasks autonomously, including compiling code, training models, setting up servers, system administration, security tasks, data science workflows, and cybersecurity vulnerabilities. The benchmark consists of a dataset of ~100 hand-crafted, human-verified tasks and an execution harness that connects language models to a terminal sandbox.",0.25,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),kimi
,Accuracy,,,zebralogic,Zebralogic,,,2025-09-05T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,10018.0,,kimi-k2-instruct-0905,,,,0.89,,,moonshotai,,,,,,,,0.89,https://moonshotai.github.io/Kimi-K2/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Kimi K2-Instruct-0905,moonshotai,Moonshot AI,1000000000000.0,1000000000000.0,True,15500000000000.0,True,False,mit,Open & Permissive,2025-09-05,2025.0,9.0,2025-09,Very Large (>70B),"Kimi K2-Instruct-0905 is the latest, most capable version of Kimi K2, achieving state-of-the-art performance in frontier knowledge, math, and coding among non-thinking models. This Mixture-of-Experts model features 32 billion activated parameters and 1 trillion total parameters, meticulously optimized for agentic tasks. Key features include enhanced agentic coding intelligence, extended context length to 256K tokens, and a hybrid architecture trained with MuonClip optimizer on 15.5T tokens. The model achieves 65.8% on SWE-bench Verified (single attempt), 47.3% on SWE-bench Multilingual, and excels at tool use with 70.6% on Tau2-retail. It is a reflex-grade model without long thinking, designed to act and execute complex tasks seamlessly.",ZebraLogic,['reasoning'],text,False,1.0,en,"ZebraLogic is an evaluation framework for assessing large language models' logical reasoning capabilities through logic grid puzzles derived from constraint satisfaction problems (CSPs). The benchmark consists of 1,000 programmatically generated puzzles with controllable and quantifiable complexity, revealing a 'curse of complexity' where model accuracy declines significantly as problem complexity grows.",0.89,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),kimi
,0-shot,,,api-bank,API-Bank,,,2025-07-19T19:56:14.382379+00:00,benchmark_result,,,,True,,,,,,1562.0,,llama-3.1-405b-instruct,,,,0.92,,,meta,,,,,,,,0.92,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.382379+00:00,,,,,,False,,False,0.0,False,0.0,0.92,0.92,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,API-Bank,['reasoning'],text,False,1.0,en,"A comprehensive benchmark for tool-augmented LLMs that evaluates API planning, retrieval, and calling capabilities. Contains 314 tool-use dialogues with 753 API calls across 73 API tools, designed to assess how effectively LLMs can utilize external tools and overcome obstacles in tool leveraging.",0.92,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,0-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.118562+00:00,benchmark_result,,,,True,,,,,,16.0,,llama-3.1-405b-instruct,,,,0.969,,,meta,,,,,,,,0.969,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:11.118562+00:00,,,,,,False,,False,0.0,False,0.0,0.969,0.969,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.969,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,0-shot,,,bfcl,BFCL,,,2025-07-19T19:56:12.775431+00:00,benchmark_result,,,,True,,,,,,848.0,,llama-3.1-405b-instruct,,,,0.885,,,meta,,,,,,,,0.885,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:12.775431+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.885,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,drop,DROP,,,2025-07-19T19:56:13.004517+00:00,benchmark_result,,,,True,,,,,,950.0,,llama-3.1-405b-instruct,,,,0.848,,,meta,,,,,,,,0.848,https://arxiv.org/pdf/2407.21783,,,,,,,,2025-07-19T19:56:13.004517+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.848,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,gorilla-benchmark-api-bench,Gorilla Benchmark API Bench,,,2025-07-19T19:56:14.390263+00:00,benchmark_result,,,,True,,,,,,1565.0,,llama-3.1-405b-instruct,,,,0.353,,,meta,,,,,,,,0.353,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.390263+00:00,,,,,,False,,False,0.0,False,0.0,0.353,0.353,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,Gorilla Benchmark API Bench,"['reasoning', 'code']",text,False,1.0,en,"APIBench, a comprehensive dataset of over 11,000 instruction-API pairs from HuggingFace, TorchHub, and TensorHub APIs for evaluating language models' ability to generate accurate API calls.",0.353,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.662460+00:00,benchmark_result,,,,True,,,,,,291.0,,llama-3.1-405b-instruct,,,,0.507,,,meta,,,,,,,,0.507,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:11.662460+00:00,,,,,,False,,False,0.0,False,0.0,0.507,0.507,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.507,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"8-shot, CoT, em_maj1@1",,,gsm8k,GSM8k,,,2025-07-19T19:56:13.071677+00:00,benchmark_result,,,,True,,,,,,988.0,,llama-3.1-405b-instruct,,,,0.968,,,meta,,,,,,,,0.968,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:13.071677+00:00,,,,,,False,,False,0.0,False,0.0,0.968,0.968,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.968,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,"0-shot, pass@1",,,humaneval,HumanEval,,,2025-07-19T19:56:12.636480+00:00,benchmark_result,,,,True,,,,,,780.0,,llama-3.1-405b-instruct,,,,0.89,,,meta,,,,,,,,0.89,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:12.636480+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.89,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Standard,,,ifeval,IFEval,,,2025-07-19T19:56:12.270752+00:00,benchmark_result,,,,True,,,,,,616.0,,llama-3.1-405b-instruct,,,,0.886,,,meta,,,,,,,,0.886,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:12.270752+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.886,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"0-shot, CoT, final_em",,,math,MATH,,,2025-07-19T19:56:11.846056+00:00,benchmark_result,,,,True,,,,,,394.0,,llama-3.1-405b-instruct,,,,0.738,,,meta,,,,,,,,0.738,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:11.846056+00:00,,,,,,False,,False,0.0,False,0.0,0.738,0.738,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.738,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, base, pass@1",,,mbpp-evalplus,MBPP EvalPlus,,,2025-07-19T19:56:14.428183+00:00,benchmark_result,,,,True,,,,,,1578.0,,llama-3.1-405b-instruct,,,,0.886,,,meta,,,,,,,,0.886,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.428183+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,MBPP EvalPlus,"['reasoning', 'general']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. EvalPlus extends MBPP with significantly more test cases (35x) for more rigorous evaluation of LLM-synthesized code, providing high-quality and precise evaluation.",0.886,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"5-shot, macro_avg/acc",,,mmlu,MMLU,,,2025-07-19T19:56:11.249582+00:00,benchmark_result,,,,True,,,,,,79.0,,llama-3.1-405b-instruct,,,,0.873,,,meta,,,,,,,,0.873,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:11.249582+00:00,,,,,,False,,False,0.0,False,0.0,0.873,0.873,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.873,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"0-shot, macro_avg/acc",,,mmlu-(cot),MMLU (CoT),,,2025-07-19T19:56:14.339647+00:00,benchmark_result,,,,True,,,,,,1548.0,,llama-3.1-405b-instruct,,,,0.886,,,meta,,,,,,,,0.886,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.339647+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,MMLU (CoT),"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"Chain-of-Thought variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses chain-of-thought prompting to elicit step-by-step reasoning.",0.886,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"5-shot, CoT, micro_avg/acc_char",,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.458814+00:00,benchmark_result,,,,True,,,,,,186.0,,llama-3.1-405b-instruct,,,,0.733,,,meta,,,,,,,,0.733,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:11.458814+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.733,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, CoT, em",,,multilingual-mgsm-(cot),Multilingual MGSM (CoT),,,2025-07-19T19:56:14.409472+00:00,benchmark_result,,,,True,,,,,,1572.0,,llama-3.1-405b-instruct,,,,0.916,,,meta,,,,,,,,0.916,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.409472+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,Multilingual MGSM (CoT),"['math', 'reasoning']",text,True,1.0,en,Multilingual Grade School Math (MGSM) benchmark evaluates language models' chain-of-thought reasoning abilities across ten typologically diverse languages. Contains 250 grade-school math problems manually translated from GSM8K dataset into languages including Bengali and Swahili.,0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,"0-shot, pass@1",,,multipl-e-humaneval,Multipl-E HumanEval,,,2025-07-19T19:56:14.352505+00:00,benchmark_result,,,,True,,,,,,1552.0,,llama-3.1-405b-instruct,,,,0.752,,,meta,,,,,,,,0.752,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.352505+00:00,,,,,,False,,False,0.0,False,0.0,0.752,0.752,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,Multipl-E HumanEval,"['language', 'general']",text,True,1.0,en,"MultiPL-E is a scalable and extensible approach to benchmarking neural code generation that translates unit test-driven code generation benchmarks across multiple programming languages. It extends the HumanEval benchmark to 18 additional programming languages, enabling evaluation of code generation models across diverse programming paradigms and providing insights into how models generalize programming knowledge across language boundaries.",0.752,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, pass@1",,,multipl-e-mbpp,Multipl-E MBPP,,,2025-07-19T19:56:14.359473+00:00,benchmark_result,,,,True,,,,,,1555.0,,llama-3.1-405b-instruct,,,,0.657,,,meta,,,,,,,,0.657,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.359473+00:00,,,,,,False,,False,0.0,False,0.0,0.657,0.657,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,Multipl-E MBPP,"['general', 'reasoning']",text,True,1.0,en,"MultiPL-E extends the Mostly Basic Python Problems (MBPP) benchmark to 18+ programming languages for evaluating multilingual code generation capabilities. MBPP contains 974 crowd-sourced programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality. Each problem includes a task description, code solution, and automated test cases.",0.657,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"0-shot, macro_avg/acc",,,nexus,Nexus,,,2025-07-19T19:56:14.398966+00:00,benchmark_result,,,,True,,,,,,1568.0,,llama-3.1-405b-instruct,,,,0.587,,,meta,,,,,,,,0.587,https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct,,,,,,,,2025-07-19T19:56:14.398966+00:00,,,,,,False,,False,0.0,False,0.0,0.587,0.587,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 405B Instruct,meta,Meta,405000000000.0,405000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 405B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks. The model supports 8 languages and has a 128K token context length.,Nexus,['general'],text,False,1.0,en,NexusRaven benchmark for evaluating function calling capabilities of large language models in zero-shot scenarios across cybersecurity tools and API interactions,0.587,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,api-bank,API-Bank,,,2025-07-19T19:56:14.378301+00:00,benchmark_result,,,,True,,,,,,1560.0,,llama-3.1-70b-instruct,,,,0.9,,,meta,,,,,,,,0.9,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.378301+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,API-Bank,['reasoning'],text,False,1.0,en,"A comprehensive benchmark for tool-augmented LLMs that evaluates API planning, retrieval, and calling capabilities. Contains 314 tool-use dialogues with 753 API calls across 73 API tools, designed to assess how effectively LLMs can utilize external tools and overcome obstacles in tool leveraging.",0.9,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,0-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.113697+00:00,benchmark_result,,,,True,,,,,,14.0,,llama-3.1-70b-instruct,,,,0.948,,,meta,,,,,,,,0.948,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:11.113697+00:00,,,,,,False,,False,0.0,False,0.0,0.948,0.948,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.948,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,Standard evaluation,,,bfcl,BFCL,,,2025-07-19T19:56:12.771784+00:00,benchmark_result,,,,True,,,,,,846.0,,llama-3.1-70b-instruct,,,,0.848,,,meta,,,,,,,,0.848,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:12.771784+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.848,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,drop,DROP,,,2025-07-19T19:56:13.001514+00:00,benchmark_result,,,,True,,,,,,948.0,,llama-3.1-70b-instruct,,,,0.796,,,meta,,,,,,,,0.796,https://arxiv.org/pdf/2407.21783,,,,,,,,2025-07-19T19:56:13.001514+00:00,,,,,,False,,False,0.0,False,0.0,0.796,0.796,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.796,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot,,,gorilla-benchmark-api-bench,Gorilla Benchmark API Bench,,,2025-07-19T19:56:14.386457+00:00,benchmark_result,,,,True,,,,,,1563.0,,llama-3.1-70b-instruct,,,,0.297,,,meta,,,,,,,,0.297,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.386457+00:00,,,,,,False,,False,0.0,False,0.0,0.297,0.297,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,Gorilla Benchmark API Bench,"['reasoning', 'code']",text,False,1.0,en,"APIBench, a comprehensive dataset of over 11,000 instruction-API pairs from HuggingFace, TorchHub, and TensorHub APIs for evaluating language models' ability to generate accurate API calls.",0.297,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.657221+00:00,benchmark_result,,,,True,,,,,,288.0,,llama-3.1-70b-instruct,,,,0.417,,,meta,,,,,,,,0.417,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:11.657221+00:00,,,,,,False,,False,0.0,False,0.0,0.417,0.417,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.417,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,8-shot Chain-of-Thought,,,gsm-8k-(cot),GSM-8K (CoT),,,2025-07-19T19:56:14.362878+00:00,benchmark_result,,,,True,,,,,,1556.0,,llama-3.1-70b-instruct,,,,0.951,,,meta,,,,,,,,0.951,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.362878+00:00,,,,,,False,,False,0.0,False,0.0,0.951,0.951,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,GSM-8K (CoT),"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K with Chain-of-Thought prompting, featuring 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.951,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.632931+00:00,benchmark_result,,,,True,,,,,,778.0,,llama-3.1-70b-instruct,,,,0.805,,,meta,,,,,,,,0.805,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:12.632931+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.805,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Standard evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.266791+00:00,benchmark_result,,,,True,,,,,,614.0,,llama-3.1-70b-instruct,,,,0.875,,,meta,,,,,,,,0.875,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:12.266791+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.875,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot Chain-of-Thought,,,math-(cot),MATH (CoT),,,2025-07-19T19:56:14.371489+00:00,benchmark_result,,,,True,,,,,,1558.0,,llama-3.1-70b-instruct,,,,0.68,,,meta,,,,,,,,0.68,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.371489+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,MATH (CoT),"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects. This variant uses Chain-of-Thought prompting to encourage step-by-step reasoning.",0.68,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot,,,mbpp-++-base-version,MBPP ++ base version,,,2025-07-19T19:56:14.344061+00:00,benchmark_result,,,,True,,,,,,1549.0,,llama-3.1-70b-instruct,,,,0.86,,,meta,,,,,,,,0.86,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.344061+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,MBPP ++ base version,"['reasoning', 'general']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality. This is an enhanced version with additional test cases.",0.86,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.243294+00:00,benchmark_result,,,,True,,,,,,76.0,,llama-3.1-70b-instruct,,,,0.836,,,meta,,,,,,,,0.836,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:11.243294+00:00,,,,,,False,,False,0.0,False,0.0,0.836,0.836,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.836,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot Chain-of-Thought,,,mmlu-(cot),MMLU (CoT),,,2025-07-19T19:56:14.334507+00:00,benchmark_result,,,,True,,,,,,1546.0,,llama-3.1-70b-instruct,,,,0.86,,,meta,,,,,,,,0.86,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.334507+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,MMLU (CoT),"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"Chain-of-Thought variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses chain-of-thought prompting to elicit step-by-step reasoning.",0.86,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,5-shot Chain-of-Thought,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.455089+00:00,benchmark_result,,,,True,,,,,,184.0,,llama-3.1-70b-instruct,,,,0.664,,,meta,,,,,,,,0.664,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:11.455089+00:00,,,,,,False,,False,0.0,False,0.0,0.664,0.664,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.664,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot Chain-of-Thought,,,multilingual-mgsm-(cot),Multilingual MGSM (CoT),,,2025-07-19T19:56:14.405488+00:00,benchmark_result,,,,True,,,,,,1570.0,,llama-3.1-70b-instruct,,,,0.869,,,meta,,,,,,,,0.869,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.405488+00:00,,,,,,False,,False,0.0,False,0.0,0.869,0.869,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,Multilingual MGSM (CoT),"['math', 'reasoning']",text,True,1.0,en,Multilingual Grade School Math (MGSM) benchmark evaluates language models' chain-of-thought reasoning abilities across ten typologically diverse languages. Contains 250 grade-school math problems manually translated from GSM8K dataset into languages including Bengali and Swahili.,0.869,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,multipl-e-humaneval,Multipl-E HumanEval,,,2025-07-19T19:56:14.347431+00:00,benchmark_result,,,,True,,,,,,1550.0,,llama-3.1-70b-instruct,,,,0.655,,,meta,,,,,,,,0.655,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.347431+00:00,,,,,,False,,False,0.0,False,0.0,0.655,0.655,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,Multipl-E HumanEval,"['language', 'general']",text,True,1.0,en,"MultiPL-E is a scalable and extensible approach to benchmarking neural code generation that translates unit test-driven code generation benchmarks across multiple programming languages. It extends the HumanEval benchmark to 18 additional programming languages, enabling evaluation of code generation models across diverse programming paradigms and providing insights into how models generalize programming knowledge across language boundaries.",0.655,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot,,,multipl-e-mbpp,Multipl-E MBPP,,,2025-07-19T19:56:14.356043+00:00,benchmark_result,,,,True,,,,,,1553.0,,llama-3.1-70b-instruct,,,,0.62,,,meta,,,,,,,,0.62,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.356043+00:00,,,,,,False,,False,0.0,False,0.0,0.62,0.62,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,Multipl-E MBPP,"['general', 'reasoning']",text,True,1.0,en,"MultiPL-E extends the Mostly Basic Python Problems (MBPP) benchmark to 18+ programming languages for evaluating multilingual code generation capabilities. MBPP contains 974 crowd-sourced programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality. Each problem includes a task description, code solution, and automated test cases.",0.62,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot,,,nexus,Nexus,,,2025-07-19T19:56:14.394299+00:00,benchmark_result,,,,True,,,,,,1566.0,,llama-3.1-70b-instruct,,,,0.567,,,meta,,,,,,,,0.567,https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct,,,,,,,,2025-07-19T19:56:14.394299+00:00,,,,,,False,,False,0.0,False,0.0,0.567,0.567,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),Llama 3.1 70B Instruct is a large language model optimized for multilingual dialogue use cases. It outperforms many available open source and closed chat models on common industry benchmarks.,Nexus,['general'],text,False,1.0,en,NexusRaven benchmark for evaluating function calling capabilities of large language models in zero-shot scenarios across cybersecurity tools and API interactions,0.567,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,api-bank,API-Bank,,,2025-07-19T19:56:14.380088+00:00,benchmark_result,,,,True,,,,,,1561.0,,llama-3.1-8b-instruct,,,,0.826,,,meta,,,,,,,,0.826,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.380088+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",API-Bank,['reasoning'],text,False,1.0,en,"A comprehensive benchmark for tool-augmented LLMs that evaluates API planning, retrieval, and calling capabilities. Contains 314 tool-use dialogues with 753 API calls across 73 API tools, designed to assess how effectively LLMs can utilize external tools and overcome obstacles in tool leveraging.",0.826,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.115810+00:00,benchmark_result,,,,True,,,,,,15.0,,llama-3.1-8b-instruct,,,,0.834,,,meta,,,,,,,,0.834,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:11.115810+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.834,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,bfcl,BFCL,,,2025-07-19T19:56:12.773659+00:00,benchmark_result,,,,True,,,,,,847.0,,llama-3.1-8b-instruct,,,,0.761,,,meta,,,,,,,,0.761,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:12.773659+00:00,,,,,,False,,False,0.0,False,0.0,0.761,0.761,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.761,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot,,,drop,DROP,,,2025-07-19T19:56:13.003032+00:00,benchmark_result,,,,True,,,,,,949.0,,llama-3.1-8b-instruct,,,,0.595,,,meta,,,,,,,,0.595,https://arxiv.org/pdf/2407.21783,,,,,,,,2025-07-19T19:56:13.003032+00:00,,,,,,False,,False,0.0,False,0.0,0.595,0.595,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.595,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,gorilla-benchmark-api-bench,Gorilla Benchmark API Bench,,,2025-07-19T19:56:14.388429+00:00,benchmark_result,,,,True,,,,,,1564.0,,llama-3.1-8b-instruct,,,,0.082,,,meta,,,,,,,,0.082,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.388429+00:00,,,,,,False,,False,0.0,False,0.0,0.082,0.082,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",Gorilla Benchmark API Bench,"['reasoning', 'code']",text,False,1.0,en,"APIBench, a comprehensive dataset of over 11,000 instruction-API pairs from HuggingFace, TorchHub, and TensorHub APIs for evaluating language models' ability to generate accurate API calls.",0.082,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,gpqa,GPQA,,,2025-07-19T19:56:11.660952+00:00,benchmark_result,,,,True,,,,,,290.0,,llama-3.1-8b-instruct,,,,0.304,,,meta,,,,,,,,0.304,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:11.660952+00:00,,,,,,False,,False,0.0,False,0.0,0.304,0.304,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.304,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,8-shot,,,gsm-8k-(cot),GSM-8K (CoT),,,2025-07-19T19:56:14.364382+00:00,benchmark_result,,,,True,,,,,,1557.0,,llama-3.1-8b-instruct,,,,0.845,,,meta,,,,,,,,0.845,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.364382+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",GSM-8K (CoT),"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K with Chain-of-Thought prompting, featuring 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.845,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.634981+00:00,benchmark_result,,,,True,,,,,,779.0,,llama-3.1-8b-instruct,,,,0.726,,,meta,,,,,,,,0.726,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:12.634981+00:00,,,,,,False,,False,0.0,False,0.0,0.726,0.726,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.726,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,unspecified,,,ifeval,IFEval,,,2025-07-19T19:56:12.268709+00:00,benchmark_result,,,,True,,,,,,615.0,,llama-3.1-8b-instruct,,,,0.804,,,meta,,,,,,,,0.804,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:12.268709+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.804,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot,,,math-(cot),MATH (CoT),,,2025-07-19T19:56:14.373274+00:00,benchmark_result,,,,True,,,,,,1559.0,,llama-3.1-8b-instruct,,,,0.519,,,meta,,,,,,,,0.519,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.373274+00:00,,,,,,False,,False,0.0,False,0.0,0.519,0.519,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",MATH (CoT),"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects. This variant uses Chain-of-Thought prompting to encourage step-by-step reasoning.",0.519,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,mbpp-evalplus-(base),MBPP EvalPlus (base),,,2025-07-19T19:56:14.424442+00:00,benchmark_result,,,,True,,,,,,1577.0,,llama-3.1-8b-instruct,,,,0.728,,,meta,,,,,,,,0.728,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.424442+00:00,,,,,,False,,False,0.0,False,0.0,0.728,0.728,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",MBPP EvalPlus (base),"['reasoning', 'general']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. EvalPlus extends MBPP with significantly more test cases (35x) for more rigorous evaluation of LLM-synthesized code, providing high-quality and precise evaluation.",0.728,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.247675+00:00,benchmark_result,,,,True,,,,,,78.0,,llama-3.1-8b-instruct,,,,0.694,,,meta,,,,,,,,0.694,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:11.247675+00:00,,,,,,False,,False,0.0,False,0.0,0.694,0.694,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.694,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot,,,mmlu-(cot),MMLU (CoT),,,2025-07-19T19:56:14.337443+00:00,benchmark_result,,,,True,,,,,,1547.0,,llama-3.1-8b-instruct,,,,0.73,,,meta,,,,,,,,0.73,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.337443+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",MMLU (CoT),"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"Chain-of-Thought variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses chain-of-thought prompting to elicit step-by-step reasoning.",0.73,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,5-shot,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.457212+00:00,benchmark_result,,,,True,,,,,,185.0,,llama-3.1-8b-instruct,,,,0.483,,,meta,,,,,,,,0.483,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:11.457212+00:00,,,,,,False,,False,0.0,False,0.0,0.483,0.483,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.483,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,multilingual-mgsm-(cot),Multilingual MGSM (CoT),,,2025-07-19T19:56:14.407707+00:00,benchmark_result,,,,True,,,,,,1571.0,,llama-3.1-8b-instruct,,,,0.689,,,meta,,,,,,,,0.689,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.407707+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",Multilingual MGSM (CoT),"['math', 'reasoning']",text,True,1.0,en,Multilingual Grade School Math (MGSM) benchmark evaluates language models' chain-of-thought reasoning abilities across ten typologically diverse languages. Contains 250 grade-school math problems manually translated from GSM8K dataset into languages including Bengali and Swahili.,0.689,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot,,,multipl-e-humaneval,Multipl-E HumanEval,,,2025-07-19T19:56:14.350301+00:00,benchmark_result,,,,True,,,,,,1551.0,,llama-3.1-8b-instruct,,,,0.508,,,meta,,,,,,,,0.508,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.350301+00:00,,,,,,False,,False,0.0,False,0.0,0.508,0.508,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",Multipl-E HumanEval,"['language', 'general']",text,True,1.0,en,"MultiPL-E is a scalable and extensible approach to benchmarking neural code generation that translates unit test-driven code generation benchmarks across multiple programming languages. It extends the HumanEval benchmark to 18 additional programming languages, enabling evaluation of code generation models across diverse programming paradigms and providing insights into how models generalize programming knowledge across language boundaries.",0.508,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,multipl-e-mbpp,Multipl-E MBPP,,,2025-07-19T19:56:14.357886+00:00,benchmark_result,,,,True,,,,,,1554.0,,llama-3.1-8b-instruct,,,,0.524,,,meta,,,,,,,,0.524,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.357886+00:00,,,,,,False,,False,0.0,False,0.0,0.524,0.524,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",Multipl-E MBPP,"['general', 'reasoning']",text,True,1.0,en,"MultiPL-E extends the Mostly Basic Python Problems (MBPP) benchmark to 18+ programming languages for evaluating multilingual code generation capabilities. MBPP contains 974 crowd-sourced programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality. Each problem includes a task description, code solution, and automated test cases.",0.524,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot,,,nexus,Nexus,,,2025-07-19T19:56:14.396611+00:00,benchmark_result,,,,True,,,,,,1567.0,,llama-3.1-8b-instruct,,,,0.385,,,meta,,,,,,,,0.385,https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct,,,,,,,,2025-07-19T19:56:14.396611+00:00,,,,,,False,,False,0.0,False,0.0,0.385,0.385,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 8B Instruct,meta,Meta,8000000000.0,8000000000.0,True,15000000000000.0,True,False,llama_3_1_community_license,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Llama 3.1 8B Instruct is a multilingual large language model optimized for dialogue use cases. It features a 128K context length, state-of-the-art tool use, and strong reasoning capabilities.",Nexus,['general'],text,False,1.0,en,NexusRaven benchmark for evaluating function calling capabilities of large language models in zero-shot scenarios across cybersecurity tools and API interactions,0.385,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,Standard evaluation,,,arc-c,ARC-C,,,2025-07-19T19:56:11.133318+00:00,benchmark_result,,,,True,,,,,,24.0,,llama-3.1-nemotron-70b-instruct,,,,0.692,,,nvidia,,,,,,,,0.692,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:11.133318+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.692,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,Standard evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.099846+00:00,benchmark_result,,,,True,,,,,,1005.0,,llama-3.1-nemotron-70b-instruct,,,,0.9143,,,nvidia,,,,,,,,0.9143,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:13.099846+00:00,,,,,,False,,False,0.0,False,0.0,0.9143,0.9143,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.9143,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,Chat evaluation,,,gsm8k-chat,GSM8K Chat,,,2025-07-19T19:56:15.104394+00:00,benchmark_result,,,,True,,,,,,1811.0,,llama-3.1-nemotron-70b-instruct,,,,0.8188,,,nvidia,,,,,,,,0.8188,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:15.104394+00:00,,,,,,False,,False,0.0,False,0.0,0.8188,0.8188,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,GSM8K Chat,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K adapted for chat format evaluation, featuring 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.8188,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Standard evaluation,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.188734+00:00,benchmark_result,,,,True,,,,,,50.0,,llama-3.1-nemotron-70b-instruct,,,,0.8558,,,nvidia,,,,,,,,0.8558,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:11.188734+00:00,,,,,,False,,False,0.0,False,0.0,0.8558,0.8558,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.8558,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Code evaluation (n=20),,,instruct-humaneval,Instruct HumanEval,,,2025-07-19T19:56:15.108307+00:00,benchmark_result,,,,True,,,,,,1812.0,,llama-3.1-nemotron-70b-instruct,,,,0.7384,,,nvidia,,,,,,,,0.7384,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:15.108307+00:00,,,,,,False,,False,0.0,False,0.0,0.7384,0.7384,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,Instruct HumanEval,['general'],text,False,1.0,en,Instruction-based variant of HumanEval benchmark for evaluating large language models' code generation capabilities with functional correctness using pass@k metric on programming problems,0.7384,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,Standard evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.292516+00:00,benchmark_result,,,,True,,,,,,102.0,,llama-3.1-nemotron-70b-instruct,,,,0.802,,,nvidia,,,,,,,,0.802,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:11.292516+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.802,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Chat evaluation,,,mmlu-chat,MMLU Chat,,,2025-07-19T19:56:15.100072+00:00,benchmark_result,,,,True,,,,,,1810.0,,llama-3.1-nemotron-70b-instruct,,,,0.8058,,,nvidia,,,,,,,,0.8058,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:15.100072+00:00,,,,,,False,,False,0.0,False,0.0,0.8058,0.8058,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,MMLU Chat,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"Chat-format variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This version uses conversational prompting format for model evaluation.",0.8058,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Chat evaluation,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.532800+00:00,benchmark_result,,,,True,,,,,,1611.0,,llama-3.1-nemotron-70b-instruct,,,,0.0899,,,nvidia,,,,,,,,0.0899,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:14.532800+00:00,,,,,,False,,False,0.0,False,0.0,0.0899,0.0899,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.000899,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),llama
,Standard evaluation,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.363751+00:00,benchmark_result,,,,True,,,,,,143.0,,llama-3.1-nemotron-70b-instruct,,,,0.5863,,,nvidia,,,,,,,,0.5863,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:11.363751+00:00,,,,,,False,,False,0.0,False,0.0,0.5863,0.5863,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.5863,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,Standard evaluation,,,winogrande,Winogrande,,,2025-07-19T19:56:11.390043+00:00,benchmark_result,,,,True,,,,,,153.0,,llama-3.1-nemotron-70b-instruct,,,,0.8453,,,nvidia,,,,,,,,0.8453,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:11.390043+00:00,,,,,,False,,False,0.0,False,0.0,0.8453,0.8453,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.8453,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,Standard evaluation,,,xlsum-english,XLSum English,,,2025-07-19T19:56:15.094560+00:00,benchmark_result,,,,True,,,,,,1809.0,,llama-3.1-nemotron-70b-instruct,,,,0.3161,,,nvidia,,,,,,,,0.3161,https://developer.nvidia.com/blog/advancing-the-accuracy-efficiency-frontier-with-llama-3-1-nemotron-51b/,,,,,,,,2025-07-19T19:56:15.094560+00:00,,,,,,False,,False,0.0,False,0.0,0.3161,0.3161,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 Nemotron 70B Instruct,nvidia,NVIDIA,70000000000.0,70000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2024-10-01,2024.0,10.0,2024-10,Very Large (>70B),A large language model customized by NVIDIA to improve the helpfulness of LLM generated responses. It is a fine-tuned version of Llama 3.1 70B Instruct. The model was trained using RLHF (REINFORCE) with HelpSteer2-Preference prompts.,XLSum English,"['summarization', 'language']",text,True,1.0,en,"Large-scale multilingual abstractive summarization dataset comprising 1 million professionally annotated article-summary pairs from BBC, covering 44 languages. XL-Sum is highly abstractive, concise, and of high quality, designed to encourage research on multilingual abstractive summarization tasks.",0.3161,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Pass@1, Reasoning",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.461794+00:00,benchmark_result,,,,True,,,,,,698.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.471,,,nvidia,,,,,,,,0.471,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.461794+00:00,,,,,,False,,False,0.0,False,0.0,0.471,0.471,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.471,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Score, Reasoning",,,bfcl-v2,BFCL v2,,,2025-07-19T19:56:14.454860+00:00,benchmark_result,,,,True,,,,,,1586.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.636,,,nvidia,,,,,,,,0.636,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.454860+00:00,,,,,,False,,False,0.0,False,0.0,0.636,0.636,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",BFCL v2,"['general', 'reasoning']",text,True,1.0,en,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",0.636,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"Diamond, Pass@1, Reasoning",,,gpqa,GPQA,,,2025-07-19T19:56:11.719213+00:00,benchmark_result,,,,True,,,,,,327.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.541,,,nvidia,,,,,,,,0.541,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:11.719213+00:00,,,,,,False,,False,0.0,False,0.0,0.541,0.541,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.541,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Strict Accuracy, Reasoning",,,ifeval,IFEval,,,2025-07-19T19:56:12.289960+00:00,benchmark_result,,,,True,,,,,,627.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.793,,,nvidia,,,,,,,,0.793,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.289960+00:00,,,,,,False,,False,0.0,False,0.0,0.793,0.793,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.793,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Pass@1, Reasoning",,,math-500,MATH-500,,,2025-07-19T19:56:12.059893+00:00,benchmark_result,,,,True,,,,,,510.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.954,,,nvidia,,,,,,,,0.954,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.059893+00:00,,,,,,False,,False,0.0,False,0.0,0.954,0.954,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.954,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,"0-shot, Pass@1, Reasoning",,,mbpp,MBPP,,,2025-07-19T19:56:13.512976+00:00,benchmark_result,,,,True,,,,,,1193.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.846,,,nvidia,,,,,,,,0.846,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:13.512976+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00846,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Score, Reasoning",,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.530016+00:00,benchmark_result,,,,True,,,,,,1610.0,,llama-3.1-nemotron-nano-8b-v1,,,,0.81,,,nvidia,,,,,,,,0.81,https://build.nvidia.com/nvidia/llama-3_1-nemotron-nano-8b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.530016+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron Nano 8B V1,nvidia,NVIDIA,8000000000.0,8000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.1-Nemotron-Nano-8B-v1 is a large language model (LLM) which is a derivative of Meta Llama-3.1-8B-Instruct (AKA the reference model). It is a reasoning model that is post trained for reasoning, human chat preferences, and tasks, such as RAG and tool calling.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.008100000000000001,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Pass@1, Reasoning",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.463355+00:00,benchmark_result,,,,True,,,,,,699.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.725,,,nvidia,,,,,,,,0.725,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.463355+00:00,,,,,,False,,False,0.0,False,0.0,0.725,0.725,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.725,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Score, Reasoning",,,bfcl-v2,BFCL v2,,,2025-07-19T19:56:14.456840+00:00,benchmark_result,,,,True,,,,,,1587.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.741,,,nvidia,,,,,,,,0.741,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.456840+00:00,,,,,,False,,False,0.0,False,0.0,0.741,0.741,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",BFCL v2,"['general', 'reasoning']",text,True,1.0,en,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",0.741,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Pass@1, Reasoning",,,gpqa,GPQA,,,2025-07-19T19:56:11.721348+00:00,benchmark_result,,,,True,,,,,,328.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.7601,,,nvidia,,,,,,,,0.7601,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:11.721348+00:00,,,,,,False,,False,0.0,False,0.0,0.7601,0.7601,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.7601,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Strict Accuracy, Reasoning",,,ifeval,IFEval,,,2025-07-19T19:56:12.292359+00:00,benchmark_result,,,,True,,,,,,628.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.8945,,,nvidia,,,,,,,,0.8945,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.292359+00:00,,,,,,False,,False,0.0,False,0.0,0.8945,0.8945,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.8945,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"Pass@1, Reasoning",,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.404565+00:00,benchmark_result,,,,True,,,,,,1143.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.6631,,,nvidia,,,,,,,,0.6631,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:13.404565+00:00,,,,,,False,,False,0.0,False,0.0,0.6631,0.6631,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.6631,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"Pass@1, Reasoning",,,math-500,MATH-500,,,2025-07-19T19:56:12.061892+00:00,benchmark_result,,,,True,,,,,,511.0,,llama-3.1-nemotron-ultra-253b-v1,,,,0.97,,,nvidia,,,,,,,,0.97,https://build.nvidia.com/nvidia/llama-3_1-nemotron-ultra-253b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.061892+00:00,,,,,,False,,False,0.0,False,0.0,0.97,0.97,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.1 Nemotron Ultra 253B v1,nvidia,NVIDIA,253000000000.0,253000000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-04-07,2025.0,4.0,2025-04,Very Large (>70B),"A 253B parameter derivative of Meta Llama 3.1 405B Instruct, developed by NVIDIA using Neural Architecture Search (NAS) and vertical compression. It underwent multi-phase post-training (SFT for Math, Code, Reasoning, Chat, Tool Calling; RL with GRPO) to enhance reasoning and instruction-following. Optimized for accuracy/efficiency tradeoff on NVIDIA GPUs. Supports 128k context.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.97,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,Test accuracy,,,ai2d,AI2D,,,2025-07-19T19:56:13.631448+00:00,benchmark_result,,,,True,,,,,,1253.0,,llama-3.2-11b-instruct,,,,0.911,,,meta,,,,,,,,0.911,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:13.631448+00:00,,,,,,False,,False,0.0,False,0.0,0.911,0.911,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.911,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,"Test, 0-shot CoT relaxed accuracy",,,chartqa,ChartQA,,,2025-07-19T19:56:12.801741+00:00,benchmark_result,,,,True,,,,,,861.0,,llama-3.2-11b-instruct,,,,0.834,,,meta,,,,,,,,0.834,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:12.801741+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.834,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),llama
,Test ANLS,,,docvqa,DocVQA,,,2025-07-19T19:56:12.839416+00:00,benchmark_result,,,,True,,,,,,883.0,,llama-3.2-11b-instruct,,,,0.884,,,meta,,,,,,,,0.884,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:12.839416+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.884,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),llama
,"0-shot, CoT",,,gpqa,GPQA,,,2025-07-19T19:56:11.663962+00:00,benchmark_result,,,,True,,,,,,292.0,,llama-3.2-11b-instruct,,,,0.328,,,meta,,,,,,,,0.328,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:11.663962+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.328,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"0-shot, CoT",,,math,MATH,,,2025-07-19T19:56:11.847598+00:00,benchmark_result,,,,True,,,,,,395.0,,llama-3.2-11b-instruct,,,,0.519,,,meta,,,,,,,,0.519,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:11.847598+00:00,,,,,,False,,False,0.0,False,0.0,0.519,0.519,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.519,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,Test accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.086640+00:00,benchmark_result,,,,True,,,,,,523.0,,llama-3.2-11b-instruct,,,,0.515,,,meta,,,,,,,,0.515,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:12.086640+00:00,,,,,,False,,False,0.0,False,0.0,0.515,0.515,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.515,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,"0-shot, CoT",,,mgsm,MGSM,,,2025-07-19T19:56:13.690958+00:00,benchmark_result,,,,True,,,,,,1284.0,,llama-3.2-11b-instruct,,,,0.689,,,meta,,,,,,,,0.689,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:13.690958+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.689,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,Macro average accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.251362+00:00,benchmark_result,,,,True,,,,,,80.0,,llama-3.2-11b-instruct,,,,0.73,,,meta,,,,,,,,0.73,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:11.251362+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.73,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Val, 0-shot CoT, micro avg accuracy",,,mmmu,MMMU,,,2025-07-19T19:56:12.164872+00:00,benchmark_result,,,,True,,,,,,566.0,,llama-3.2-11b-instruct,,,,0.507,,,meta,,,,,,,,0.507,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:12.164872+00:00,,,,,,False,,False,0.0,False,0.0,0.507,0.507,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.507,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,Test accuracy,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.288730+00:00,benchmark_result,,,,True,,,,,,1530.0,,llama-3.2-11b-instruct,,,,0.33,,,meta,,,,,,,,0.33,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:14.288730+00:00,,,,,,False,,False,0.0,False,0.0,0.33,0.33,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.33,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,Accuracy,,,vqav2-(test),VQAv2 (test),,,2025-07-19T19:56:14.434081+00:00,benchmark_result,,,,True,,,,,,1580.0,,llama-3.2-11b-instruct,,,,0.752,,,meta,,,,,,,,0.752,https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct,,,,,,,,2025-07-19T19:56:14.434081+00:00,,,,,,False,,False,0.0,False,0.0,0.752,0.752,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 11B Instruct,meta,Meta,10600000000.0,10600000000.0,True,0.0,False,True,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 11B Vision Instruct is an instruction-tuned multimodal large language model optimized for visual recognition, image reasoning, captioning, and answering general questions about an image. It accepts text and images as input and generates text as output.",VQAv2 (test),"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"VQA v2.0 (Visual Question Answering v2.0) is a balanced dataset designed to counter language priors in visual question answering. It consists of complementary image pairs where the same question yields different answers, forcing models to rely on visual understanding rather than language bias. The dataset contains 1,105,904 questions across 204,721 COCO images, requiring understanding of vision, language, and commonsense knowledge.",0.752,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),llama
,"0-shot, acc",,,arc-c,ARC-C,,,2025-07-19T19:56:11.120164+00:00,benchmark_result,,,,True,,,,,,17.0,,llama-3.2-3b-instruct,,,,0.786,,,meta,,,,,,,,0.786,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:11.120164+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.786,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, acc",,,bfcl-v2,BFCL v2,,,2025-07-19T19:56:14.446368+00:00,benchmark_result,,,,True,,,,,,1583.0,,llama-3.2-3b-instruct,,,,0.67,,,meta,,,,,,,,0.67,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.446368+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",BFCL v2,"['general', 'reasoning']",text,True,1.0,en,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",0.67,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"0-shot, acc",,,gpqa,GPQA,,,2025-07-19T19:56:11.665423+00:00,benchmark_result,,,,True,,,,,,293.0,,llama-3.2-3b-instruct,,,,0.328,,,meta,,,,,,,,0.328,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:11.665423+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.328,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"8-shot, em_maj1@1",,,gsm8k,GSM8k,,,2025-07-19T19:56:13.073210+00:00,benchmark_result,,,,True,,,,,,989.0,,llama-3.2-3b-instruct,,,,0.777,,,meta,,,,,,,,0.777,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:13.073210+00:00,,,,,,False,,False,0.0,False,0.0,0.777,0.777,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.777,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, acc",,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.175473+00:00,benchmark_result,,,,True,,,,,,44.0,,llama-3.2-3b-instruct,,,,0.698,,,meta,,,,,,,,0.698,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:11.175473+00:00,,,,,,False,,False,0.0,False,0.0,0.698,0.698,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.698,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,Avg(Prompt/Instruction acc Loose/Strict),,,ifeval,IFEval,,,2025-07-19T19:56:12.272319+00:00,benchmark_result,,,,True,,,,,,617.0,,llama-3.2-3b-instruct,,,,0.774,,,meta,,,,,,,,0.774,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:12.272319+00:00,,,,,,False,,False,0.0,False,0.0,0.774,0.774,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.774,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"0-shot, longbook_choice/acc",,,infinitebench-en.mc,InfiniteBench/En.MC,,,2025-07-19T19:56:14.464298+00:00,benchmark_result,,,,True,,,,,,1589.0,,llama-3.2-3b-instruct,,,,0.633,,,meta,,,,,,,,0.633,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.464298+00:00,,,,,,False,,False,0.0,False,0.0,0.633,0.633,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",InfiniteBench/En.MC,['long_context'],text,False,1.0,en,InfiniteBench English Multiple Choice variant - first LLM benchmark featuring average data length surpassing 100K tokens for evaluating long-context capabilities with 12 tasks spanning diverse domains,0.633,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"0-shot, longbook_qa/f1",,,infinitebench-en.qa,InfiniteBench/En.QA,,,2025-07-19T19:56:14.460560+00:00,benchmark_result,,,,True,,,,,,1588.0,,llama-3.2-3b-instruct,,,,0.198,,,meta,,,,,,,,0.198,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.460560+00:00,,,,,,False,,False,0.0,False,0.0,0.198,0.198,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",InfiniteBench/En.QA,['long_context'],text,False,1.0,en,InfiniteBench English Question Answering variant - first LLM benchmark featuring average data length surpassing 100K tokens for evaluating long-context capabilities with 12 tasks spanning diverse domains,0.198,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"0-shot, final_em",,,math,MATH,,,2025-07-19T19:56:11.849582+00:00,benchmark_result,,,,True,,,,,,396.0,,llama-3.2-3b-instruct,,,,0.48,,,meta,,,,,,,,0.48,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:11.849582+00:00,,,,,,False,,False,0.0,False,0.0,0.48,0.48,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.48,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"CoT, em",,,mgsm,MGSM,,,2025-07-19T19:56:13.692573+00:00,benchmark_result,,,,True,,,,,,1285.0,,llama-3.2-3b-instruct,,,,0.582,,,meta,,,,,,,,0.582,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:13.692573+00:00,,,,,,False,,False,0.0,False,0.0,0.582,0.582,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.582,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"5-shot, macro_avg/acc",,,mmlu,MMLU,,,2025-07-19T19:56:11.252797+00:00,benchmark_result,,,,True,,,,,,81.0,,llama-3.2-3b-instruct,,,,0.634,,,meta,,,,,,,,0.634,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:11.252797+00:00,,,,,,False,,False,0.0,False,0.0,0.634,0.634,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.634,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"0-shot, macro_avg/acc",,,nexus,Nexus,,,2025-07-19T19:56:14.401027+00:00,benchmark_result,,,,True,,,,,,1569.0,,llama-3.2-3b-instruct,,,,0.343,,,meta,,,,,,,,0.343,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.401027+00:00,,,,,,False,,False,0.0,False,0.0,0.343,0.343,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",Nexus,['general'],text,False,1.0,en,NexusRaven benchmark for evaluating function calling capabilities of large language models in zero-shot scenarios across cybersecurity tools and API interactions,0.343,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"0-shot, recall",,,nih-multi-needle,NIH/Multi-needle,,,2025-07-19T19:56:14.469424+00:00,benchmark_result,,,,True,,,,,,1590.0,,llama-3.2-3b-instruct,,,,0.847,,,meta,,,,,,,,0.847,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.469424+00:00,,,,,,False,,False,0.0,False,0.0,0.847,0.847,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",NIH/Multi-needle,['long_context'],text,False,1.0,en,Multi-needle in a haystack benchmark for evaluating long-context comprehension capabilities of language models by testing retrieval of multiple target pieces of information from extended documents,0.847,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"0-shot, micro_avg/rougeL",,,open-rewrite,Open-rewrite,,,2025-07-19T19:56:14.438526+00:00,benchmark_result,,,,True,,,,,,1581.0,,llama-3.2-3b-instruct,,,,0.401,,,meta,,,,,,,,0.401,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.438526+00:00,,,,,,False,,False,0.0,False,0.0,0.401,0.401,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",Open-rewrite,"['language', 'writing']",text,False,1.0,en,"OpenRewriteEval is a benchmark for evaluating open-ended rewriting of long-form texts, covering a wide variety of rewriting types expressed through natural language instructions including formality, expansion, conciseness, paraphrasing, and tone and style transfer.",0.401,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"1-shot, rougeL",,,tldr9+-(test),TLDR9+ (test),,,2025-07-19T19:56:14.443142+00:00,benchmark_result,,,,True,,,,,,1582.0,,llama-3.2-3b-instruct,,,,0.19,,,meta,,,,,,,,0.19,https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct,,,,,,,,2025-07-19T19:56:14.443142+00:00,,,,,,False,,False,0.0,False,0.0,0.19,0.19,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 3B Instruct,meta,Meta,3210000000.0,3210000000.0,True,9000000000000.0,True,False,llama_3_2_community_license,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 3B Instruct is a large language model that supports a context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge.",TLDR9+ (test),"['summarization', 'language']",text,False,1.0,en,"A large-scale summarization dataset containing over 9 million training instances extracted from Reddit, designed for extreme summarization (generating one-sentence summaries with high compression and abstraction). More than twice larger than previously proposed datasets.",0.19,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,-,,,ai2d,AI2D,,,2025-07-19T19:56:13.629735+00:00,benchmark_result,,,,True,,,,,,1252.0,,llama-3.2-90b-instruct,,,,0.923,,,meta,,,,,,,,0.923,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:13.629735+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.923,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,-,,,chartqa,ChartQA,,,2025-07-19T19:56:12.799861+00:00,benchmark_result,,,,True,,,,,,860.0,,llama-3.2-90b-instruct,,,,0.855,,,meta,,,,,,,,0.855,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:12.799861+00:00,,,,,,False,,False,0.0,False,0.0,0.855,0.855,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.855,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),llama
,-,,,docvqa,DocVQA,,,2025-07-19T19:56:12.837654+00:00,benchmark_result,,,,True,,,,,,882.0,,llama-3.2-90b-instruct,,,,0.901,,,meta,,,,,,,,0.901,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:12.837654+00:00,,,,,,False,,False,0.0,False,0.0,0.901,0.901,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.901,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.659193+00:00,benchmark_result,,,,True,,,,,,289.0,,llama-3.2-90b-instruct,,,,0.467,,,meta,,,,,,,,0.467,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:11.659193+00:00,,,,,,False,,False,0.0,False,0.0,0.467,0.467,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.467,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,-,,,infographicsqa,InfographicsQA,,,2025-07-19T19:56:14.420214+00:00,benchmark_result,,,,True,,,,,,1576.0,,llama-3.2-90b-instruct,,,,0.568,,,meta,,,,,,,,0.568,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:14.420214+00:00,,,,,,False,,False,0.0,False,0.0,0.568,0.568,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",InfographicsQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfographicVQA dataset with 5,485 infographic images and over 30,000 questions requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.568,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.844378+00:00,benchmark_result,,,,True,,,,,,393.0,,llama-3.2-90b-instruct,,,,0.68,,,meta,,,,,,,,0.68,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:11.844378+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.68,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,-,,,mathvista,MathVista,,,2025-07-19T19:56:12.084321+00:00,benchmark_result,,,,True,,,,,,522.0,,llama-3.2-90b-instruct,,,,0.573,,,meta,,,,,,,,0.573,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:12.084321+00:00,,,,,,False,,False,0.0,False,0.0,0.573,0.573,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.573,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,mgsm,MGSM,,,2025-07-19T19:56:13.688987+00:00,benchmark_result,,,,True,,,,,,1283.0,,llama-3.2-90b-instruct,,,,0.869,,,meta,,,,,,,,0.869,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:13.688987+00:00,,,,,,False,,False,0.0,False,0.0,0.869,0.869,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.869,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot CoT,,,mmlu,MMLU,,,2025-07-19T19:56:11.245688+00:00,benchmark_result,,,,True,,,,,,77.0,,llama-3.2-90b-instruct,,,,0.86,,,meta,,,,,,,,0.86,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:11.245688+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.86,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot CoT,,,mmmu,MMMU,,,2025-07-19T19:56:12.162828+00:00,benchmark_result,,,,True,,,,,,565.0,,llama-3.2-90b-instruct,,,,0.603,,,meta,,,,,,,,0.603,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:12.162828+00:00,,,,,,False,,False,0.0,False,0.0,0.603,0.603,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.603,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot CoT,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.287214+00:00,benchmark_result,,,,True,,,,,,1529.0,,llama-3.2-90b-instruct,,,,0.452,,,meta,,,,,,,,0.452,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:14.287214+00:00,,,,,,False,,False,0.0,False,0.0,0.452,0.452,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.452,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,-,,,textvqa,TextVQA,,,2025-07-19T19:56:12.892927+00:00,benchmark_result,,,,True,,,,,,908.0,,llama-3.2-90b-instruct,,,,0.735,,,meta,,,,,,,,0.735,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:12.892927+00:00,,,,,,False,,False,0.0,False,0.0,0.735,0.735,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.735,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),llama
,-,,,vqav2,VQAv2,,,2025-07-19T19:56:14.412800+00:00,benchmark_result,,,,True,,,,,,1573.0,,llama-3.2-90b-instruct,,,,0.781,,,meta,,,,,,,,0.781,https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/,,,,,,,,2025-07-19T19:56:14.412800+00:00,,,,,,False,,False,0.0,False,0.0,0.781,0.781,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.2 90B Instruct,meta,Meta,90000000000.0,90000000000.0,True,0.0,False,True,llama3_2,Restricted/Community,2024-09-25,2024.0,9.0,2024-09,Very Large (>70B),"Llama 3.2 90B is a large multimodal language model optimized for visual recognition, image reasoning, and captioning tasks. It supports a context length of 128,000 tokens and is designed for deployment on edge and mobile devices, offering state-of-the-art performance in image understanding and generative tasks.",VQAv2,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset that addresses language bias by providing complementary images for each question, forcing models to rely on visual understanding rather than language priors. It contains approximately twice the number of image-question pairs compared to the original VQA dataset.",0.781,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),llama
,-,,,bfcl-v2,BFCL v2,,,2025-07-19T19:56:14.448863+00:00,benchmark_result,,,,True,,,,,,1584.0,,llama-3.3-70b-instruct,,,,0.773,,,meta,,,,,,,,0.773,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:14.448863+00:00,,,,,,False,,False,0.0,False,0.0,0.773,0.773,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",BFCL v2,"['general', 'reasoning']",text,True,1.0,en,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",0.773,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.669923+00:00,benchmark_result,,,,True,,,,,,296.0,,llama-3.3-70b-instruct,,,,0.505,,,meta,,,,,,,,0.505,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:11.669923+00:00,,,,,,False,,False,0.0,False,0.0,0.505,0.505,Unknown,,,,,Undisclosed,Poor (<60%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.505,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,-,,,humaneval,HumanEval,,,2025-07-19T19:56:12.637990+00:00,benchmark_result,,,,True,,,,,,781.0,,llama-3.3-70b-instruct,,,,0.884,,,meta,,,,,,,,0.884,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:12.637990+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.884,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,-,,,ifeval,IFEval,,,2025-07-19T19:56:12.274109+00:00,benchmark_result,,,,True,,,,,,618.0,,llama-3.3-70b-instruct,,,,0.921,,,meta,,,,,,,,0.921,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:12.274109+00:00,,,,,,False,,False,0.0,False,0.0,0.921,0.921,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.921,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.854268+00:00,benchmark_result,,,,True,,,,,,399.0,,llama-3.3-70b-instruct,,,,0.77,,,meta,,,,,,,,0.77,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:11.854268+00:00,,,,,,False,,False,0.0,False,0.0,0.77,0.77,Unknown,,,,,Undisclosed,Good (70-79%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.77,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,-,,,mbpp-evalplus,MBPP EvalPlus,,,2025-07-19T19:56:14.429699+00:00,benchmark_result,,,,True,,,,,,1579.0,,llama-3.3-70b-instruct,,,,0.876,,,meta,,,,,,,,0.876,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:14.429699+00:00,,,,,,False,,False,0.0,False,0.0,0.876,0.876,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",MBPP EvalPlus,"['reasoning', 'general']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. EvalPlus extends MBPP with significantly more test cases (35x) for more rigorous evaluation of LLM-synthesized code, providing high-quality and precise evaluation.",0.876,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,-,,,mgsm,MGSM,,,2025-07-19T19:56:13.697414+00:00,benchmark_result,,,,True,,,,,,1288.0,,llama-3.3-70b-instruct,,,,0.911,,,meta,,,,,,,,0.911,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:13.697414+00:00,,,,,,False,,False,0.0,False,0.0,0.911,0.911,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.911,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,0-shot CoT,,,mmlu,MMLU,,,2025-07-19T19:56:11.259963+00:00,benchmark_result,,,,True,,,,,,84.0,,llama-3.3-70b-instruct,,,,0.86,,,meta,,,,,,,,0.86,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:11.259963+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.86,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.463251+00:00,benchmark_result,,,,True,,,,,,189.0,,llama-3.3-70b-instruct,,,,0.689,,,meta,,,,,,,,0.689,https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/MODEL_CARD.md,,,,,,,,2025-07-19T19:56:11.463251+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 3.3 70B Instruct,meta,Meta,70000000000.0,70000000000.0,True,15000000000000.0,True,False,llama_3_3_community_license_agreement,Restricted/Community,2024-12-06,2024.0,12.0,2024-12,Very Large (>70B),"Llama 3.3 is a multilingual large language model optimized for dialogue use cases across multiple languages. It is a pretrained and instruction-tuned generative model with 70 billion parameters, outperforming many open-source and closed chat models on common industry benchmarks. Llama 3.3 supports a context length of 128,000 tokens and is designed for commercial and research use in multiple languages.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.689,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"Pass@1, Reasoning On",,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.459628+00:00,benchmark_result,,,,True,,,,,,697.0,,llama-3.3-nemotron-super-49b-v1,,,,0.584,,,nvidia,,,,,,,,0.584,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.459628+00:00,,,,,,False,,False,0.0,False,0.0,0.584,0.584,Unknown,,,,,Undisclosed,Poor (<60%),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.584,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Score, Reasoning Off",,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.113375+00:00,benchmark_result,,,,True,,,,,,1461.0,,llama-3.3-nemotron-super-49b-v1,,,,0.883,,,nvidia,,,,,,,,0.883,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.113375+00:00,,,,,,False,,False,0.0,False,0.0,0.883,0.883,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.883,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,"Score, Reasoning On",,,bfcl-v2,BFCL v2,,,2025-07-19T19:56:14.452681+00:00,benchmark_result,,,,True,,,,,,1585.0,,llama-3.3-nemotron-super-49b-v1,,,,0.737,,,nvidia,,,,,,,,0.737,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.452681+00:00,,,,,,False,,False,0.0,False,0.0,0.737,0.737,Unknown,,,,,Undisclosed,Good (70-79%),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",BFCL v2,"['general', 'reasoning']",text,True,1.0,en,"Berkeley Function Calling Leaderboard (BFCL) v2 is a comprehensive benchmark for evaluating large language models' function calling capabilities. It features 2,251 question-function-answer pairs with enterprise and OSS-contributed functions, addressing data contamination and bias through live, user-contributed scenarios. The benchmark evaluates AST accuracy, executable accuracy, irrelevance detection, and relevance detection across multiple programming languages (Python, Java, JavaScript) and includes complex real-world function calling scenarios with multi-lingual prompts.",0.737,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,"Pass@1, Reasoning On",,,gpqa,GPQA,,,2025-07-19T19:56:11.717785+00:00,benchmark_result,,,,True,,,,,,326.0,,llama-3.3-nemotron-super-49b-v1,,,,0.6667,,,nvidia,,,,,,,,0.6667,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:11.717785+00:00,,,,,,False,,False,0.0,False,0.0,0.6667,0.6667,Unknown,,,,,Undisclosed,Fair (60-69%),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.6667,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,"Pass@1, Reasoning On",,,math-500,MATH-500,,,2025-07-19T19:56:12.058280+00:00,benchmark_result,,,,True,,,,,,509.0,,llama-3.3-nemotron-super-49b-v1,,,,0.966,,,nvidia,,,,,,,,0.966,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:12.058280+00:00,,,,,,False,,False,0.0,False,0.0,0.966,0.966,Unknown,,,,,Undisclosed,Excellent (90%+),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.966,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),llama
,"Pass@1, Reasoning On",,,mbpp,MBPP,,,2025-07-19T19:56:13.511549+00:00,benchmark_result,,,,True,,,,,,1192.0,,llama-3.3-nemotron-super-49b-v1,,,,0.913,,,nvidia,,,,,,,,0.913,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:13.511549+00:00,,,,,,False,,False,0.0,False,0.0,0.913,0.913,Unknown,,,,,Undisclosed,Excellent (90%+),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.009130000000000001,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,"Score, Reasoning On",,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.527840+00:00,benchmark_result,,,,True,,,,,,1609.0,,llama-3.3-nemotron-super-49b-v1,,,,0.917,,,nvidia,,,,,,,,0.917,https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/modelcard,,,,,,,,2025-07-19T19:56:14.527840+00:00,,,,,,False,,False,0.0,False,0.0,0.917,0.917,Unknown,,,,,Undisclosed,Excellent (90%+),Llama-3.3 Nemotron Super 49B v1,nvidia,NVIDIA,49900000000.0,49900000000.0,True,0.0,False,False,llama_3_1_community_license,Restricted/Community,2025-03-18,2025.0,3.0,2025-03,Very Large (>70B),"Llama-3.3-Nemotron-Super-49B-v1 is a large language model (LLM) derived from Meta Llama-3.3-70B-Instruct. It's post-trained for reasoning, chat, RAG, and tool calling, offering a balance between accuracy and efficiency (optimized for single H100). It underwent multi-phase post-training including SFT and RL (RLOO, RPO).",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.009170000000000001,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,chartqa,ChartQA,,,2025-07-19T19:56:12.803334+00:00,benchmark_result,,,,True,,,,,,862.0,,llama-4-maverick,,,,0.9,,,meta,,,,,,,,0.9,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.803334+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.9,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,0-shot CoT,,,docvqa,DocVQA,,,2025-07-19T19:56:12.841331+00:00,benchmark_result,,,,True,,,,,,884.0,,llama-4-maverick,,,,0.944,,,meta,,,,,,,,0.944,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.841331+00:00,,,,,,False,,False,0.0,False,0.0,0.944,0.944,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.944,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,0-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.666983+00:00,benchmark_result,,,,True,,,,,,294.0,,llama-4-maverick,,,,0.698,,,meta,,,,,,,,0.698,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:11.666983+00:00,,,,,,False,,False,0.0,False,0.0,0.698,0.698,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.698,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot CoT,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.326624+00:00,benchmark_result,,,,True,,,,,,1115.0,,llama-4-maverick,,,,0.434,,,meta,,,,,,,,0.434,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:13.326624+00:00,,,,,,False,,False,0.0,False,0.0,0.434,0.434,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.434,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,4-shot em_maj1@1,,,math,MATH,,,2025-07-19T19:56:11.851038+00:00,benchmark_result,,,,True,,,,,,397.0,,llama-4-maverick,,,,0.612,,,meta,,,,,,,,0.612,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:11.851038+00:00,,,,,,False,,False,0.0,False,0.0,0.612,0.612,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.612,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,0-shot CoT,,,mathvista,MathVista,,,2025-07-19T19:56:12.088308+00:00,benchmark_result,,,,True,,,,,,524.0,,llama-4-maverick,,,,0.737,,,meta,,,,,,,,0.737,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.088308+00:00,,,,,,False,,False,0.0,False,0.0,0.737,0.737,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.737,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),llama
,3-shot pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.485323+00:00,benchmark_result,,,,True,,,,,,1179.0,,llama-4-maverick,,,,0.776,,,meta,,,,,,,,0.776,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:13.485323+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00776,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,mgsm,MGSM,,,2025-07-19T19:56:13.694238+00:00,benchmark_result,,,,True,,,,,,1286.0,,llama-4-maverick,,,,0.923,,,meta,,,,,,,,0.923,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:13.694238+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.923,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,5-shot macro_avg/acc_char,,,mmlu,MMLU,,,2025-07-19T19:56:11.254352+00:00,benchmark_result,,,,True,,,,,,82.0,,llama-4-maverick,,,,0.855,,,meta,,,,,,,,0.855,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:11.254352+00:00,,,,,,False,,False,0.0,False,0.0,0.855,0.855,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.855,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.460210+00:00,benchmark_result,,,,True,,,,,,187.0,,llama-4-maverick,,,,0.805,,,meta,,,,,,,,0.805,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:11.460210+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.805,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),llama
,0-shot CoT,,,mmmu,MMMU,,,2025-07-19T19:56:12.167124+00:00,benchmark_result,,,,True,,,,,,567.0,,llama-4-maverick,,,,0.734,,,meta,,,,,,,,0.734,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.167124+00:00,,,,,,False,,False,0.0,False,0.0,0.734,0.734,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.734,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot CoT,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.290598+00:00,benchmark_result,,,,True,,,,,,1531.0,,llama-4-maverick,,,,0.596,,,meta,,,,,,,,0.596,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:14.290598+00:00,,,,,,False,,False,0.0,False,0.0,0.596,0.596,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.596,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),llama
,1-shot average/f1,,,tydiqa,TydiQA,,,2025-07-19T19:56:14.475429+00:00,benchmark_result,,,,True,,,,,,1591.0,,llama-4-maverick,,,,0.317,,,meta,,,,,,,,0.317,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:14.475429+00:00,,,,,,False,,False,0.0,False,0.0,0.317,0.317,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Maverick,meta,Meta,400000000000.0,400000000000.0,True,22000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Maverick is a natively multimodal model capable of processing both text and images. It features a 17 billion active parameter mixture-of-experts (MoE) architecture with 128 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 1 million token context window.",TydiQA,"['language', 'reasoning']",text,True,1.0,en,A multilingual question answering benchmark covering 11 typologically diverse languages with 204K question-answer pairs. Questions are written by people seeking genuine information and data is collected directly in each language without translation to test model generalization across diverse linguistic structures.,0.317,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,chartqa,ChartQA,,,2025-07-19T19:56:12.804916+00:00,benchmark_result,,,,True,,,,,,863.0,,llama-4-scout,,,,0.888,,,meta,,,,,,,,0.888,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.804916+00:00,,,,,,False,,False,0.0,False,0.0,0.888,0.888,Unknown,,,,,Undisclosed,Very Good (80-89%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.888,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),llama
,0-shot (ANLS),,,docvqa,DocVQA,,,2025-07-19T19:56:12.842838+00:00,benchmark_result,,,,True,,,,,,885.0,,llama-4-scout,,,,0.944,,,meta,,,,,,,,0.944,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.842838+00:00,,,,,,False,,False,0.0,False,0.0,0.944,0.944,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.944,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),llama
,0-shot (accuracy),,,gpqa,GPQA,,,2025-07-19T19:56:11.668436+00:00,benchmark_result,,,,True,,,,,,295.0,,llama-4-scout,,,,0.572,,,meta,,,,,,,,0.572,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:11.668436+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.572,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.328074+00:00,benchmark_result,,,,True,,,,,,1116.0,,llama-4-scout,,,,0.328,,,meta,,,,,,,,0.328,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:13.328074+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.328,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,4-shot em_maj1@1,,,math,MATH,,,2025-07-19T19:56:11.852669+00:00,benchmark_result,,,,True,,,,,,398.0,,llama-4-scout,,,,0.503,,,meta,,,,,,,,0.503,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:11.852669+00:00,,,,,,False,,False,0.0,False,0.0,0.503,0.503,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.503,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot CoT,,,mathvista,MathVista,,,2025-07-19T19:56:12.089981+00:00,benchmark_result,,,,True,,,,,,525.0,,llama-4-scout,,,,0.707,,,meta,,,,,,,,0.707,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.089981+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.707,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),llama
,3-shot pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.487376+00:00,benchmark_result,,,,True,,,,,,1180.0,,llama-4-scout,,,,0.678,,,meta,,,,,,,,0.678,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:13.487376+00:00,,,,,,False,,False,0.0,False,0.0,0.678,0.678,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0067800000000000004,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,0-shot (average/em),,,mgsm,MGSM,,,2025-07-19T19:56:13.695659+00:00,benchmark_result,,,,True,,,,,,1287.0,,llama-4-scout,,,,0.906,,,meta,,,,,,,,0.906,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:13.695659+00:00,,,,,,False,,False,0.0,False,0.0,0.906,0.906,Unknown,,,,,Undisclosed,Excellent (90%+),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.906,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),llama
,5-shot macro_avg/acc_char,,,mmlu,MMLU,,,2025-07-19T19:56:11.258246+00:00,benchmark_result,,,,True,,,,,,83.0,,llama-4-scout,,,,0.796,,,meta,,,,,,,,0.796,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:11.258246+00:00,,,,,,False,,False,0.0,False,0.0,0.796,0.796,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.796,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot (macro_avg/acc),,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.461726+00:00,benchmark_result,,,,True,,,,,,188.0,,llama-4-scout,,,,0.743,,,meta,,,,,,,,0.743,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:11.461726+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.743,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),llama
,0-shot CoT,,,mmmu,MMMU,,,2025-07-19T19:56:12.169227+00:00,benchmark_result,,,,True,,,,,,568.0,,llama-4-scout,,,,0.694,,,meta,,,,,,,,0.694,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,,,,,,,2025-07-19T19:56:12.169227+00:00,,,,,,False,,False,0.0,False,0.0,0.694,0.694,Unknown,,,,,Undisclosed,Fair (60-69%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.694,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),llama
,1-shot average/f1,,,tydiqa,TydiQA,,,2025-07-19T19:56:14.477364+00:00,benchmark_result,,,,True,,,,,,1592.0,,llama-4-scout,,,,0.315,,,meta,,,,,,,,0.315,https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct,,,,,,,,2025-07-19T19:56:14.477364+00:00,,,,,,False,,False,0.0,False,0.0,0.315,0.315,Unknown,,,,,Undisclosed,Poor (<60%),Llama 4 Scout,meta,Meta,109000000000.0,109000000000.0,True,40000000000000.0,True,True,llama_4_community_license_agreement,Restricted/Community,2025-04-05,2025.0,4.0,2025-04,Very Large (>70B),"Llama 4 Scout is a natively multimodal model capable of processing both text and images. It features a 17 billion activated parameter (109B total) mixture-of-experts (MoE) architecture with 16 experts, supporting a wide range of multimodal tasks such as conversational interaction, image analysis, and code generation. The model includes a 10 million token context window.",TydiQA,"['language', 'reasoning']",text,True,1.0,en,A multilingual question answering benchmark covering 11 typologically diverse languages with 204K question-answer pairs. Questions are written by people seeking genuine information and data is collected directly in each language without translation to test model generalization across diverse linguistic structures.,0.315,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),llama
,accuracy,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.379075+00:00,benchmark_result,,,,True,,,,,,665.0,,magistral-medium,,,,0.471,,,mistral,,,,,,,,0.471,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:12.379075+00:00,,,,,,False,,False,0.0,False,0.0,0.471,0.471,Unknown,,,,,Undisclosed,Poor (<60%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.471,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),magistral
,pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.011044+00:00,benchmark_result,,,,True,,,,,,480.0,,magistral-medium,,,,0.736,,,mistral,,,,,,,,0.736,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:12.011044+00:00,,,,,,False,,False,0.0,False,0.0,0.736,0.736,Unknown,,,,,Undisclosed,Good (70-79%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.736,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),magistral
,pass@1,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.473748+00:00,benchmark_result,,,,True,,,,,,704.0,,magistral-medium,,,,0.649,,,mistral,,,,,,,,0.649,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:12.473748+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.649,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),magistral
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.745089+00:00,benchmark_result,,,,True,,,,,,343.0,,magistral-medium,,,,0.708,,,mistral,,,,,,,,0.708,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:11.745089+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.708,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),magistral
,text subset,,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.525031+00:00,benchmark_result,,,,True,,,,,,724.0,,magistral-medium,,,,0.09,,,mistral,,,,,,,,0.09,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:12.525031+00:00,,,,,,False,,False,0.0,False,0.0,0.09,0.09,Unknown,,,,,Undisclosed,Poor (<60%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.09,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),magistral
,v6,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.408465+00:00,benchmark_result,,,,True,,,,,,1145.0,,magistral-medium,,,,0.503,,,mistral,,,,,,,,0.503,https://arxiv.org/pdf/2506.10910,,,,,,,,2025-07-19T19:56:13.410002+00:00,,,,,,False,,False,0.0,False,0.0,0.503,0.503,Unknown,,,,,Undisclosed,Poor (<60%),Magistral Medium,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Trained solely with reinforcement learning on top of Mistral Medium 3, Magistral Medium is a reasoning model that achieves strong performance on complex math and code tasks without relying on distillation from existing reasoning models. The training uses an RLVR framework with modifications to GRPO, enabling improved reasoning ability and multilingual consistency.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.503,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),magistral
,Score,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.009597+00:00,benchmark_result,,,,True,,,,,,479.0,,magistral-small-2506,,,,0.7068,,,mistral,,,,,,,,0.7068,https://huggingface.co/mistralai/Magistral-Small-2506,,,,,,,,2025-07-19T19:56:12.009597+00:00,,,,,,False,,False,0.0,False,0.0,0.7068,0.7068,Unknown,,,,,Undisclosed,Good (70-79%),Magistral Small 2506,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.7068,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),magistral
,Score,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.471565+00:00,benchmark_result,,,,True,,,,,,703.0,,magistral-small-2506,,,,0.6276,,,mistral,,,,,,,,0.6276,https://huggingface.co/mistralai/Magistral-Small-2506,,,,,,,,2025-07-19T19:56:12.471565+00:00,,,,,,False,,False,0.0,False,0.0,0.6276,0.6276,Unknown,,,,,Undisclosed,Fair (60-69%),Magistral Small 2506,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.6276,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),magistral
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.743610+00:00,benchmark_result,,,,True,,,,,,342.0,,magistral-small-2506,,,,0.6818,,,mistral,,,,,,,,0.6818,https://huggingface.co/mistralai/Magistral-Small-2506,,,,,,,,2025-07-19T19:56:11.743610+00:00,,,,,,False,,False,0.0,False,0.0,0.6818,0.6818,Unknown,,,,,Undisclosed,Fair (60-69%),Magistral Small 2506,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.6818,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),magistral
,v5,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.406640+00:00,benchmark_result,,,,True,,,,,,1144.0,,magistral-small-2506,,,,0.513,,,mistral,,,,,,,,0.513,https://mistral.ai/news/codestral/,,,,,,,,2025-07-19T19:56:13.406640+00:00,,,,,,False,,False,0.0,False,0.0,0.513,0.513,Unknown,,,,,Undisclosed,Poor (<60%),Magistral Small 2506,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-06-10,2025.0,6.0,2025-06,Very Large (>70B),"Building upon Mistral Small 3.1 (2503), with added reasoning capabilities, undergoing SFT from Magistral Medium traces and RL on top, it's a small, efficient reasoning model with 24B parameters. Magistral Small can be deployed locally, fitting within a single RTX 4090 or a 32GB RAM MacBook once quantized.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.513,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),magistral
,Average F1 for top 5 conditions,,,chexpert-cxr,CheXpert CXR,,,2025-07-19T19:56:14.023334+00:00,benchmark_result,,,,True,,,,,,1425.0,,medgemma-4b-it,,,,0.481,,,google,,,,,,,,0.481,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.023334+00:00,,,,,,False,,False,0.0,False,0.0,0.481,0.481,Unknown,,,,,Undisclosed,Poor (<60%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",CheXpert CXR,"['healthcare', 'vision']",image,False,1.0,en,"CheXpert is a large dataset of 224,316 chest radiographs from 65,240 patients for automated chest X-ray interpretation. The dataset includes uncertainty labels for 14 medical observations extracted from radiology reports. It serves as a benchmark for developing and evaluating automated chest radiograph interpretation models.",0.481,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),medgemma
,Accuracy,,,dermmcqa,DermMCQA,,,2025-07-19T19:56:14.026812+00:00,benchmark_result,,,,True,,,,,,1426.0,,medgemma-4b-it,,,,0.718,,,google,,,,,,,,0.718,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.026812+00:00,,,,,,False,,False,0.0,False,0.0,0.718,0.718,Unknown,,,,,Undisclosed,Good (70-79%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",DermMCQA,['healthcare'],text,False,1.0,en,Dermatology multiple choice question assessment benchmark for evaluating medical knowledge and diagnostic reasoning in dermatological conditions and treatments.,0.718,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),medgemma
,Accuracy,,,medxpertqa,MedXpertQA,,,2025-07-19T19:56:14.042823+00:00,benchmark_result,,,,True,,,,,,1430.0,,medgemma-4b-it,,,,0.188,,,google,,,,,,,,0.188,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.042823+00:00,,,,,,False,,False,0.0,False,0.0,0.188,0.188,Unknown,,,,,Undisclosed,Poor (<60%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",MedXpertQA,"['healthcare', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"A comprehensive benchmark to evaluate expert-level medical knowledge and advanced reasoning, featuring 4,460 questions spanning 17 specialties and 11 body systems. Includes both text-only and multimodal subsets with expert-level exam questions incorporating diverse medical images and rich clinical information.",0.188,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),medgemma
,Average F1 for top 5 conditions,,,mimic-cxr,MIMIC CXR,,,2025-07-19T19:56:14.019964+00:00,benchmark_result,,,,True,,,,,,1424.0,,medgemma-4b-it,,,,0.889,,,google,,,,,,,,0.889,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.019964+00:00,,,,,,False,,False,0.0,False,0.0,0.889,0.889,Unknown,,,,,Undisclosed,Very Good (80-89%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",MIMIC CXR,"['healthcare', 'vision', 'multimodal']",multimodal,False,1.0,en,"MIMIC-CXR is a large publicly available dataset of chest radiographs with free-text radiology reports. Contains 377,110 images corresponding to 227,835 radiographic studies from 65,379 patients at Beth Israel Deaconess Medical Center. The dataset is de-identified and widely used for medical imaging research, automated report generation, and medical AI development.",0.889,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),medgemma
,Accuracy,,,pathmcqa,PathMCQA,,,2025-07-19T19:56:14.039089+00:00,benchmark_result,,,,True,,,,,,1429.0,,medgemma-4b-it,,,,0.698,,,google,,,,,,,,0.698,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.039089+00:00,,,,,,False,,False,0.0,False,0.0,0.698,0.698,Unknown,,,,,Undisclosed,Fair (60-69%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",PathMCQA,"['healthcare', 'vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"PathMMU is a massive multimodal expert-level benchmark for understanding and reasoning in pathology, containing 33,428 multimodal multi-choice questions and 24,067 images validated by seven pathologists. It evaluates Large Multimodal Models (LMMs) performance on pathology tasks, with the top-performing model GPT-4V achieving only 49.8% zero-shot performance compared to 71.8% for human pathologists.",0.698,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),medgemma
,Tokenized F1,,,slakevqa,SlakeVQA,,,2025-07-19T19:56:14.029835+00:00,benchmark_result,,,,True,,,,,,1427.0,,medgemma-4b-it,,,,0.623,,,google,,,,,,,,0.623,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.029835+00:00,,,,,,False,,False,0.0,False,0.0,0.623,0.623,Unknown,,,,,Undisclosed,Fair (60-69%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",SlakeVQA,"['vision', 'healthcare', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A semantically-labeled knowledge-enhanced dataset for medical visual question answering. Contains 642 radiology images (CT scans, MRI scans, X-rays) covering five body parts and 14,028 bilingual English-Chinese question-answer pairs annotated by experienced physicians. Features comprehensive semantic labels and a structural medical knowledge base with both vision-only and knowledge-based questions requiring external medical knowledge reasoning.",0.623,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),medgemma
,Tokenized F1,,,vqa-rad,VQA-Rad,,,2025-07-19T19:56:14.035504+00:00,benchmark_result,,,,True,,,,,,1428.0,,medgemma-4b-it,,,,0.499,,,google,,,,,,,,0.499,https://huggingface.co/google/medgemma-4b-it,,,,,,,,2025-07-19T19:56:14.035504+00:00,,,,,,False,,False,0.0,False,0.0,0.499,0.499,Unknown,,,,,Undisclosed,Poor (<60%),MedGemma 4B IT,google,Google,4300000000.0,4300000000.0,True,0.0,False,True,health_ai_developer_foundations_terms_of_use,Restricted/Community,2025-05-20,2025.0,5.0,2025-05,Very Large (>70B),"MedGemma is a collection of Gemma 3 variants that are trained for performance on medical text and image comprehension. MedGemma 4B utilizes a SigLIP image encoder that has been specifically pre-trained on a variety of de-identified medical data, including chest X-rays, dermatology images, ophthalmology images, and histopathology slides. Its LLM component is trained on a diverse set of medical data, including radiology images, histopathology patches, ophthalmology images, and dermatology images. MedGemma is a multimodal model primarily evaluated on single-image tasks. It has not been evaluated for multi-turn applications and may be more sensitive to specific prompts than its predecessor, Gemma 3. Developers should consider bias in validation data and data contamination concerns when using MedGemma.",VQA-Rad,"['vision', 'healthcare', 'multimodal']",multimodal,False,1.0,en,"VQA-RAD (Visual Question Answering in Radiology) is the first manually constructed dataset of medical visual question answering containing 3,515 clinically generated visual questions and answers about radiology images. The dataset includes questions created by clinical trainees on 315 radiology images from MedPix covering head, chest, and abdominal scans, designed to support AI development for medical image analysis and improve patient care.",0.499,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),medgemma
,-,,,agieval,AGIEval,,,2025-07-19T19:56:13.978647+00:00,benchmark_result,,,,True,,,,,,1410.0,,ministral-8b-instruct-2410,,,,0.483,,,mistral,,,,,,,,0.483,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:13.978647+00:00,,,,,,False,,False,0.0,False,0.0,0.483,0.483,Unknown,,,,,Undisclosed,Poor (<60%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",AGIEval,"['reasoning', 'general', 'math']",text,False,1.0,en,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",0.483,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),ministral
,-,,,arc-c,ARC-C,,,2025-07-19T19:56:11.142536+00:00,benchmark_result,,,,True,,,,,,30.0,,ministral-8b-instruct-2410,,,,0.719,,,mistral,,,,,,,,0.719,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:11.142536+00:00,,,,,,False,,False,0.0,False,0.0,0.719,0.719,Unknown,,,,,Undisclosed,Good (70-79%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.719,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),ministral
,,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.118772+00:00,benchmark_result,,,,True,,,,,,1464.0,,ministral-8b-instruct-2410,,,,0.709,,,mistral,,,,,,,,0.709,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:14.118772+00:00,,,,,,False,,False,0.0,False,0.0,0.709,0.709,Unknown,,,,,Undisclosed,Good (70-79%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.709,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),ministral
,-,,,french-mmlu,French MMLU,,,2025-07-19T19:56:15.137792+00:00,benchmark_result,,,,True,,,,,,1820.0,,ministral-8b-instruct-2410,,,,0.575,,,mistral,,,,,,,,0.575,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:15.137792+00:00,,,,,,False,,False,0.0,False,0.0,0.575,0.575,Unknown,,,,,Undisclosed,Poor (<60%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",French MMLU,"['general', 'language', 'reasoning']",text,True,1.0,en,"French version of MMLU-Pro, a multilingual benchmark for evaluating language models' cross-lingual reasoning capabilities across 14 diverse domains including mathematics, physics, chemistry, law, engineering, psychology, and health.",0.575,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),ministral
,-,,,humaneval,HumanEval,,,2025-07-19T19:56:12.681246+00:00,benchmark_result,,,,True,,,,,,806.0,,ministral-8b-instruct-2410,,,,0.348,,,mistral,,,,,,,,0.348,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:12.681246+00:00,,,,,,False,,False,0.0,False,0.0,0.348,0.348,Unknown,,,,,Undisclosed,Poor (<60%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.348,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),ministral
,-,,,math,MATH,,,2025-07-19T19:56:11.895272+00:00,benchmark_result,,,,True,,,,,,422.0,,ministral-8b-instruct-2410,,,,0.545,,,mistral,,,,,,,,0.545,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:11.895272+00:00,,,,,,False,,False,0.0,False,0.0,0.545,0.545,Unknown,,,,,Undisclosed,Poor (<60%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.545,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),ministral
,-,,,mbpp-pass@1,MBPP pass@1,,,2025-07-19T19:56:15.141858+00:00,benchmark_result,,,,True,,,,,,1821.0,,ministral-8b-instruct-2410,,,,0.7,,,mistral,,,,,,,,0.7,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:15.141858+00:00,,,,,,False,,False,0.0,False,0.0,0.7,0.7,Unknown,,,,,Undisclosed,Good (70-79%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",MBPP pass@1,"['reasoning', 'general']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases. This variant uses pass@1 evaluation metric measuring the percentage of problems solved correctly on the first attempt.",0.7,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),ministral
,-,,,mmlu,MMLU,,,2025-07-19T19:56:11.309619+00:00,benchmark_result,,,,True,,,,,,112.0,,ministral-8b-instruct-2410,,,,0.65,,,mistral,,,,,,,,0.65,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:11.309619+00:00,,,,,,False,,False,0.0,False,0.0,0.65,0.65,Unknown,,,,,Undisclosed,Fair (60-69%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.65,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),ministral
,Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.535003+00:00,benchmark_result,,,,True,,,,,,1612.0,,ministral-8b-instruct-2410,,,,0.83,,,mistral,,,,,,,,0.83,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:14.535003+00:00,,,,,,False,,False,0.0,False,0.0,0.83,0.83,Unknown,,,,,Undisclosed,Very Good (80-89%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.0083,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),ministral
,-,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.582765+00:00,benchmark_result,,,,True,,,,,,253.0,,ministral-8b-instruct-2410,,,,0.655,,,mistral,,,,,,,,0.655,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:11.582765+00:00,,,,,,False,,False,0.0,False,0.0,0.655,0.655,Unknown,,,,,Undisclosed,Fair (60-69%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.655,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),ministral
,-,,,winogrande,Winogrande,,,2025-07-19T19:56:11.394106+00:00,benchmark_result,,,,True,,,,,,155.0,,ministral-8b-instruct-2410,,,,0.753,,,mistral,,,,,,,,0.753,https://huggingface.co/mistralai/Ministral-8B-Instruct-2410,,,,,,,,2025-07-19T19:56:11.394106+00:00,,,,,,False,,False,0.0,False,0.0,0.753,0.753,Unknown,,,,,Undisclosed,Good (70-79%),Ministral 8B Instruct,mistral,Mistral AI,8019808256.0,8019808256.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-10-16,2024.0,10.0,2024-10,Very Large (>70B),"The Ministral-8B-Instruct-2410 is an instruct fine-tuned model for local intelligence, on-device computing, and at-the-edge use cases, significantly outperforming existing models of similar size.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.753,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),ministral
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.113392+00:00,benchmark_result,,,,True,,,,,,1014.0,,mistral-large-2-2407,,,,0.93,,,mistral,,,,,,,,0.93,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,,,,,,,,2025-07-19T19:56:13.113392+00:00,,,,,,False,,False,0.0,False,0.0,0.93,0.93,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Large 2,mistral,Mistral AI,123000000000.0,123000000000.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-07-24,2024.0,7.0,2024-07,Very Large (>70B),"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.93,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),mistral
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.687406+00:00,benchmark_result,,,,True,,,,,,810.0,,mistral-large-2-2407,,,,0.92,,,mistral,,,,,,,,0.92,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,,,,,,,,2025-07-19T19:56:12.687406+00:00,,,,,,False,,False,0.0,False,0.0,0.92,0.92,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Large 2,mistral,Mistral AI,123000000000.0,123000000000.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-07-24,2024.0,7.0,2024-07,Very Large (>70B),"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.92,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),mistral
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.316024+00:00,benchmark_result,,,,True,,,,,,116.0,,mistral-large-2-2407,,,,0.84,,,mistral,,,,,,,,0.84,https://mistral.ai/news/mistral-large-2407/,,,,,,,,2025-07-19T19:56:11.316024+00:00,,,,,,False,,False,0.0,False,0.0,0.84,0.84,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Large 2,mistral,Mistral AI,123000000000.0,123000000000.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-07-24,2024.0,7.0,2024-07,Very Large (>70B),"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.84,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,Accuracy,,,mmlu-french,MMLU French,,,2025-07-19T19:56:15.178056+00:00,benchmark_result,,,,True,,,,,,1828.0,,mistral-large-2-2407,,,,0.828,,,mistral,,,,,,,,0.828,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,,,,,,,,2025-07-19T19:56:15.178056+00:00,,,,,,False,,False,0.0,False,0.0,0.828,0.828,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Large 2,mistral,Mistral AI,123000000000.0,123000000000.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-07-24,2024.0,7.0,2024-07,Very Large (>70B),"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",MMLU French,"['language', 'reasoning', 'math', 'general']",text,True,1.0,fr,"French language variant of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. This multilingual version tests model performance in French.",0.828,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.541051+00:00,benchmark_result,,,,True,,,,,,1615.0,,mistral-large-2-2407,,,,0.863,,,mistral,,,,,,,,0.863,https://huggingface.co/mistralai/Mistral-Large-Instruct-2407,,,,,,,,2025-07-19T19:56:14.541051+00:00,,,,,,False,,False,0.0,False,0.0,0.863,0.863,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Large 2,mistral,Mistral AI,123000000000.0,123000000000.0,True,0.0,False,False,mistral_research_license,Restricted/Community,2024-07-24,2024.0,7.0,2024-07,Very Large (>70B),"A 123B parameter model with strong capabilities in code generation, mathematics, and reasoning. Features enhanced multilingual support across dozens of languages, 128k context window, and advanced function calling capabilities. Excels in instruction-following and maintains concise outputs.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00863,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,0-shot evaluation,,,commonsenseqa,CommonSenseQA,,,2025-07-19T19:56:15.133096+00:00,benchmark_result,,,,True,,,,,,1819.0,,mistral-nemo-instruct-2407,,,,0.704,,,mistral,,,,,,,,0.704,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:15.133096+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",CommonSenseQA,"['reasoning', 'language']",text,False,1.0,en,"CommonSenseQA is a multiple-choice question answering dataset that requires different types of commonsense knowledge to predict correct answers. It contains 12,102 questions with one correct answer and four distractors, designed to test semantic reasoning and conceptual relationships. Questions are created based on ConceptNet concepts and require prior world knowledge for accurate reasoning.",0.704,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),mistral
,0-shot evaluation,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.196732+00:00,benchmark_result,,,,True,,,,,,54.0,,mistral-nemo-instruct-2407,,,,0.835,,,mistral,,,,,,,,0.835,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:11.196732+00:00,,,,,,False,,False,0.0,False,0.0,0.835,0.835,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.835,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,5-shot evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.308247+00:00,benchmark_result,,,,True,,,,,,111.0,,mistral-nemo-instruct-2407,,,,0.68,,,mistral,,,,,,,,0.68,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:11.308247+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.68,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,5-shot evaluation,,,natural-questions,Natural Questions,,,2025-07-19T19:56:13.191770+00:00,benchmark_result,,,,True,,,,,,1050.0,,mistral-nemo-instruct-2407,,,,0.312,,,mistral,,,,,,,,0.312,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:13.191770+00:00,,,,,,False,,False,0.0,False,0.0,0.312,0.312,Unknown,,,,,Undisclosed,Poor (<60%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",Natural Questions,"['reasoning', 'general', 'search']",text,False,1.0,en,"Natural Questions is a question answering dataset featuring real anonymized queries issued to Google search engine. It contains 307,373 training examples where annotators provide long answers (passages) and short answers (entities) from Wikipedia pages, or mark them as unanswerable.",0.312,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,0-shot evaluation,,,openbookqa,OpenBookQA,,,2025-07-19T19:56:14.138075+00:00,benchmark_result,,,,True,,,,,,1472.0,,mistral-nemo-instruct-2407,,,,0.606,,,mistral,,,,,,,,0.606,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:14.138075+00:00,,,,,,False,,False,0.0,False,0.0,0.606,0.606,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",OpenBookQA,"['reasoning', 'general']",text,False,1.0,en,"OpenBookQA is a question-answering dataset modeled after open book exams for assessing human understanding. It contains 5,957 multiple-choice elementary-level science questions that probe understanding of 1,326 core science facts and their application to novel situations, requiring combination of open book facts with broad common knowledge through multi-hop reasoning.",0.606,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,5-shot evaluation,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.581108+00:00,benchmark_result,,,,True,,,,,,252.0,,mistral-nemo-instruct-2407,,,,0.738,,,mistral,,,,,,,,0.738,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:11.581108+00:00,,,,,,False,,False,0.0,False,0.0,0.738,0.738,Unknown,,,,,Undisclosed,Good (70-79%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.738,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),mistral
,0-shot evaluation,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.369082+00:00,benchmark_result,,,,True,,,,,,146.0,,mistral-nemo-instruct-2407,,,,0.503,,,mistral,,,,,,,,0.503,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:11.369082+00:00,,,,,,False,,False,0.0,False,0.0,0.503,0.503,Unknown,,,,,Undisclosed,Poor (<60%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.503,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,0-shot evaluation,,,winogrande,Winogrande,,,2025-07-19T19:56:11.392106+00:00,benchmark_result,,,,True,,,,,,154.0,,mistral-nemo-instruct-2407,,,,0.768,,,mistral,,,,,,,,0.768,https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407,,,,,,,,2025-07-19T19:56:11.392106+00:00,,,,,,False,,False,0.0,False,0.0,0.768,0.768,Unknown,,,,,Undisclosed,Good (70-79%),Mistral NeMo Instruct,mistral,Mistral AI,12000000000.0,12000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-18,2024.0,7.0,2024-07,Very Large (>70B),"A state-of-the-art 12B multilingual model with a 128k context window, designed for global applications and strong in multiple languages.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.768,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),mistral
,-,,,agieval,AGIEval,,,2025-07-19T19:56:13.980585+00:00,benchmark_result,,,,True,,,,,,1411.0,,mistral-small-24b-base-2501,,,,0.658,,,mistral,,,,,,,,0.658,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:13.980585+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",AGIEval,"['reasoning', 'general', 'math']",text,False,1.0,en,"A human-centric benchmark for evaluating foundation models on standardized exams including college entrance exams (Gaokao, SAT), law school admission tests (LSAT), math competitions, lawyer qualification tests, and civil service exams. Contains 20 tasks (18 multiple-choice, 2 cloze) designed to assess understanding, knowledge, reasoning, and calculation abilities in real-world academic and professional contexts.",0.658,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,0-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.143960+00:00,benchmark_result,,,,True,,,,,,31.0,,mistral-small-24b-base-2501,,,,0.9129,,,mistral,,,,,,,,0.9129,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.143960+00:00,,,,,,False,,False,0.0,False,0.0,0.9129,0.9129,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.9129,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),mistral
,"5-shot, CoT",,,gpqa,GPQA,,,2025-07-19T19:56:11.748111+00:00,benchmark_result,,,,True,,,,,,345.0,,mistral-small-24b-base-2501,,,,0.3437,,,mistral,,,,,,,,0.3437,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.748111+00:00,,,,,,False,,False,0.0,False,0.0,0.3437,0.3437,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.3437,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,"5-shot, maj@1",,,gsm8k,GSM8k,,,2025-07-19T19:56:13.111924+00:00,benchmark_result,,,,True,,,,,,1013.0,,mistral-small-24b-base-2501,,,,0.8073,,,mistral,,,,,,,,0.8073,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:13.111924+00:00,,,,,,False,,False,0.0,False,0.0,0.8073,0.8073,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.8073,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,"5-shot, MaJ",,,math,MATH,,,2025-07-19T19:56:11.898806+00:00,benchmark_result,,,,True,,,,,,424.0,,mistral-small-24b-base-2501,,,,0.4598,,,mistral,,,,,,,,0.4598,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.898806+00:00,,,,,,False,,False,0.0,False,0.0,0.4598,0.4598,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.4598,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,Pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.516399+00:00,benchmark_result,,,,True,,,,,,1195.0,,mistral-small-24b-base-2501,,,,0.6964,,,mistral,,,,,,,,0.6964,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:13.516399+00:00,,,,,,False,,False,0.0,False,0.0,0.6964,0.6964,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.006964000000000001,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.311218+00:00,benchmark_result,,,,True,,,,,,113.0,,mistral-small-24b-base-2501,,,,0.8073,,,mistral,,,,,,,,0.8073,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.311218+00:00,,,,,,False,,False,0.0,False,0.0,0.8073,0.8073,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.8073,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.511957+00:00,benchmark_result,,,,True,,,,,,217.0,,mistral-small-24b-base-2501,,,,0.5437,,,mistral,,,,,,,,0.5437,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.511957+00:00,,,,,,False,,False,0.0,False,0.0,0.5437,0.5437,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.5437,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.585944+00:00,benchmark_result,,,,True,,,,,,254.0,,mistral-small-24b-base-2501,,,,0.8032,,,mistral,,,,,,,,0.8032,https://huggingface.co/mistralai/Mistral-Small-24B-Base-2501,,,,,,,,2025-07-19T19:56:11.585944+00:00,,,,,,False,,False,0.0,False,0.0,0.8032,0.8032,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Base,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is competitive with larger models such as Llama 3.3 70B or Qwen 32B, and is an excellent open replacement for opaque proprietary models like GPT4o-mini. Mistral Small 3 is on par with Llama 3.3 70B instruct, while being more than 3x faster on the same hardware.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.8032,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,Score,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.120697+00:00,benchmark_result,,,,True,,,,,,1465.0,,mistral-small-24b-instruct-2501,,,,0.876,,,mistral,,,,,,,,0.876,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:14.120697+00:00,,,,,,False,,False,0.0,False,0.0,0.876,0.876,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.876,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,5 shot COT,,,gpqa,GPQA,,,2025-07-19T19:56:11.746578+00:00,benchmark_result,,,,True,,,,,,344.0,,mistral-small-24b-instruct-2501,,,,0.453,,,mistral,,,,,,,,0.453,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:11.746578+00:00,,,,,,False,,False,0.0,False,0.0,0.453,0.453,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.453,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,5 shot COT,,,humaneval,HumanEval,,,2025-07-19T19:56:12.682647+00:00,benchmark_result,,,,True,,,,,,807.0,,mistral-small-24b-instruct-2501,,,,0.848,,,mistral,,,,,,,,0.848,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:12.682647+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.848,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,Score,,,ifeval,IFEval,,,2025-07-19T19:56:12.295754+00:00,benchmark_result,,,,True,,,,,,630.0,,mistral-small-24b-instruct-2501,,,,0.829,,,mistral,,,,,,,,0.829,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:12.295754+00:00,,,,,,False,,False,0.0,False,0.0,0.829,0.829,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.829,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,instruct,,,math,MATH,,,2025-07-19T19:56:11.896887+00:00,benchmark_result,,,,True,,,,,,423.0,,mistral-small-24b-instruct-2501,,,,0.706,,,mistral,,,,,,,,0.706,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:11.896887+00:00,,,,,,False,,False,0.0,False,0.0,0.706,0.706,Unknown,,,,,Undisclosed,Good (70-79%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.706,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),mistral
,5 shot COT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.510254+00:00,benchmark_result,,,,True,,,,,,216.0,,mistral-small-24b-instruct-2501,,,,0.663,,,mistral,,,,,,,,0.663,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:11.510254+00:00,,,,,,False,,False,0.0,False,0.0,0.663,0.663,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.663,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.537073+00:00,benchmark_result,,,,True,,,,,,1613.0,,mistral-small-24b-instruct-2501,,,,0.835,,,mistral,,,,,,,,0.835,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:14.537073+00:00,,,,,,False,,False,0.0,False,0.0,0.835,0.835,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00835,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,Score,,,wild-bench,Wild Bench,,,2025-07-19T19:56:15.128734+00:00,benchmark_result,,,,True,,,,,,1818.0,,mistral-small-24b-instruct-2501,,,,0.522,,,mistral,,,,,,,,0.522,https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501,,,,,,,,2025-07-19T19:56:15.128734+00:00,,,,,,False,,False,0.0,False,0.0,0.522,0.522,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-01-30,2025.0,1.0,2025-01,Very Large (>70B),"Mistral Small 3 is a 24B-parameter LLM licensed under Apache-2.0. It focuses on low-latency, high-efficiency instruction following, maintaining performance comparable to larger models. It provides quick, accurate responses for conversational agents, function calling, and domain-specific fine-tuning. Suitable for local inference when quantized, it rivals models 2–3× its size while using significantly fewer compute resources.",Wild Bench,"['general', 'reasoning']",text,False,1.0,en,"WildBench is an automated evaluation framework that benchmarks large language models using 1,024 challenging, real-world tasks selected from over one million human-chatbot conversation logs. It introduces two evaluation metrics (WB-Reward and WB-Score) that achieve high correlation with human preferences and uses task-specific checklists for systematic evaluation.",0.522,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.749533+00:00,benchmark_result,,,,True,,,,,,346.0,,mistral-small-3.1-24b-base-2503,,,,0.375,,,mistral,,,,,,,,0.375,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503,,,,,,,,2025-07-19T19:56:11.749533+00:00,,,,,,False,,False,0.0,False,0.0,0.375,0.375,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Base,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.375,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,-,,,mmlu,MMLU,,,2025-07-19T19:56:11.312907+00:00,benchmark_result,,,,True,,,,,,114.0,,mistral-small-3.1-24b-base-2503,,,,0.8101,,,mistral,,,,,,,,0.8101,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503,,,,,,,,2025-07-19T19:56:11.312907+00:00,,,,,,False,,False,0.0,False,0.0,0.8101,0.8101,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.1 24B Base,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.8101,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,0-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.513719+00:00,benchmark_result,,,,True,,,,,,218.0,,mistral-small-3.1-24b-base-2503,,,,0.5603,,,mistral,,,,,,,,0.5603,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503,,,,,,,,2025-07-19T19:56:11.513719+00:00,,,,,,False,,False,0.0,False,0.0,0.5603,0.5603,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Base,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.5603,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,CoT accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.207080+00:00,benchmark_result,,,,True,,,,,,587.0,,mistral-small-3.1-24b-base-2503,,,,0.5927,,,mistral,,,,,,,,0.5927,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503,,,,,,,,2025-07-19T19:56:12.207080+00:00,,,,,,False,,False,0.0,False,0.0,0.5927,0.5927,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Base,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.5927,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.587622+00:00,benchmark_result,,,,True,,,,,,255.0,,mistral-small-3.1-24b-base-2503,,,,0.805,,,mistral,,,,,,,,0.805,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Base-2503,,,,,,,,2025-07-19T19:56:11.587622+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.1 24B Base,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Pretrained base model version of Mistral Small 3.1. Features improved text performance, multimodal understanding, multilingual capabilities, and an expanded 128k token context window compared to Mistral Small 3. Designed for fine-tuning.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.805,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,"Diamond, 5-shot CoT",,,gpqa,GPQA,,,2025-07-19T19:56:11.740584+00:00,benchmark_result,,,,True,,,,,,340.0,,mistral-small-3.1-24b-instruct-2503,,,,0.4596,,,mistral,,,,,,,,0.4596,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.741944+00:00,,,,,,False,,False,0.0,False,0.0,0.4596,0.4596,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.4596,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,standard,,,humaneval,HumanEval,,,2025-07-19T19:56:12.677771+00:00,benchmark_result,,,,True,,,,,,805.0,,mistral-small-3.1-24b-instruct-2503,,,,0.8841,,,mistral,,,,,,,,0.8841,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:12.677771+00:00,,,,,,False,,False,0.0,False,0.0,0.8841,0.8841,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.8841,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,standard,,,math,MATH,,,2025-07-19T19:56:11.893255+00:00,benchmark_result,,,,True,,,,,,421.0,,mistral-small-3.1-24b-instruct-2503,,,,0.693,,,mistral,,,,,,,,0.693,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.893255+00:00,,,,,,False,,False,0.0,False,0.0,0.693,0.693,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.693,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,standard,,,mbpp,MBPP,,,2025-07-19T19:56:13.514872+00:00,benchmark_result,,,,True,,,,,,1194.0,,mistral-small-3.1-24b-instruct-2503,,,,0.7471,,,mistral,,,,,,,,0.7471,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:13.514872+00:00,,,,,,False,,False,0.0,False,0.0,0.7471,0.7471,Unknown,,,,,Undisclosed,Good (70-79%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.007471,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,-,,,mmlu,MMLU,,,2025-07-19T19:56:11.306426+00:00,benchmark_result,,,,True,,,,,,110.0,,mistral-small-3.1-24b-instruct-2503,,,,0.8062,,,mistral,,,,,,,,0.8062,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.306426+00:00,,,,,,False,,False,0.0,False,0.0,0.8062,0.8062,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.8062,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,5-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.508555+00:00,benchmark_result,,,,True,,,,,,215.0,,mistral-small-3.1-24b-instruct-2503,,,,0.6676,,,mistral,,,,,,,,0.6676,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.508555+00:00,,,,,,False,,False,0.0,False,0.0,0.6676,0.6676,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.6676,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,CoT accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.203401+00:00,benchmark_result,,,,True,,,,,,585.0,,mistral-small-3.1-24b-instruct-2503,,,,0.5927,,,mistral,,,,,,,,0.5927,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:12.203401+00:00,,,,,,False,,False,0.0,False,0.0,0.5927,0.5927,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.5927,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,"TotalAcc, Correct",,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.552923+00:00,benchmark_result,,,,True,,,,,,237.0,,mistral-small-3.1-24b-instruct-2503,,,,0.1043,,,mistral,,,,,,,,0.1043,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.552923+00:00,,,,,,False,,False,0.0,False,0.0,0.1043,0.1043,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.1043,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,5-shot,,,triviaqa,TriviaQA,,,2025-07-19T19:56:11.579482+00:00,benchmark_result,,,,True,,,,,,251.0,,mistral-small-3.1-24b-instruct-2503,,,,0.805,,,mistral,,,,,,,,0.805,https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503,,,,,,,,2025-07-19T19:56:11.579482+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.1 24B Instruct,mistral,Mistral AI,24000000000.0,24000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-17,2025.0,3.0,2025-03,Very Large (>70B),"Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.",TriviaQA,"['general', 'reasoning']",text,False,1.0,en,"A large-scale reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents (six per question on average) that provide high quality distant supervision for answering the questions. The dataset features relatively complex, compositional questions with considerable syntactic and lexical variability, requiring cross-sentence reasoning to find answers.",0.805,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,-,,,ai2d,AI2D,,,2025-08-03T22:06:15.105841+00:00,benchmark_result,,,,True,,,,,,16767.0,,mistral-small-3.2-24b-instruct-2506,,,,0.9291,,,mistral,,,,,,,,0.9291,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.105841+00:00,,,,,,False,,False,0.0,False,0.0,0.9291,0.9291,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.9291,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),mistral
,v2,,,arena-hard,Arena Hard,,,2025-08-03T22:06:15.107885+00:00,benchmark_result,,,,True,,,,,,16768.0,,mistral-small-3.2-24b-instruct-2506,,,,0.431,,,mistral,,,,,,,,0.431,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.107885+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.431,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,-,,,chartqa,ChartQA,,,2025-08-03T22:06:15.109760+00:00,benchmark_result,,,,True,,,,,,16769.0,,mistral-small-3.2-24b-instruct-2506,,,,0.874,,,mistral,,,,,,,,0.874,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.109760+00:00,,,,,,False,,False,0.0,False,0.0,0.874,0.874,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.874,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),mistral
,-,,,docvqa,DocVQA,,,2025-08-03T22:06:15.111977+00:00,benchmark_result,,,,True,,,,,,16770.0,,mistral-small-3.2-24b-instruct-2506,,,,0.9486,,,mistral,,,,,,,,0.9486,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.111977+00:00,,,,,,False,,False,0.0,False,0.0,0.9486,0.9486,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.9486,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),mistral
,5-shot CoT,,,gpqa,GPQA,,,2025-08-03T22:06:15.113518+00:00,benchmark_result,,,,True,,,,,,16771.0,,mistral-small-3.2-24b-instruct-2506,,,,0.4613,,,mistral,,,,,,,,0.4613,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.113518+00:00,,,,,,False,,False,0.0,False,0.0,0.4613,0.4613,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.4613,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,Pass@5,,,humaneval-plus,HumanEval Plus,,,2025-08-03T22:06:15.116763+00:00,benchmark_result,,,,True,,,,,,16773.0,,mistral-small-3.2-24b-instruct-2506,,,,0.929,,,mistral,,,,,,,,0.929,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.116763+00:00,,,,,,False,,False,0.0,False,0.0,0.929,0.929,Unknown,,,,,Undisclosed,Excellent (90%+),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,HumanEval Plus,"['reasoning', 'code']",text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.929,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),mistral
,-,,,if,IF,,,2025-08-03T22:06:15.118250+00:00,benchmark_result,,,,True,,,,,,16774.0,,mistral-small-3.2-24b-instruct-2506,,,,0.8478,,,mistral,,,,,,,,0.8478,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.118250+00:00,,,,,,False,,False,0.0,False,0.0,0.8478,0.8478,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,IF,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.8478,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,5-shot,,,math,MATH,,,2025-08-03T22:06:15.119723+00:00,benchmark_result,,,,True,,,,,,16775.0,,mistral-small-3.2-24b-instruct-2506,,,,0.6942,,,mistral,,,,,,,,0.6942,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.119723+00:00,,,,,,False,,False,0.0,False,0.0,0.6942,0.6942,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.6942,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,-,,,mathvista,MathVista,,,2025-08-03T22:06:15.121246+00:00,benchmark_result,,,,True,,,,,,16776.0,,mistral-small-3.2-24b-instruct-2506,,,,0.6709,,,mistral,,,,,,,,0.6709,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.121246+00:00,,,,,,False,,False,0.0,False,0.0,0.6709,0.6709,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.6709,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),mistral
,Pass@5,,,mbpp-plus,MBPP Plus,,,2025-08-03T22:06:15.122828+00:00,benchmark_result,,,,True,,,,,,16777.0,,mistral-small-3.2-24b-instruct-2506,,,,0.7833,,,mistral,,,,,,,,0.7833,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.122828+00:00,,,,,,False,,False,0.0,False,0.0,0.7833,0.7833,Unknown,,,,,Undisclosed,Good (70-79%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MBPP Plus,"['reasoning', 'code']",text,False,1.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality. This is an enhanced version with additional test cases for more rigorous evaluation.",0.7833,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),mistral
,5-shot,,,mmlu,MMLU,,,2025-08-03T22:06:15.124220+00:00,benchmark_result,,,,True,,,,,,16778.0,,mistral-small-3.2-24b-instruct-2506,,,,0.805,,,mistral,,,,,,,,0.805,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.124220+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.805,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),mistral
,5-shot CoT,,,mmlu-pro,MMLU-Pro,,,2025-08-03T22:06:15.125972+00:00,benchmark_result,,,,True,,,,,,16779.0,,mistral-small-3.2-24b-instruct-2506,,,,0.6906,,,mistral,,,,,,,,0.6906,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.125972+00:00,,,,,,False,,False,0.0,False,0.0,0.6906,0.6906,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.6906,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,-,,,mmmu,MMMU,,,2025-08-03T22:06:15.127425+00:00,benchmark_result,,,,True,,,,,,16780.0,,mistral-small-3.2-24b-instruct-2506,,,,0.625,,,mistral,,,,,,,,0.625,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.127425+00:00,,,,,,False,,False,0.0,False,0.0,0.625,0.625,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.625,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,TotalAcc,,,simpleqa,SimpleQA,,,2025-08-03T22:06:15.129114+00:00,benchmark_result,,,,True,,,,,,16781.0,,mistral-small-3.2-24b-instruct-2506,,,,0.121,,,mistral,,,,,,,,0.121,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.129114+00:00,,,,,,False,,False,0.0,False,0.0,0.121,0.121,Unknown,,,,,Undisclosed,Poor (<60%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.121,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),mistral
,v2,,,wild-bench,Wild Bench,,,2025-08-03T22:06:15.130665+00:00,benchmark_result,,,,True,,,,,,16782.0,,mistral-small-3.2-24b-instruct-2506,,,,0.6533,,,mistral,,,,,,,,0.6533,https://huggingface.co/mistralai/Mistral-Small-3.2-24B-Instruct-2506,,,,,,,,2025-08-03T22:06:15.130665+00:00,,,,,,False,,False,0.0,False,0.0,0.6533,0.6533,Unknown,,,,,Undisclosed,Fair (60-69%),Mistral Small 3.2 24B Instruct,mistral,Mistral AI,23600000000.0,23600000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-06-20,2025.0,6.0,2025-06,Very Large (>70B),Mistral-Small-3.2-24B-Instruct-2506 is a minor update of Mistral-Small-3.1-24B-Instruct-2503.,Wild Bench,"['general', 'reasoning']",text,False,1.0,en,"WildBench is an automated evaluation framework that benchmarks large language models using 1,024 challenging, real-world tasks selected from over one million human-chatbot conversation logs. It introduces two evaluation metrics (WB-Reward and WB-Score) that achieve high correlation with human preferences and uses task-specific checklists for systematic evaluation.",0.6533,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),mistral
,0-shot chain-of-thought,,,arc-c,ARC-C,,,2025-07-19T19:56:11.080108+00:00,benchmark_result,,,,True,,,,,,2.0,,nova-lite,,,,0.924,,,amazon,,,,,,,,0.924,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.080108+00:00,,,,,,False,,False,0.0,False,0.0,0.924,0.924,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.924,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,3-shot CoT,,,bbh,BBH,,,2025-07-19T19:56:13.034481+00:00,benchmark_result,,,,True,,,,,,967.0,,nova-lite,,,,0.824,,,amazon,,,,,,,,0.824,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:13.034481+00:00,,,,,,False,,False,0.0,False,0.0,0.824,0.824,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.824,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,bfcl,BFCL,,,2025-07-19T19:56:12.766776+00:00,benchmark_result,,,,True,,,,,,843.0,,nova-lite,,,,0.666,,,amazon,,,,,,,,0.666,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.766776+00:00,,,,,,False,,False,0.0,False,0.0,0.666,0.666,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.666,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,relaxed accuracy,,,chartqa,ChartQA,,,2025-07-19T19:56:12.786772+00:00,benchmark_result,,,,True,,,,,,853.0,,nova-lite,,,,0.868,,,amazon,,,,,,,,0.868,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.786772+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.868,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,crag,CRAG,,,2025-07-19T19:56:12.743484+00:00,benchmark_result,,,,True,,,,,,834.0,,nova-lite,,,,0.438,,,amazon,,,,,,,,0.438,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.743484+00:00,,,,,,False,,False,0.0,False,0.0,0.438,0.438,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",CRAG,"['reasoning', 'search']",text,False,1.0,en,"CRAG (Comprehensive RAG Benchmark) is a factual question answering benchmark consisting of 4,409 question-answer pairs across 5 domains (finance, sports, music, movie, open domain) and 8 question categories. The benchmark includes mock APIs to simulate web and Knowledge Graph search, designed to represent the diverse and dynamic nature of real-world QA tasks with temporal dynamism ranging from years to seconds. It evaluates retrieval-augmented generation systems for trustworthy question answering.",0.438,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,ANLS,,,docvqa,DocVQA,,,2025-07-19T19:56:12.827478+00:00,benchmark_result,,,,True,,,,,,876.0,,nova-lite,,,,0.924,,,amazon,,,,,,,,0.924,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.827478+00:00,,,,,,False,,False,0.0,False,0.0,0.924,0.924,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.924,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),nova
,0-shot,,,drop,DROP,,,2025-07-19T19:56:12.984716+00:00,benchmark_result,,,,True,,,,,,939.0,,nova-lite,,,,0.802,,,amazon,,,,,,,,0.802,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.984716+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.802,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.918221+00:00,benchmark_result,,,,True,,,,,,918.0,,nova-lite,,,,0.714,,,amazon,,,,,,,,0.714,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.918221+00:00,,,,,,False,,False,0.0,False,0.0,0.714,0.714,Unknown,,,,,Undisclosed,Good (70-79%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.714,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),nova
,0-shot accuracy,,,finqa,FinQA,,,2025-07-19T19:56:12.736609+00:00,benchmark_result,,,,True,,,,,,831.0,,nova-lite,,,,0.736,,,amazon,,,,,,,,0.736,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.736609+00:00,,,,,,False,,False,0.0,False,0.0,0.736,0.736,Unknown,,,,,Undisclosed,Good (70-79%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",FinQA,"['finance', 'math', 'reasoning']",text,False,1.0,en,"A large-scale dataset for numerical reasoning over financial data with question-answering pairs written by financial experts, featuring complex numerical reasoning and understanding of heterogeneous representations with annotated gold reasoning programs for full explainability",0.736,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,6-shot CoT,,,gpqa,GPQA,,,2025-07-19T19:56:11.594691+00:00,benchmark_result,,,,True,,,,,,258.0,,nova-lite,,,,0.42,,,amazon,,,,,,,,0.42,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.594691+00:00,,,,,,False,,False,0.0,False,0.0,0.42,0.42,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.42,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,accuracy,,,groundui-1k,GroundUI-1K,,,2025-07-19T19:56:12.761300+00:00,benchmark_result,,,,True,,,,,,841.0,,nova-lite,,,,0.802,,,amazon,,,,,,,,0.802,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.761300+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",GroundUI-1K,"['multimodal', 'vision']",multimodal,False,1.0,en,"A subset of GroundUI-18K for UI grounding evaluation, where models must predict action coordinates on screenshots based on single-step instructions across web, desktop, and mobile platforms.",0.802,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,0-shot CoT,,,gsm8k,GSM8k,,,2025-07-19T19:56:11.407299+00:00,benchmark_result,,,,True,,,,,,160.0,,nova-lite,,,,0.945,,,amazon,,,,,,,,0.945,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.407299+00:00,,,,,,False,,False,0.0,False,0.0,0.945,0.945,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.945,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.601822+00:00,benchmark_result,,,,True,,,,,,759.0,,nova-lite,,,,0.854,,,amazon,,,,,,,,0.854,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.601822+00:00,,,,,,False,,False,0.0,False,0.0,0.854,0.854,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.854,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,0-shot CoT,,,ifeval,IFEval,,,2025-07-19T19:56:12.248959+00:00,benchmark_result,,,,True,,,,,,604.0,,nova-lite,,,,0.897,,,amazon,,,,,,,,0.897,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.248959+00:00,,,,,,False,,False,0.0,False,0.0,0.897,0.897,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.897,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,lvbench,LVBench,,,2025-07-19T19:56:12.726573+00:00,benchmark_result,,,,True,,,,,,826.0,,nova-lite,,,,0.404,,,amazon,,,,,,,,0.404,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.726573+00:00,,,,,,False,,False,0.0,False,0.0,0.404,0.404,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",LVBench,"['vision', 'multimodal', 'long_context']",multimodal,False,1.0,en,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",0.404,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),nova
,0-shot CoT,,,math,MATH,,,2025-07-19T19:56:11.810622+00:00,benchmark_result,,,,True,,,,,,374.0,,nova-lite,,,,0.733,,,amazon,,,,,,,,0.733,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.810622+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.733,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,accuracy,,,mm-mind2web,MM-Mind2Web,,,2025-07-19T19:56:12.755878+00:00,benchmark_result,,,,True,,,,,,839.0,,nova-lite,,,,0.607,,,amazon,,,,,,,,0.607,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.755878+00:00,,,,,,False,,False,0.0,False,0.0,0.607,0.607,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",MM-Mind2Web,"['multimodal', 'frontend_development', 'reasoning']",multimodal,False,1.0,en,"A multimodal web navigation benchmark comprising 2,000 open-ended tasks spanning 137 websites across 31 domains. Each task includes HTML documents paired with webpage screenshots, action sequences, and complex web interactions.",0.607,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,0-shot chain-of-thought,,,mmlu,MMLU,,,2025-07-19T19:56:11.212315+00:00,benchmark_result,,,,True,,,,,,60.0,,nova-lite,,,,0.805,,,amazon,,,,,,,,0.805,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.212315+00:00,,,,,,False,,False,0.0,False,0.0,0.805,0.805,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.805,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,CoT accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.134288+00:00,benchmark_result,,,,True,,,,,,550.0,,nova-lite,,,,0.562,,,amazon,,,,,,,,0.562,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.134288+00:00,,,,,,False,,False,0.0,False,0.0,0.562,0.562,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.562,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,rouge-l,,,squality,SQuALITY,,,2025-07-19T19:56:12.715662+00:00,benchmark_result,,,,True,,,,,,821.0,,nova-lite,,,,0.192,,,amazon,,,,,,,,0.192,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.715662+00:00,,,,,,False,,False,0.0,False,0.0,0.192,0.192,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",SQuALITY,"['summarization', 'long_context', 'language']",text,False,1.0,en,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",0.192,False,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,weighted accuracy,,,textvqa,TextVQA,,,2025-07-19T19:56:12.878076+00:00,benchmark_result,,,,True,,,,,,901.0,,nova-lite,,,,0.802,,,amazon,,,,,,,,0.802,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.878076+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.802,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,COMET22,,,translation-en→set1-comet22,Translation en→Set1 COMET22,,,2025-07-19T19:56:12.962491+00:00,benchmark_result,,,,True,,,,,,930.0,,nova-lite,,,,0.888,,,amazon,,,,,,,,0.888,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.962491+00:00,,,,,,False,,False,0.0,False,0.0,0.888,0.888,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",Translation en→Set1 COMET22,['language'],text,True,1.0,en,COMET-22 is an ensemble machine translation evaluation metric combining a COMET estimator model trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It demonstrates improved correlations compared to state-of-the-art metrics and increased robustness to critical errors.,0.888,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-en→set1-spbleu,Translation en→Set1 spBleu,,,2025-07-19T19:56:12.942744+00:00,benchmark_result,,,,True,,,,,,927.0,,nova-lite,,,,0.415,,,amazon,,,,,,,,0.415,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.942744+00:00,,,,,,False,,False,0.0,False,0.0,0.415,0.415,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",Translation en→Set1 spBleu,['language'],text,True,1.0,en,"Translation evaluation using spBLEU (SentencePiece BLEU), a BLEU metric computed over text tokenized with a language-agnostic SentencePiece subword model. Introduced in the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.415,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,COMET22,,,translation-set1→en-comet22,Translation Set1→en COMET22,,,2025-07-19T19:56:12.977060+00:00,benchmark_result,,,,True,,,,,,936.0,,nova-lite,,,,0.888,,,amazon,,,,,,,,0.888,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.977060+00:00,,,,,,False,,False,0.0,False,0.0,0.888,0.888,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",Translation Set1→en COMET22,['language'],text,True,1.0,en,COMET-22 is a neural machine translation evaluation metric that uses an ensemble of two models: a COMET estimator trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It provides improved correlations with human judgments and increased robustness to critical errors compared to previous metrics.,0.888,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-set1→en-spbleu,Translation Set1→en spBleu,,,2025-07-19T19:56:12.969524+00:00,benchmark_result,,,,True,,,,,,933.0,,nova-lite,,,,0.431,,,amazon,,,,,,,,0.431,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.969524+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",Translation Set1→en spBleu,['language'],text,True,1.0,en,"spBLEU (SentencePiece BLEU) evaluation metric for machine translation quality assessment, using language-agnostic SentencePiece tokenization with BLEU scoring. Part of the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.431,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,CIDEr,,,vatex,VATEX,,,2025-07-19T19:56:12.912261+00:00,benchmark_result,,,,True,,,,,,916.0,,nova-lite,,,,0.778,,,amazon,,,,,,,,0.778,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.912261+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",VATEX,"['multimodal', 'video', 'language']",multimodal,True,1.0,en,"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. Contains over 41,250 videos and 825,000 captions in both English and Chinese, with over 206,000 English-Chinese parallel translation pairs. Supports multilingual video captioning and video-guided machine translation tasks.",0.778,False,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,composite step accuracy,,,visualwebbench,VisualWebBench,,,2025-07-19T19:56:12.750738+00:00,benchmark_result,,,,True,,,,,,837.0,,nova-lite,,,,0.777,,,amazon,,,,,,,,0.777,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.750738+00:00,,,,,,False,,False,0.0,False,0.0,0.777,0.777,Unknown,,,,,Undisclosed,Good (70-79%),Nova Lite,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"A low-cost multimodal model that is lightning fast for processing images, video, documents, and text.",VisualWebBench,"['vision', 'multimodal', 'frontend_development']",multimodal,False,1.0,en,"A multimodal benchmark designed to assess the capabilities of multimodal large language models (MLLMs) across web page understanding and grounding tasks. Comprises 7 tasks (captioning, webpage QA, heading OCR, element OCR, element grounding, action prediction, and action grounding) with 1.5K human-curated instances from 139 real websites across 87 sub-domains.",0.777,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),nova
,0-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.088301+00:00,benchmark_result,,,,True,,,,,,4.0,,nova-micro,,,,0.902,,,amazon,,,,,,,,0.902,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.088301+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.902,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,3-shot Chain-of-Thought,,,bbh,BBH,,,2025-07-19T19:56:13.038288+00:00,benchmark_result,,,,True,,,,,,969.0,,nova-micro,,,,0.795,,,amazon,,,,,,,,0.795,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:13.038288+00:00,,,,,,False,,False,0.0,False,0.0,0.795,0.795,Unknown,,,,,Undisclosed,Good (70-79%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.795,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,accuracy,,,bfcl,BFCL,,,2025-07-19T19:56:12.770319+00:00,benchmark_result,,,,True,,,,,,845.0,,nova-micro,,,,0.562,,,amazon,,,,,,,,0.562,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.770319+00:00,,,,,,False,,False,0.0,False,0.0,0.562,0.562,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.562,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,accuracy,,,crag,CRAG,,,2025-07-19T19:56:12.746657+00:00,benchmark_result,,,,True,,,,,,836.0,,nova-micro,,,,0.431,,,amazon,,,,,,,,0.431,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.746657+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,CRAG,"['reasoning', 'search']",text,False,1.0,en,"CRAG (Comprehensive RAG Benchmark) is a factual question answering benchmark consisting of 4,409 question-answer pairs across 5 domains (finance, sports, music, movie, open domain) and 8 question categories. The benchmark includes mock APIs to simulate web and Knowledge Graph search, designed to represent the diverse and dynamic nature of real-world QA tasks with temporal dynamism ranging from years to seconds. It evaluates retrieval-augmented generation systems for trustworthy question answering.",0.431,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,6-shot Chain-of-Thought,,,drop,DROP,,,2025-07-19T19:56:12.987950+00:00,benchmark_result,,,,True,,,,,,941.0,,nova-micro,,,,0.793,,,amazon,,,,,,,,0.793,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.987950+00:00,,,,,,False,,False,0.0,False,0.0,0.793,0.793,Unknown,,,,,Undisclosed,Good (70-79%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.793,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,0-shot accuracy,,,finqa,FinQA,,,2025-07-19T19:56:12.740201+00:00,benchmark_result,,,,True,,,,,,833.0,,nova-micro,,,,0.652,,,amazon,,,,,,,,0.652,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.740201+00:00,,,,,,False,,False,0.0,False,0.0,0.652,0.652,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,FinQA,"['finance', 'math', 'reasoning']",text,False,1.0,en,"A large-scale dataset for numerical reasoning over financial data with question-answering pairs written by financial experts, featuring complex numerical reasoning and understanding of heterogeneous representations with annotated gold reasoning programs for full explainability",0.652,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,0-shot Chain-of-Thought,,,gpqa,GPQA,,,2025-07-19T19:56:11.598530+00:00,benchmark_result,,,,True,,,,,,260.0,,nova-micro,,,,0.4,,,amazon,,,,,,,,0.4,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.598530+00:00,,,,,,False,,False,0.0,False,0.0,0.4,0.4,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.4,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,0-shot Chain-of-Thought,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.051041+00:00,benchmark_result,,,,True,,,,,,976.0,,nova-micro,,,,0.923,,,amazon,,,,,,,,0.923,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:13.051041+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.923,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.605066+00:00,benchmark_result,,,,True,,,,,,761.0,,nova-micro,,,,0.811,,,amazon,,,,,,,,0.811,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.605066+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.811,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,0-shot,,,ifeval,IFEval,,,2025-07-19T19:56:12.252589+00:00,benchmark_result,,,,True,,,,,,606.0,,nova-micro,,,,0.872,,,amazon,,,,,,,,0.872,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.252589+00:00,,,,,,False,,False,0.0,False,0.0,0.872,0.872,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.872,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,0-shot Chain-of-Thought,,,math,MATH,,,2025-07-19T19:56:11.814150+00:00,benchmark_result,,,,True,,,,,,376.0,,nova-micro,,,,0.693,,,amazon,,,,,,,,0.693,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.814150+00:00,,,,,,False,,False,0.0,False,0.0,0.693,0.693,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.693,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,0-shot Chain-of-Thought,,,mmlu,MMLU,,,2025-07-19T19:56:11.217284+00:00,benchmark_result,,,,True,,,,,,62.0,,nova-micro,,,,0.776,,,amazon,,,,,,,,0.776,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.217284+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.776,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,rouge-l,,,squality,SQuALITY,,,2025-07-19T19:56:12.719314+00:00,benchmark_result,,,,True,,,,,,823.0,,nova-micro,,,,0.188,,,amazon,,,,,,,,0.188,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.719314+00:00,,,,,,False,,False,0.0,False,0.0,0.188,0.188,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,SQuALITY,"['summarization', 'long_context', 'language']",text,False,1.0,en,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",0.188,False,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,COMET22,,,translation-en→set1-comet22,Translation en→Set1 COMET22,,,2025-07-19T19:56:12.966157+00:00,benchmark_result,,,,True,,,,,,932.0,,nova-micro,,,,0.885,,,amazon,,,,,,,,0.885,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.966157+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,Translation en→Set1 COMET22,['language'],text,True,1.0,en,COMET-22 is an ensemble machine translation evaluation metric combining a COMET estimator model trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It demonstrates improved correlations compared to state-of-the-art metrics and increased robustness to critical errors.,0.885,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-en→set1-spbleu,Translation en→Set1 spBleu,,,2025-07-19T19:56:12.958167+00:00,benchmark_result,,,,True,,,,,,929.0,,nova-micro,,,,0.402,,,amazon,,,,,,,,0.402,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.958167+00:00,,,,,,False,,False,0.0,False,0.0,0.402,0.402,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,Translation en→Set1 spBleu,['language'],text,True,1.0,en,"Translation evaluation using spBLEU (SentencePiece BLEU), a BLEU metric computed over text tokenized with a language-agnostic SentencePiece subword model. Introduced in the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.402,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,COMET22,,,translation-set1→en-comet22,Translation Set1→en COMET22,,,2025-07-19T19:56:12.980365+00:00,benchmark_result,,,,True,,,,,,938.0,,nova-micro,,,,0.887,,,amazon,,,,,,,,0.887,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.980365+00:00,,,,,,False,,False,0.0,False,0.0,0.887,0.887,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,Translation Set1→en COMET22,['language'],text,True,1.0,en,COMET-22 is a neural machine translation evaluation metric that uses an ensemble of two models: a COMET estimator trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It provides improved correlations with human judgments and increased robustness to critical errors compared to previous metrics.,0.887,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-set1→en-spbleu,Translation Set1→en spBleu,,,2025-07-19T19:56:12.973209+00:00,benchmark_result,,,,True,,,,,,935.0,,nova-micro,,,,0.426,,,amazon,,,,,,,,0.426,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.973209+00:00,,,,,,False,,False,0.0,False,0.0,0.426,0.426,Unknown,,,,,Undisclosed,Poor (<60%),Nova Micro,amazon,Amazon,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,A text-only model that delivers lowest-latency responses at very low cost while maintaining strong performance on core language tasks. Optimized for speed and efficiency while preserving high accuracy on key benchmarks.,Translation Set1→en spBleu,['language'],text,True,1.0,en,"spBLEU (SentencePiece BLEU) evaluation metric for machine translation quality assessment, using language-agnostic SentencePiece tokenization with BLEU scoring. Part of the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.426,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,0-shot Chain-of-Thought,,,arc-c,ARC-C,,,2025-07-19T19:56:11.085849+00:00,benchmark_result,,,,True,,,,,,3.0,,nova-pro,,,,0.948,,,amazon,,,,,,,,0.948,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.085849+00:00,,,,,,False,,False,0.0,False,0.0,0.948,0.948,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.948,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,3-shot Chain-of-Thought,,,bbh,BBH,,,2025-07-19T19:56:13.036192+00:00,benchmark_result,,,,True,,,,,,968.0,,nova-pro,,,,0.869,,,amazon,,,,,,,,0.869,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:13.036192+00:00,,,,,,False,,False,0.0,False,0.0,0.869,0.869,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.869,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,bfcl,BFCL,,,2025-07-19T19:56:12.768714+00:00,benchmark_result,,,,True,,,,,,844.0,,nova-pro,,,,0.684,,,amazon,,,,,,,,0.684,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.768714+00:00,,,,,,False,,False,0.0,False,0.0,0.684,0.684,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.684,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,relaxed accuracy,,,chartqa,ChartQA,,,2025-07-19T19:56:12.788270+00:00,benchmark_result,,,,True,,,,,,854.0,,nova-pro,,,,0.892,,,amazon,,,,,,,,0.892,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.788270+00:00,,,,,,False,,False,0.0,False,0.0,0.892,0.892,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.892,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,crag,CRAG,,,2025-07-19T19:56:12.744994+00:00,benchmark_result,,,,True,,,,,,835.0,,nova-pro,,,,0.503,,,amazon,,,,,,,,0.503,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.744994+00:00,,,,,,False,,False,0.0,False,0.0,0.503,0.503,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",CRAG,"['reasoning', 'search']",text,False,1.0,en,"CRAG (Comprehensive RAG Benchmark) is a factual question answering benchmark consisting of 4,409 question-answer pairs across 5 domains (finance, sports, music, movie, open domain) and 8 question categories. The benchmark includes mock APIs to simulate web and Knowledge Graph search, designed to represent the diverse and dynamic nature of real-world QA tasks with temporal dynamism ranging from years to seconds. It evaluates retrieval-augmented generation systems for trustworthy question answering.",0.503,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,ANLS,,,docvqa,DocVQA,,,2025-07-19T19:56:12.829064+00:00,benchmark_result,,,,True,,,,,,877.0,,nova-pro,,,,0.935,,,amazon,,,,,,,,0.935,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.829064+00:00,,,,,,False,,False,0.0,False,0.0,0.935,0.935,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.935,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),nova
,0-shot,,,drop,DROP,,,2025-07-19T19:56:12.986311+00:00,benchmark_result,,,,True,,,,,,940.0,,nova-pro,,,,0.854,,,amazon,,,,,,,,0.854,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.986311+00:00,,,,,,False,,False,0.0,False,0.0,0.854,0.854,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.854,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,accuracy,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.920400+00:00,benchmark_result,,,,True,,,,,,919.0,,nova-pro,,,,0.721,,,amazon,,,,,,,,0.721,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.920400+00:00,,,,,,False,,False,0.0,False,0.0,0.721,0.721,Unknown,,,,,Undisclosed,Good (70-79%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.721,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),nova
,0-shot accuracy,,,finqa,FinQA,,,2025-07-19T19:56:12.738456+00:00,benchmark_result,,,,True,,,,,,832.0,,nova-pro,,,,0.772,,,amazon,,,,,,,,0.772,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.738456+00:00,,,,,,False,,False,0.0,False,0.0,0.772,0.772,Unknown,,,,,Undisclosed,Good (70-79%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",FinQA,"['finance', 'math', 'reasoning']",text,False,1.0,en,"A large-scale dataset for numerical reasoning over financial data with question-answering pairs written by financial experts, featuring complex numerical reasoning and understanding of heterogeneous representations with annotated gold reasoning programs for full explainability",0.772,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,6-shot Chain-of-Thought,,,gpqa,GPQA,,,2025-07-19T19:56:11.596541+00:00,benchmark_result,,,,True,,,,,,259.0,,nova-pro,,,,0.469,,,amazon,,,,,,,,0.469,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.596541+00:00,,,,,,False,,False,0.0,False,0.0,0.469,0.469,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.469,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,accuracy,,,groundui-1k,GroundUI-1K,,,2025-07-19T19:56:12.762846+00:00,benchmark_result,,,,True,,,,,,842.0,,nova-pro,,,,0.814,,,amazon,,,,,,,,0.814,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.762846+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",GroundUI-1K,"['multimodal', 'vision']",multimodal,False,1.0,en,"A subset of GroundUI-18K for UI grounding evaluation, where models must predict action coordinates on screenshots based on single-step instructions across web, desktop, and mobile platforms.",0.814,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,0-shot Chain-of-Thought,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.049455+00:00,benchmark_result,,,,True,,,,,,975.0,,nova-pro,,,,0.948,,,amazon,,,,,,,,0.948,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:13.049455+00:00,,,,,,False,,False,0.0,False,0.0,0.948,0.948,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.948,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,0-shot pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.603428+00:00,benchmark_result,,,,True,,,,,,760.0,,nova-pro,,,,0.89,,,amazon,,,,,,,,0.89,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.603428+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.89,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,0-shot,,,ifeval,IFEval,,,2025-07-19T19:56:12.250818+00:00,benchmark_result,,,,True,,,,,,605.0,,nova-pro,,,,0.921,,,amazon,,,,,,,,0.921,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.250818+00:00,,,,,,False,,False,0.0,False,0.0,0.921,0.921,Unknown,,,,,Undisclosed,Excellent (90%+),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.921,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),nova
,accuracy,,,lvbench,LVBench,,,2025-07-19T19:56:12.728104+00:00,benchmark_result,,,,True,,,,,,827.0,,nova-pro,,,,0.416,,,amazon,,,,,,,,0.416,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.728104+00:00,,,,,,False,,False,0.0,False,0.0,0.416,0.416,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",LVBench,"['vision', 'multimodal', 'long_context']",multimodal,False,1.0,en,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",0.416,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),nova
,0-shot Chain-of-Thought,,,math,MATH,,,2025-07-19T19:56:11.812663+00:00,benchmark_result,,,,True,,,,,,375.0,,nova-pro,,,,0.766,,,amazon,,,,,,,,0.766,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.812663+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.766,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,step accuracy,,,mm-mind2web,MM-Mind2Web,,,2025-07-19T19:56:12.757670+00:00,benchmark_result,,,,True,,,,,,840.0,,nova-pro,,,,0.637,,,amazon,,,,,,,,0.637,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.757670+00:00,,,,,,False,,False,0.0,False,0.0,0.637,0.637,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",MM-Mind2Web,"['multimodal', 'frontend_development', 'reasoning']",multimodal,False,1.0,en,"A multimodal web navigation benchmark comprising 2,000 open-ended tasks spanning 137 websites across 31 domains. Each task includes HTML documents paired with webpage screenshots, action sequences, and complex web interactions.",0.637,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,0-shot Chain-of-Thought,,,mmlu,MMLU,,,2025-07-19T19:56:11.214544+00:00,benchmark_result,,,,True,,,,,,61.0,,nova-pro,,,,0.859,,,amazon,,,,,,,,0.859,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:11.214544+00:00,,,,,,False,,False,0.0,False,0.0,0.859,0.859,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.859,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,Chain-of-Thought,,,mmmu,MMMU,,,2025-07-19T19:56:12.135953+00:00,benchmark_result,,,,True,,,,,,551.0,,nova-pro,,,,0.617,,,amazon,,,,,,,,0.617,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.135953+00:00,,,,,,False,,False,0.0,False,0.0,0.617,0.617,Unknown,,,,,Undisclosed,Fair (60-69%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.617,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),nova
,ROUGE-L,,,squality,SQuALITY,,,2025-07-19T19:56:12.717624+00:00,benchmark_result,,,,True,,,,,,822.0,,nova-pro,,,,0.198,,,amazon,,,,,,,,0.198,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.717624+00:00,,,,,,False,,False,0.0,False,0.0,0.198,0.198,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",SQuALITY,"['summarization', 'long_context', 'language']",text,False,1.0,en,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",0.198,False,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,weighted accuracy,,,textvqa,TextVQA,,,2025-07-19T19:56:12.880228+00:00,benchmark_result,,,,True,,,,,,902.0,,nova-pro,,,,0.815,,,amazon,,,,,,,,0.815,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.880228+00:00,,,,,,False,,False,0.0,False,0.0,0.815,0.815,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.815,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),nova
,COMET22,,,translation-en→set1-comet22,Translation en→Set1 COMET22,,,2025-07-19T19:56:12.964047+00:00,benchmark_result,,,,True,,,,,,931.0,,nova-pro,,,,0.891,,,amazon,,,,,,,,0.891,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.964047+00:00,,,,,,False,,False,0.0,False,0.0,0.891,0.891,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",Translation en→Set1 COMET22,['language'],text,True,1.0,en,COMET-22 is an ensemble machine translation evaluation metric combining a COMET estimator model trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It demonstrates improved correlations compared to state-of-the-art metrics and increased robustness to critical errors.,0.891,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-en→set1-spbleu,Translation en→Set1 spBleu,,,2025-07-19T19:56:12.950458+00:00,benchmark_result,,,,True,,,,,,928.0,,nova-pro,,,,0.434,,,amazon,,,,,,,,0.434,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.950458+00:00,,,,,,False,,False,0.0,False,0.0,0.434,0.434,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",Translation en→Set1 spBleu,['language'],text,True,1.0,en,"Translation evaluation using spBLEU (SentencePiece BLEU), a BLEU metric computed over text tokenized with a language-agnostic SentencePiece subword model. Introduced in the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.434,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,COMET22,,,translation-set1→en-comet22,Translation Set1→en COMET22,,,2025-07-19T19:56:12.978787+00:00,benchmark_result,,,,True,,,,,,937.0,,nova-pro,,,,0.89,,,amazon,,,,,,,,0.89,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.978787+00:00,,,,,,False,,False,0.0,False,0.0,0.89,0.89,Unknown,,,,,Undisclosed,Very Good (80-89%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",Translation Set1→en COMET22,['language'],text,True,1.0,en,COMET-22 is a neural machine translation evaluation metric that uses an ensemble of two models: a COMET estimator trained with Direct Assessments and a multitask model that predicts sentence-level scores and word-level OK/BAD tags. It provides improved correlations with human judgments and increased robustness to critical errors compared to previous metrics.,0.89,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),nova
,spBleu,,,translation-set1→en-spbleu,Translation Set1→en spBleu,,,2025-07-19T19:56:12.971295+00:00,benchmark_result,,,,True,,,,,,934.0,,nova-pro,,,,0.444,,,amazon,,,,,,,,0.444,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.971295+00:00,,,,,,False,,False,0.0,False,0.0,0.444,0.444,Unknown,,,,,Undisclosed,Poor (<60%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",Translation Set1→en spBleu,['language'],text,True,1.0,en,"spBLEU (SentencePiece BLEU) evaluation metric for machine translation quality assessment, using language-agnostic SentencePiece tokenization with BLEU scoring. Part of the FLORES-101 evaluation benchmark for low-resource and multilingual machine translation.",0.444,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),nova
,CIDEr,,,vatex,VATEX,,,2025-07-19T19:56:12.913837+00:00,benchmark_result,,,,True,,,,,,917.0,,nova-pro,,,,0.778,,,amazon,,,,,,,,0.778,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.913837+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",VATEX,"['multimodal', 'video', 'language']",multimodal,True,1.0,en,"VaTeX: A Large-Scale, High-Quality Multilingual Dataset for Video-and-Language Research. Contains over 41,250 videos and 825,000 captions in both English and Chinese, with over 206,000 English-Chinese parallel translation pairs. Supports multilingual video captioning and video-guided machine translation tasks.",0.778,False,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),nova
,composite,,,visualwebbench,VisualWebBench,,,2025-07-19T19:56:12.752533+00:00,benchmark_result,,,,True,,,,,,838.0,,nova-pro,,,,0.797,,,amazon,,,,,,,,0.797,https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card,,,,,,,,2025-07-19T19:56:12.752533+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),Nova Pro,amazon,Amazon,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-11-20,2024.0,11.0,2024-11,Undisclosed,"Amazon Nova Pro is a highly-capable multimodal model with state-of-the-art performance across text, image, and video understanding. It excels at core capabilities like language understanding, mathematical reasoning, and multimodal tasks while offering industry-leading speed and cost efficiency.",VisualWebBench,"['vision', 'multimodal', 'frontend_development']",multimodal,False,1.0,en,"A multimodal benchmark designed to assess the capabilities of multimodal large language models (MLLMs) across web page understanding and grounding tasks. Comprises 7 tasks (captioning, webpage QA, heading OCR, element OCR, element grounding, action prediction, and action grounding) with 1.5K human-curated instances from 139 real websites across 87 sub-domains.",0.797,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),nova
,accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.025628+00:00,benchmark_result,,,,True,,,,,,490.0,,o1-2024-12-17,,,,0.743,,,openai,,,,,,,,0.743,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.025628+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.743,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,pass@1,,,frontiermath,FrontierMath,,,2025-07-19T19:56:15.186673+00:00,benchmark_result,,,,True,,,,,,1831.0,,o1-2024-12-17,,,,0.055,,,openai,,,,,,,,0.055,https://openai.com/index/o1-and-new-tools-for-developers/,,,,,,,,2025-07-19T19:56:15.186673+00:00,,,,,,False,,False,0.0,False,0.0,0.055,0.055,Unknown,,,,,Undisclosed,Poor (<60%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.055,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.768954+00:00,benchmark_result,,,,True,,,,,,358.0,,o1-2024-12-17,,,,0.78,,,openai,,,,,,,,0.78,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:11.768954+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.78,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,pass@1,,,gpqa-biology,GPQA Biology,,,2025-07-19T19:56:15.394088+00:00,benchmark_result,,,,True,,,,,,1911.0,,o1-2024-12-17,,,,0.692,,,openai,,,,,,,,0.692,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:15.394088+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GPQA Biology,"['reasoning', 'general']",text,False,1.0,en,"Biology subset of GPQA, containing challenging multiple-choice questions written by domain experts in biology. These Google-proof questions require graduate-level knowledge and reasoning.",0.692,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o1
,pass@1,,,gpqa-chemistry,GPQA Chemistry,,,2025-07-19T19:56:15.399030+00:00,benchmark_result,,,,True,,,,,,1912.0,,o1-2024-12-17,,,,0.647,,,openai,,,,,,,,0.647,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:15.399030+00:00,,,,,,False,,False,0.0,False,0.0,0.647,0.647,Unknown,,,,,Undisclosed,Fair (60-69%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GPQA Chemistry,"['reasoning', 'chemistry']",text,False,1.0,en,"Chemistry subset of GPQA, containing challenging multiple-choice questions written by domain experts in chemistry. These Google-proof questions require graduate-level knowledge and reasoning.",0.647,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o1
,pass@1,,,gpqa-physics,GPQA Physics,,,2025-07-19T19:56:15.403790+00:00,benchmark_result,,,,True,,,,,,1913.0,,o1-2024-12-17,,,,0.928,,,openai,,,,,,,,0.928,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:15.403790+00:00,,,,,,False,,False,0.0,False,0.0,0.928,0.928,Unknown,,,,,Undisclosed,Excellent (90%+),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GPQA Physics,"['reasoning', 'physics']",text,False,1.0,en,"Physics subset of GPQA, containing challenging multiple-choice questions written by domain experts in physics. These Google-proof questions require graduate-level knowledge and reasoning.",0.928,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,pass@1,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.116437+00:00,benchmark_result,,,,True,,,,,,1016.0,,o1-2024-12-17,,,,0.971,,,openai,,,,,,,,0.971,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:13.116437+00:00,,,,,,False,,False,0.0,False,0.0,0.971,0.971,Unknown,,,,,Undisclosed,Excellent (90%+),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.971,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),o1
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.696047+00:00,benchmark_result,,,,True,,,,,,814.0,,o1-2024-12-17,,,,0.881,,,openai,,,,,,,,0.881,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:12.696047+00:00,,,,,,False,,False,0.0,False,0.0,0.881,0.881,Unknown,,,,,Undisclosed,Very Good (80-89%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.881,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,coding,,,livebench,LiveBench,,,2025-07-19T19:56:12.587814+00:00,benchmark_result,,,,True,,,,,,755.0,,o1-2024-12-17,,,,0.67,,,openai,,,,,,,,0.67,https://openai.com/index/openai-o3-mini//,,,,,,,,2025-07-19T19:56:12.587814+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.67,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o1
,pass@1,,,math,MATH,,,2025-07-19T19:56:11.905279+00:00,benchmark_result,,,,True,,,,,,428.0,,o1-2024-12-17,,,,0.964,,,openai,,,,,,,,0.964,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:11.905279+00:00,,,,,,False,,False,0.0,False,0.0,0.964,0.964,Unknown,,,,,Undisclosed,Excellent (90%+),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.964,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),o1
,pass@1,,,mathvista,MathVista,,,2025-07-19T19:56:12.126058+00:00,benchmark_result,,,,True,,,,,,546.0,,o1-2024-12-17,,,,0.718,,,openai,,,,,,,,0.718,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.126058+00:00,,,,,,False,,False,0.0,False,0.0,0.718,0.718,Unknown,,,,,Undisclosed,Good (70-79%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.718,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),o1
,pass@1,,,mgsm,MGSM,,,2025-07-19T19:56:13.715686+00:00,benchmark_result,,,,True,,,,,,1298.0,,o1-2024-12-17,,,,0.893,,,openai,,,,,,,,0.893,https://openai.com/index/o1-and-new-tools-for-developers/,,,,,,,,2025-07-19T19:56:13.715686+00:00,,,,,,False,,False,0.0,False,0.0,0.893,0.893,Unknown,,,,,Undisclosed,Very Good (80-89%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.893,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,pass@1,,,mmlu,MMLU,,,2025-07-19T19:56:11.330211+00:00,benchmark_result,,,,True,,,,,,125.0,,o1-2024-12-17,,,,0.918,,,openai,,,,,,,,0.918,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:11.330211+00:00,,,,,,False,,False,0.0,False,0.0,0.918,0.918,Unknown,,,,,Undisclosed,Excellent (90%+),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.918,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,accuracy,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.165932+00:00,benchmark_result,,,,True,,,,,,1486.0,,o1-2024-12-17,,,,0.877,,,openai,,,,,,,,0.877,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.165932+00:00,,,,,,False,,False,0.0,False,0.0,0.877,0.877,Unknown,,,,,Undisclosed,Very Good (80-89%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.877,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,pass@1,,,mmmu,MMMU,,,2025-07-19T19:56:12.228467+00:00,benchmark_result,,,,True,,,,,,596.0,,o1-2024-12-17,,,,0.776,,,openai,,,,,,,,0.776,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.228467+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.776,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.561209+00:00,benchmark_result,,,,True,,,,,,241.0,,o1-2024-12-17,,,,0.47,,,openai,,,,,,,,0.47,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:11.561209+00:00,,,,,,False,,False,0.0,False,0.0,0.47,0.47,Unknown,,,,,Undisclosed,Poor (<60%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.47,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,verified,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.865799+00:00,benchmark_result,,,,True,,,,,,1361.0,,o1-2024-12-17,,,,0.41,,,openai,,,,,,,,0.41,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.865799+00:00,,,,,,False,,False,0.0,False,0.0,0.41,0.41,Unknown,,,,,Undisclosed,Poor (<60%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.41,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,agents,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.021642+00:00,benchmark_result,,,,True,,,,,,1783.0,,o1-2024-12-17,,,,0.5,,,openai,,,,,,,,0.5,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.021642+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.5,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,agents,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.992114+00:00,benchmark_result,,,,True,,,,,,1769.0,,o1-2024-12-17,,,,0.708,,,openai,,,,,,,,0.708,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.992114+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),o1,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.708,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,Pass@12 accuracy,,,cybersecurity-ctfs,Cybersecurity CTFs,,,2025-07-19T19:56:15.390045+00:00,benchmark_result,,,,True,,,,,,1910.0,,o1-mini,,,,0.287,,,openai,,,,,,,,0.287,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:15.390045+00:00,,,,,,False,,False,0.0,False,0.0,0.287,0.287,Unknown,,,,,Undisclosed,Poor (<60%),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",Cybersecurity CTFs,['safety'],text,False,1.0,en,"Cybersecurity Capture the Flag (CTF) benchmark for evaluating LLMs in offensive security challenges. Contains diverse cybersecurity tasks including cryptography, web exploitation, binary analysis, and forensics to assess AI capabilities in cybersecurity problem-solving.",0.287,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,"Diamond, 0-shot Chain of Thought",,,gpqa,GPQA,,,2025-07-19T19:56:11.765864+00:00,benchmark_result,,,,True,,,,,,356.0,,o1-mini,,,,0.6,,,openai,,,,,,,,0.6,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:11.765864+00:00,,,,,,False,,False,0.0,False,0.0,0.6,0.6,Unknown,,,,,Undisclosed,Fair (60-69%),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.6,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o1
,Pass@1 accuracy,,,humaneval,HumanEval,,,2025-07-19T19:56:12.692107+00:00,benchmark_result,,,,True,,,,,,812.0,,o1-mini,,,,0.924,,,openai,,,,,,,,0.924,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:12.692107+00:00,,,,,,False,,False,0.0,False,0.0,0.924,0.924,Unknown,,,,,Undisclosed,Excellent (90%+),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.924,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,0-shot Chain of Thought,,,math-500,MATH-500,,,2025-07-19T19:56:12.065288+00:00,benchmark_result,,,,True,,,,,,513.0,,o1-mini,,,,0.9,,,openai,,,,,,,,0.9,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:12.065288+00:00,,,,,,False,,False,0.0,False,0.0,0.9,0.9,Unknown,,,,,Undisclosed,Excellent (90%+),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.9,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,0-shot Chain of Thought,,,mmlu,MMLU,,,2025-07-19T19:56:11.327239+00:00,benchmark_result,,,,True,,,,,,123.0,,o1-mini,,,,0.852,,,openai,,,,,,,,0.852,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:11.327239+00:00,,,,,,False,,False,0.0,False,0.0,0.852,0.852,Unknown,,,,,Undisclosed,Very Good (80-89%),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.852,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,Evaluation on validation set,,,superglue,SuperGLUE,,,2025-07-19T19:56:15.385801+00:00,benchmark_result,,,,True,,,,,,1909.0,,o1-mini,,,,0.75,,,openai,,,,,,,,0.75,https://openai.com/index/openai-o1-mini-advancing-cost-efficient-reasoning/,,,,,,,,2025-07-19T19:56:15.385801+00:00,,,,,,False,,False,0.0,False,0.0,0.75,0.75,Unknown,,,,,Undisclosed,Good (70-79%),o1-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"o1-mini is a cost-efficient language model developed by OpenAI, designed for advanced reasoning tasks while minimizing computational resources.",SuperGLUE,"['general', 'language', 'reasoning']",text,False,1.0,en,"SuperGLUE is a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, improved resources, and a new public leaderboard. It includes 8 primary tasks: BoolQ (Boolean Questions), CB (CommitmentBank), COPA (Choice of Plausible Alternatives), MultiRC (Multi-Sentence Reading Comprehension), ReCoRD (Reading Comprehension with Commonsense Reasoning), RTE (Recognizing Textual Entailment), WiC (Word-in-Context), and WSC (Winograd Schema Challenge). The benchmark evaluates diverse language understanding capabilities including reading comprehension, commonsense reasoning, causal reasoning, coreference resolution, textual entailment, and word sense disambiguation across multiple domains.",0.75,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,pass@1,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.027037+00:00,benchmark_result,,,,True,,,,,,491.0,,o1-preview,,,,0.42,,,openai,,,,,,,,0.42,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:12.027037+00:00,,,,,,False,,False,0.0,False,0.0,0.42,0.42,Unknown,,,,,Undisclosed,Poor (<60%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.42,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.772534+00:00,benchmark_result,,,,True,,,,,,360.0,,o1-preview,,,,0.733,,,openai,,,,,,,,0.733,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:11.772534+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.733,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,Coding,,,livebench,LiveBench,,,2025-07-19T19:56:12.589687+00:00,benchmark_result,,,,True,,,,,,756.0,,o1-preview,,,,0.523,,,openai,,,,,,,,0.523,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:12.589687+00:00,,,,,,False,,False,0.0,False,0.0,0.523,0.523,Unknown,,,,,Undisclosed,Poor (<60%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.523,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,pass@1,,,math,MATH,,,2025-07-19T19:56:11.910412+00:00,benchmark_result,,,,True,,,,,,430.0,,o1-preview,,,,0.855,,,openai,,,,,,,,0.855,https://openai.com/index/learning-to-reason-with-llms,,,,,,,,2025-07-19T19:56:11.910412+00:00,,,,,,False,,False,0.0,False,0.0,0.855,0.855,Unknown,,,,,Undisclosed,Very Good (80-89%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.855,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,pass@1,,,mgsm,MGSM,,,2025-07-19T19:56:13.718867+00:00,benchmark_result,,,,True,,,,,,1300.0,,o1-preview,,,,0.908,,,openai,,,,,,,,0.908,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:13.718867+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.908,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,pass@1,,,mmlu,MMLU,,,2025-07-19T19:56:11.333269+00:00,benchmark_result,,,,True,,,,,,127.0,,o1-preview,,,,0.908,,,openai,,,,,,,,0.908,https://openai.com/index/learning-to-reason-with-llms,,,,,,,,2025-07-19T19:56:11.333269+00:00,,,,,,False,,False,0.0,False,0.0,0.908,0.908,Unknown,,,,,Undisclosed,Excellent (90%+),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.908,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o1
,Factuality,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.562695+00:00,benchmark_result,,,,True,,,,,,242.0,,o1-preview,,,,0.424,,,openai,,,,,,,,0.424,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:11.562695+00:00,,,,,,False,,False,0.0,False,0.0,0.424,0.424,Unknown,,,,,Undisclosed,Poor (<60%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.424,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,Verified,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.867753+00:00,benchmark_result,,,,True,,,,,,1362.0,,o1-preview,,,,0.413,,,openai,,,,,,,,0.413,https://openai.com/index/learning-to-reason-with-llms/,,,,,,,,2025-07-19T19:56:13.867753+00:00,,,,,,False,,False,0.0,False,0.0,0.413,0.413,Unknown,,,,,Undisclosed,Poor (<60%),o1-preview,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2024-09-12,2024.0,9.0,2024-09,Undisclosed,"A research preview model focused on mathematical and logical reasoning capabilities, demonstrating improved performance on tasks requiring step-by-step reasoning, mathematical problem-solving, and code generation. The model shows enhanced capabilities in formal reasoning while maintaining strong general capabilities.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.413,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o1
,Pass@1 accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.021363+00:00,benchmark_result,,,,True,,,,,,487.0,,o1-pro,,,,0.86,,,openai,,,,,,,,0.86,https://openai.com/index/introducing-chatgpt-pro/,,,,,,,,2025-07-19T19:56:12.021363+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),o1-pro,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"o1-pro is OpenAI's advanced language model optimized for complex reasoning and specialized professional tasks, offering enhanced capabilities while maintaining high efficiency.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.86,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o1
,"Diamond, Pass@1 accuracy",,,gpqa,GPQA,,,2025-07-19T19:56:11.762804+00:00,benchmark_result,,,,True,,,,,,354.0,,o1-pro,,,,0.79,,,openai,,,,,,,,0.79,https://openai.com/index/introducing-chatgpt-pro/,,,,,,,,2025-07-19T19:56:11.762804+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),o1-pro,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2024-12-17,2024.0,12.0,2024-12,Undisclosed,"o1-pro is OpenAI's advanced language model optimized for complex reasoning and specialized professional tasks, offering enhanced capabilities while maintaining high efficiency.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.79,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o1
,accuracy (whole),,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.380617+00:00,benchmark_result,,,,True,,,,,,666.0,,o3-2025-04-16,,,,0.813,,,openai,,,,,,,,0.813,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.380617+00:00,,,,,,False,,False,0.0,False,0.0,0.813,0.813,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.813,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,accuracy (no tools),,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.012342+00:00,benchmark_result,,,,True,,,,,,481.0,,o3-2025-04-16,,,,0.916,,,openai,,,,,,,,0.916,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.012342+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o3
,pass@1 (no tools),,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.475926+00:00,benchmark_result,,,,True,,,,,,705.0,,o3-2025-04-16,,,,0.864,,,openai,,,,,,,,0.864,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.475926+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.864,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,test set evaluation,,,arc-agi,ARC-AGI,,,2025-07-19T19:56:15.190370+00:00,benchmark_result,,,,True,,,,,,1832.0,,o3-2025-04-16,,,,0.88,,,openai,,,,,,,,0.88,https://www.youtube.com/live/SKBG1sqdyIU?si=lWccKHt8bnttuYta,,,,,,,,2025-07-19T19:56:15.190370+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",ARC-AGI,"['reasoning', 'vision', 'spatial_reasoning']",image,False,1.0,en,"The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a benchmark designed to test general intelligence and abstract reasoning capabilities through visual grid-based transformation tasks. Each task consists of 2-5 demonstration pairs showing input grids transformed into output grids according to underlying rules, with test-takers required to infer these rules and apply them to novel test inputs. The benchmark uses colored grids (up to 30x30) with 10 discrete colors/symbols, designed to measure human-like general fluid intelligence and skill-acquisition efficiency with minimal prior knowledge.",0.88,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),o3
,accuracy,,,arc-agi-v2,ARC-AGI v2,,,2025-07-19T19:56:13.925569+00:00,benchmark_result,,,,False,,,,,,1389.0,,o3-2025-04-16,,,,0.065,,,openai,,,,,,,,0.065,https://x.com/xai/status/1943158495588815072,,,,,,,,2025-07-19T19:56:13.925569+00:00,,,,,,False,,False,0.0,False,0.0,0.065,0.065,Unknown,,,,,Undisclosed,Poor (<60%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",ARC-AGI v2,"['reasoning', 'vision', 'spatial_reasoning']",multimodal,False,1.0,en,"ARC-AGI-2 is an upgraded benchmark for measuring abstract reasoning and problem-solving abilities in AI systems through visual grid transformation tasks. It evaluates fluid intelligence via input-output grid pairs (1x1 to 30x30) using colored cells (0-9), requiring models to identify underlying transformation rules from demonstration examples and apply them to test cases. Designed to be easy for humans but challenging for AI, focusing on core cognitive abilities like spatial reasoning, pattern recognition, and compositional generalization.",0.065,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),o3
,accuracy (with python + browsing),,,browsecomp,BrowseComp,,,2025-07-19T19:56:15.215315+00:00,benchmark_result,,,,True,,,,,,1842.0,,o3-2025-04-16,,,,0.497,,,openai,,,,,,,,0.497,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.215315+00:00,,,,,,False,,False,0.0,False,0.0,0.497,0.497,Unknown,,,,,Undisclosed,Poor (<60%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.497,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,OpenAI o3 with thinking mode - Scientific figure reasoning and interpretation.,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.193874+00:00,benchmark_result,,,,True,,,,,,1833.0,,o3-2025-04-16,,,,0.786,,,openai,,,,,,,,0.786,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:15.193874+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.786,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),o3
,OpenAI o3 with thinking mode enabled - Instruction-following in freeform writing.,,,collie,COLLIE,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2006.0,,o3-2025-04-16,,,,0.984,,,openai,,,,,,,,0.984,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.984,0.984,Unknown,,,,,Undisclosed,Excellent (90%+),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.984,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),o3
,OpenAI o3 with thinking mode - Multimodal spatial reasoning.,,,erqa,ERQA,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2012.0,,o3-2025-04-16,,,,0.64,,,openai,,,,,,,,0.64,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",ERQA,"['vision', 'reasoning', 'spatial_reasoning']",multimodal,False,1.0,en,"Embodied Reasoning Question Answering benchmark consisting of 400 multiple-choice visual questions across spatial reasoning, trajectory reasoning, action reasoning, state estimation, and multi-view reasoning for evaluating AI capabilities in physical world interactions",0.64,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),o3
,accuracy,,,frontiermath,FrontierMath,,,2025-07-19T19:56:15.181554+00:00,benchmark_result,,,,True,,,,,,1829.0,,o3-2025-04-16,,,,0.158,,,openai,,,,,,,,0.158,https://www.youtube.com/live/SKBG1sqdyIU?si=lWccKHt8bnttuYta,,,,,,,,2025-07-19T19:56:15.181554+00:00,,,,,,False,,False,0.0,False,0.0,0.158,0.158,Unknown,,,,,Undisclosed,Poor (<60%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.158,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,OpenAI o3 - Diamond thinking no tools,,,gpqa,GPQA,,,2025-07-19T19:56:11.750986+00:00,benchmark_result,,,,True,,,,,,347.0,,o3-2025-04-16,,,,0.833,,,openai,,,,,,,,0.833,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:11.750986+00:00,,,,,,False,,False,0.0,False,0.0,0.833,0.833,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.833,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,accuracy (no tools),,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.526631+00:00,benchmark_result,,,,True,,,,,,725.0,,o3-2025-04-16,,,,0.243,,,openai,,,,,,,,0.243,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.526631+00:00,,,,,,False,,False,0.0,False,0.0,0.243,0.243,Unknown,,,,,Undisclosed,Poor (<60%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.243,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.112692+00:00,benchmark_result,,,,True,,,,,,538.0,,o3-2025-04-16,,,,0.868,,,openai,,,,,,,,0.868,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.112692+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.868,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),o3
,OpenAI o3 with thinking mode - College-level visual problem-solving with multimodal reasoning.,,,mmmu,MMMU,,,2025-07-19T19:56:12.211231+00:00,benchmark_result,,,,True,,,,,,589.0,,o3-2025-04-16,,,,0.829,,,openai,,,,,,,,0.829,https://openai.com/index/gpt-5/,,,,,,,,2025-07-19T19:56:12.211231+00:00,,,,,,False,,False,0.0,False,0.0,0.829,0.829,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.829,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,OpenAI o3 with thinking mode - Graduate-level visual problem-solving with advanced multimodal reasoning.,,,mmmu-pro,MMMU-Pro,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2010.0,,o3-2025-04-16,,,,0.764,,,openai,,,,,,,,0.764,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.764,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),o3
,accuracy,,,scale-multichallenge,Scale MultiChallenge,,,2025-07-19T19:56:15.208929+00:00,benchmark_result,,,,True,,,,,,1840.0,,o3-2025-04-16,,,,0.604,,,openai,,,,,,,,0.604,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.208929+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Scale MultiChallenge,"['reasoning', 'communication', 'general']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark developed by Scale AI that evaluates large language models on four challenging conversation categories: instruction retention, inference memory of user information, reliable versioned editing, and self-coherence. Each challenge requires accurate instruction-following, context allocation, and in-context reasoning. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge.",0.604,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,accuracy,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.851256+00:00,benchmark_result,,,,True,,,,,,1354.0,,o3-2025-04-16,,,,0.691,,,openai,,,,,,,,0.691,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:13.851256+00:00,,,,,,False,,False,0.0,False,0.0,0.691,0.691,Unknown,,,,,Undisclosed,Fair (60-69%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.691,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,accuracy (avg Airline/Retail),,,tau-bench,Tau-bench,,,2025-07-19T19:56:15.221470+00:00,benchmark_result,,,,True,,,,,,1844.0,,o3-2025-04-16,,,,0.63,,,openai,,,,,,,,0.63,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.221470+00:00,,,,,,False,,False,0.0,False,0.0,0.63,0.63,Unknown,,,,,Undisclosed,Fair (60-69%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Tau-bench,"['general', 'reasoning']",text,False,1.0,en,τ-bench: A benchmark for tool-agent-user interaction in real-world domains. Tests language agents' ability to interact with users and follow domain-specific rules through dynamic conversations using API tools and policy guidelines across retail and airline domains. Evaluates consistency and reliability of agent behavior over multiple trials.,0.63,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,OpenAI o3 with thinking mode - Function calling benchmark (airline domain).,,,tau2-airline,Tau2 airline,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2007.0,,o3-2025-04-16,,,,0.648,,,openai,,,,,,,,0.648,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.648,0.648,Unknown,,,,,Undisclosed,Fair (60-69%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.648,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,OpenAI o3 with thinking mode - Function calling benchmark (retail domain).,,,tau2-retail,Tau2 retail,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2008.0,,o3-2025-04-16,,,,0.802,,,openai,,,,,,,,0.802,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.802,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,OpenAI o3 with thinking mode - Function calling benchmark (telecom domain).,,,tau2-telecom,Tau2 telecom,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2009.0,,o3-2025-04-16,,,,0.582,,,openai,,,,,,,,0.582,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.582,0.582,Unknown,,,,,Undisclosed,Poor (<60%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.582,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,OpenAI o3 with thinking mode - Video-based multimodal reasoning (max frame 256).,,,videommmu,VideoMMMU,,,2025-07-24T12:00:00.000000+00:00,benchmark_result,,,,True,,,,,,2011.0,,o3-2025-04-16,,,,0.833,,,openai,,,,,,,,0.833,https://openai.com/index/gpt-5/,,,,,,,,2025-07-24T12:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.833,0.833,Unknown,,,,,Undisclosed,Very Good (80-89%),o3,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"OpenAI's most powerful reasoning model. o3 is a well-rounded and powerful model across domains. It sets a new standard for math, science, coding, and visual reasoning tasks. It also excels at technical writing and instruction-following. Use it to think through multi-step problems that involve analysis across text, code, and images.",VideoMMMU,"['multimodal', 'vision', 'reasoning']",multimodal,False,1.0,en,"Video-MMMU evaluates Large Multimodal Models' ability to acquire knowledge from expert-level professional videos across six disciplines through three cognitive stages: perception, comprehension, and adaptation. Contains 300 videos and 900 human-annotated questions spanning Art, Business, Science, Medicine, Humanities, and Engineering.",0.833,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),o3
,benchmark score,,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.387419+00:00,benchmark_result,,,,True,,,,,,670.0,,o3-mini,,,,0.667,,,openai,,,,,,,,0.667,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.387419+00:00,,,,,,False,,False,0.0,False,0.0,0.667,0.667,Unknown,,,,,Undisclosed,Fair (60-69%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.667,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,benchmark score,,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.806560+00:00,benchmark_result,,,,True,,,,,,1334.0,,o3-mini,,,,0.604,,,openai,,,,,,,,0.604,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:13.806560+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.604,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o3
,test set evaluation,,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.018382+00:00,benchmark_result,,,,True,,,,,,485.0,,o3-mini,,,,0.873,,,openai,,,,,,,,0.873,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:12.018382+00:00,,,,,,False,,False,0.0,False,0.0,0.873,0.873,Unknown,,,,,Undisclosed,Very Good (80-89%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.873,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,benchmark score,,,collie,COLLIE,,,2025-07-19T19:56:15.259314+00:00,benchmark_result,,,,True,,,,,,1859.0,,o3-mini,,,,0.987,,,openai,,,,,,,,0.987,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.259314+00:00,,,,,,False,,False,0.0,False,0.0,0.987,0.987,Unknown,,,,,Undisclosed,Excellent (90%+),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",COLLIE,"['language', 'reasoning', 'writing']",text,False,1.0,en,"COLLIE is a grammar-based framework for systematic construction of constrained text generation tasks. It allows specification of rich, compositional constraints across diverse generation levels and modeling challenges including language understanding, logical reasoning, and semantic planning. The COLLIE-v1 dataset contains 2,080 instances across 13 constraint structures.",0.987,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),o3
,benchmark score,,,complexfuncbench,ComplexFuncBench,,,2025-07-19T19:56:15.344047+00:00,benchmark_result,,,,True,,,,,,1894.0,,o3-mini,,,,0.176,,,openai,,,,,,,,0.176,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.344047+00:00,,,,,,False,,False,0.0,False,0.0,0.176,0.176,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",ComplexFuncBench,"['long_context', 'reasoning']",text,False,1.0,en,"ComplexFuncBench is a benchmark designed to evaluate large language models' capabilities in handling complex function calling scenarios. It encompasses multi-step and constrained function calling tasks that require long-parameter filling, parameter value reasoning, and managing contexts up to 128k tokens. The benchmark includes 1,000 samples across five real-world scenarios.",0.176,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,pass @ 1,,,frontiermath,FrontierMath,,,2025-07-19T19:56:15.183728+00:00,benchmark_result,,,,True,,,,,,1830.0,,o3-mini,,,,0.092,,,openai,,,,,,,,0.092,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:15.183728+00:00,,,,,,False,,False,0.0,False,0.0,0.092,0.092,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",FrontierMath,"['math', 'reasoning']",text,False,1.0,en,"A benchmark of hundreds of original, exceptionally challenging mathematics problems crafted and vetted by expert mathematicians, covering most major branches of modern mathematics from number theory and real analysis to algebraic geometry and category theory.",0.092,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.758026+00:00,benchmark_result,,,,True,,,,,,351.0,,o3-mini,,,,0.772,,,openai,,,,,,,,0.772,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:11.758026+00:00,,,,,,False,,False,0.0,False,0.0,0.772,0.772,Unknown,,,,,Undisclosed,Good (70-79%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.772,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o3
,benchmark score,,,graphwalks-bfs-<128k,Graphwalks BFS <128k,,,2025-07-19T19:56:15.368369+00:00,benchmark_result,,,,True,,,,,,1904.0,,o3-mini,,,,0.51,,,openai,,,,,,,,0.51,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.368369+00:00,,,,,,False,,False,0.0,False,0.0,0.51,0.51,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,o3
,benchmark score,,,graphwalks-parents-<128k,Graphwalks parents <128k,,,2025-07-19T19:56:15.310391+00:00,benchmark_result,,,,True,,,,,,1880.0,,o3-mini,,,,0.583,,,openai,,,,,,,,0.583,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.310391+00:00,,,,,,False,,False,0.0,False,0.0,0.583,0.583,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",,,,,,,,,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,No Score,o3
,benchmark score,,,ifeval,IFEval,,,2025-07-19T19:56:12.302770+00:00,benchmark_result,,,,True,,,,,,634.0,,o3-mini,,,,0.939,,,openai,,,,,,,,0.939,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.302770+00:00,,,,,,False,,False,0.0,False,0.0,0.939,0.939,Unknown,,,,,Undisclosed,Excellent (90%+),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.939,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o3
,benchmark score,,,internal-api-instruction-following-(hard),Internal API instruction following (hard),,,2025-07-19T19:56:15.228737+00:00,benchmark_result,,,,True,,,,,,1847.0,,o3-mini,,,,0.5,,,openai,,,,,,,,0.5,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.228737+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Internal API instruction following (hard),['general'],text,False,1.0,en,Internal API instruction following (hard) benchmark - specific documentation not found in official sources,0.5,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,o3-mini high,,,livebench,LiveBench,,,2025-07-19T19:56:12.585789+00:00,benchmark_result,,,,True,,,,,,754.0,,o3-mini,,,,0.846,,,openai,,,,,,,,0.846,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:12.585789+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.846,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,o3-mini high,,,math,MATH,,,2025-07-19T19:56:11.901889+00:00,benchmark_result,,,,True,,,,,,426.0,,o3-mini,,,,0.979,,,openai,,,,,,,,0.979,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:11.901889+00:00,,,,,,False,,False,0.0,False,0.0,0.979,0.979,Unknown,,,,,Undisclosed,Excellent (90%+),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.979,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),o3
,o3-mini high,,,mgsm,MGSM,,,2025-07-19T19:56:13.712633+00:00,benchmark_result,,,,True,,,,,,1296.0,,o3-mini,,,,0.92,,,openai,,,,,,,,0.92,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:13.712633+00:00,,,,,,False,,False,0.0,False,0.0,0.92,0.92,Unknown,,,,,Undisclosed,Excellent (90%+),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.92,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o3
,o3-mini high,,,mmlu,MMLU,,,2025-07-19T19:56:11.320589+00:00,benchmark_result,,,,True,,,,,,119.0,,o3-mini,,,,0.869,,,openai,,,,,,,,0.869,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:11.320589+00:00,,,,,,False,,False,0.0,False,0.0,0.869,0.869,Unknown,,,,,Undisclosed,Very Good (80-89%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.869,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,benchmark score,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.646496+00:00,benchmark_result,,,,True,,,,,,1652.0,,o3-mini,,,,0.795,,,openai,,,,,,,,0.795,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.646496+00:00,,,,,,False,,False,0.0,False,0.0,0.795,0.795,Unknown,,,,,Undisclosed,Good (70-79%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.795,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o3
,benchmark score,,,multichallenge,MultiChallenge,,,2025-07-19T19:56:12.560158+00:00,benchmark_result,,,,True,,,,,,742.0,,o3-mini,,,,0.399,,,openai,,,,,,,,0.399,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:12.560158+00:00,,,,,,False,,False,0.0,False,0.0,0.399,0.399,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Multi-Challenge,"['communication', 'reasoning']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key categories: instruction retention (maintaining instructions throughout conversations), inference memory (recalling and connecting details from previous turns), reliable versioned editing (adapting to evolving instructions during collaborative editing), and self-coherence (avoiding contradictions in responses). The benchmark evaluates models on sustained, contextually complex dialogues across diverse topics including travel planning, technical documentation, and professional communication.",0.399,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,benchmark score,,,multichallenge-(o3-mini-grader),MultiChallenge (o3-mini grader),,,2025-07-19T19:56:15.243415+00:00,benchmark_result,,,,True,,,,,,1853.0,,o3-mini,,,,0.502,,,openai,,,,,,,,0.502,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.243415+00:00,,,,,,False,,False,0.0,False,0.0,0.502,0.502,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",MultiChallenge (o3-mini grader),"['reasoning', 'language']",text,False,1.0,en,"A realistic multi-turn conversation evaluation benchmark that challenges frontier LLMs across four key areas: instruction retention, inference memory, reliable versioned editing, and self-coherence. Despite near-perfect scores on existing benchmarks, frontier models achieve less than 50% accuracy on MultiChallenge.",0.502,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,benchmark score,,,multilingual-mmlu,Multilingual MMLU,,,2025-07-19T19:56:14.143822+00:00,benchmark_result,,,,True,,,,,,1474.0,,o3-mini,,,,0.807,,,openai,,,,,,,,0.807,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.143822+00:00,,,,,,False,,False,0.0,False,0.0,0.807,0.807,Unknown,,,,,Undisclosed,Very Good (80-89%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",Multilingual MMLU,"['general', 'reasoning', 'language']",text,True,1.0,en,"MMLU-ProX is a comprehensive multilingual benchmark covering 29 typologically diverse languages, building upon MMLU-Pro. Each language version consists of 11,829 identical questions enabling direct cross-linguistic comparisons. The benchmark evaluates large language models' reasoning capabilities across linguistic and cultural boundaries through challenging, reasoning-focused questions with 10 answer choices.",0.807,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o3
,benchmark score,,,openai-mrcr:-2-needle-128k,OpenAI-MRCR: 2 needle 128k,,,2025-07-19T19:56:15.274261+00:00,benchmark_result,,,,True,,,,,,1865.0,,o3-mini,,,,0.187,,,openai,,,,,,,,0.187,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.274261+00:00,,,,,,False,,False,0.0,False,0.0,0.187,0.187,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",OpenAI-MRCR: 2 needle 128k,"['long_context', 'reasoning']",text,False,1.0,en,"Multi-round Co-reference Resolution (MRCR) benchmark for evaluating an LLM's ability to distinguish between multiple needles hidden in long context. Models are given a long, multi-turn synthetic conversation and must retrieve a specific instance of a repeated request, requiring reasoning and disambiguation skills beyond simple retrieval.",0.187,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,accuracy,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.554563+00:00,benchmark_result,,,,True,,,,,,238.0,,o3-mini,,,,0.15,,,openai,,,,,,,,0.15,https://openai.com/index/introducing-gpt-4-5/,,,,,,,,2025-07-19T19:56:11.554563+00:00,,,,,,False,,False,0.0,False,0.0,0.15,0.15,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.15,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,verified,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.856039+00:00,benchmark_result,,,,True,,,,,,1357.0,,o3-mini,,,,0.493,,,openai,,,,,,,,0.493,https://openai.com/index/openai-o3-mini/,,,,,,,,2025-07-19T19:56:13.856039+00:00,,,,,,False,,False,0.0,False,0.0,0.493,0.493,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.493,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,percentage score,,,swe-lancer,SWE-Lancer,,,2025-07-19T19:56:15.355089+00:00,benchmark_result,,,,True,,,,,,1898.0,,o3-mini,,,,0.18,,,openai,,,,,,,,0.18,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.355089+00:00,,,,,,False,,False,0.0,False,0.0,0.18,0.18,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",SWE-Lancer,"['reasoning', 'code']",text,False,1.0,en,"A benchmark for evaluating large language models on real-world freelance software engineering tasks from Upwork. Contains over 1,400 tasks valued at $1 million USD total, ranging from $50 bug fixes to $32,000 feature implementations. Includes both independent engineering tasks graded via end-to-end tests and managerial tasks assessed against original engineering managers' choices.",0.18,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,percentage score,,,swe-lancer-(ic-diamond-subset),SWE-Lancer (IC-Diamond subset),,,2025-07-19T19:56:15.362026+00:00,benchmark_result,,,,True,,,,,,1901.0,,o3-mini,,,,0.074,,,openai,,,,,,,,0.074,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.362026+00:00,,,,,,False,,False,0.0,False,0.0,0.074,0.074,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",SWE-Lancer (IC-Diamond subset),"['reasoning', 'code']",text,False,1.0,en,"SWE-Lancer (IC-Diamond subset) is a benchmark of real-world freelance software engineering tasks from Upwork, ranging from $50 bug fixes to $32,000 feature implementations. It evaluates AI models on independent engineering tasks using end-to-end tests triple-verified by experienced software engineers, and includes managerial tasks where models choose between technical implementation proposals.",0.074,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,benchmark score,,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.013372+00:00,benchmark_result,,,,True,,,,,,1779.0,,o3-mini,,,,0.324,,,openai,,,,,,,,0.324,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:15.013372+00:00,,,,,,False,,False,0.0,False,0.0,0.324,0.324,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.324,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,benchmark score,,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.984653+00:00,benchmark_result,,,,True,,,,,,1765.0,,o3-mini,,,,0.576,,,openai,,,,,,,,0.576,https://openai.com/index/gpt-4-1/,,,,,,,,2025-07-19T19:56:14.984653+00:00,,,,,,False,,False,0.0,False,0.0,0.576,0.576,Unknown,,,,,Undisclosed,Poor (<60%),o3-mini,openai,OpenAI,,0.0,False,0.0,False,False,proprietary,Proprietary,2025-01-30,2025.0,1.0,2025-01,Undisclosed,"A smaller variant of O3, expected to offer enhanced multimodal capabilities, improved reasoning, and more efficient resource utilization compared to previous models while maintaining strong performance on core tasks.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.576,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o3
,"accuracy (whole, o4-mini-high)",,,aider-polyglot,Aider-Polyglot,,,2025-07-19T19:56:12.384371+00:00,benchmark_result,,,,True,,,,,,668.0,,o4-mini,,,,0.689,,,openai,,,,,,,,0.689,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.384371+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.689,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o4
,"accuracy (diff, o4-mini-high)",,,aider-polyglot-edit,Aider-Polyglot Edit,,,2025-07-19T19:56:13.803065+00:00,benchmark_result,,,,True,,,,,,1332.0,,o4-mini,,,,0.582,,,openai,,,,,,,,0.582,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:13.803065+00:00,,,,,,False,,False,0.0,False,0.0,0.582,0.582,Unknown,,,,,Undisclosed,Poor (<60%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",Aider-Polyglot Edit,"['general', 'code']",text,False,1.0,en,"A challenging multi-language coding benchmark that evaluates models' code editing abilities across C++, Go, Java, JavaScript, Python, and Rust. Contains 225 of Exercism's most difficult programming problems, selected as problems that were solved by 3 or fewer out of 7 top coding models. The benchmark focuses on code editing tasks and measures both correctness of solutions and proper edit format usage. Designed to re-calibrate evaluation scales so top models score between 5-50%.",0.582,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o4
,accuracy (no tools),,,aime-2024,AIME 2024,,,2025-07-19T19:56:12.015345+00:00,benchmark_result,,,,True,,,,,,483.0,,o4-mini,,,,0.934,,,openai,,,,,,,,0.934,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.015345+00:00,,,,,,False,,False,0.0,False,0.0,0.934,0.934,Unknown,,,,,Undisclosed,Excellent (90%+),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.934,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o4
,accuracy (no tools),,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.477657+00:00,benchmark_result,,,,True,,,,,,706.0,,o4-mini,,,,0.927,,,openai,,,,,,,,0.927,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.477657+00:00,,,,,,False,,False,0.0,False,0.0,0.927,0.927,Unknown,,,,,Undisclosed,Excellent (90%+),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.927,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),o4
,accuracy (with python + browsing),,,browsecomp,BrowseComp,,,2025-07-19T19:56:15.217475+00:00,benchmark_result,,,,True,,,,,,1843.0,,o4-mini,,,,0.515,,,openai,,,,,,,,0.515,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.217475+00:00,,,,,,False,,False,0.0,False,0.0,0.515,0.515,Unknown,,,,,Undisclosed,Poor (<60%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",BrowseComp,"['reasoning', 'search']",text,False,1.0,en,"BrowseComp is a benchmark comprising 1,266 questions that challenge AI agents to persistently navigate the internet in search of hard-to-find, entangled information. The benchmark measures agents' ability to exercise persistence in information gathering, demonstrate creativity in web navigation, and find concise, verifiable answers. Despite the difficulty of the questions, BrowseComp is simple and easy-to-use, as predicted answers are short and easily verifiable against reference answers.",0.515,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o4
,accuracy,,,charxiv-r,CharXiv-R,,,2025-07-19T19:56:15.197036+00:00,benchmark_result,,,,True,,,,,,1835.0,,o4-mini,,,,0.72,,,openai,,,,,,,,0.72,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.197036+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",CharXiv-R,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"CharXiv-R is the reasoning component of the CharXiv benchmark, focusing on complex reasoning questions that require synthesizing information across visual chart elements. It evaluates multimodal large language models on their ability to understand and reason about scientific charts from arXiv papers through various reasoning tasks.",0.72,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),o4
,diamond accuracy (no tools),,,gpqa,GPQA,,,2025-07-19T19:56:11.754610+00:00,benchmark_result,,,,True,,,,,,349.0,,o4-mini,,,,0.814,,,openai,,,,,,,,0.814,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:11.754610+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.814,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o4
,accuracy (no tools),,,humanity's-last-exam,Humanity's Last Exam,,,2025-07-19T19:56:12.528160+00:00,benchmark_result,,,,True,,,,,,726.0,,o4-mini,,,,0.147,,,openai,,,,,,,,0.147,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.528160+00:00,,,,,,False,,False,0.0,False,0.0,0.147,0.147,Unknown,,,,,Undisclosed,Poor (<60%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.147,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o4
,accuracy,,,mathvista,MathVista,,,2025-07-19T19:56:12.115868+00:00,benchmark_result,,,,True,,,,,,540.0,,o4-mini,,,,0.843,,,openai,,,,,,,,0.843,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.115868+00:00,,,,,,False,,False,0.0,False,0.0,0.843,0.843,Unknown,,,,,Undisclosed,Very Good (80-89%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.843,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),o4
,accuracy,,,mmmu,MMMU,,,2025-07-19T19:56:12.218993+00:00,benchmark_result,,,,True,,,,,,591.0,,o4-mini,,,,0.816,,,openai,,,,,,,,0.816,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:12.218993+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.816,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),o4
,accuracy,,,scale-multichallenge,Scale MultiChallenge,,,2025-07-19T19:56:15.211372+00:00,benchmark_result,,,,True,,,,,,1841.0,,o4-mini,,,,0.43,,,openai,,,,,,,,0.43,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.211372+00:00,,,,,,False,,False,0.0,False,0.0,0.43,0.43,Unknown,,,,,Undisclosed,Poor (<60%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",Scale MultiChallenge,"['reasoning', 'communication', 'general']",text,False,1.0,en,"MultiChallenge is a realistic multi-turn conversation evaluation benchmark developed by Scale AI that evaluates large language models on four challenging conversation categories: instruction retention, inference memory of user information, reliable versioned editing, and self-coherence. Each challenge requires accurate instruction-following, context allocation, and in-context reasoning. Despite achieving near-perfect scores on existing multi-turn evaluation benchmarks, all frontier models have less than 50% accuracy on MultiChallenge.",0.43,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o4
,accuracy,,,swe-bench-verified,SWE-Bench Verified,,,2025-07-19T19:56:13.854236+00:00,benchmark_result,,,,True,,,,,,1356.0,,o4-mini,,,,0.681,,,openai,,,,,,,,0.681,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:13.854236+00:00,,,,,,False,,False,0.0,False,0.0,0.681,0.681,Unknown,,,,,Undisclosed,Fair (60-69%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",SWE-Bench Verified,"['reasoning', 'frontend_development', 'code']",text,False,1.0,en,"A verified subset of 500 software engineering problems from real GitHub issues, validated by human annotators for evaluating language models' ability to resolve real-world coding issues by generating patches for Python codebases.",0.681,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),o4
,accuracy (o4-mini-high),,,tau-bench-airline,TAU-bench Airline,,,2025-07-19T19:56:15.009611+00:00,benchmark_result,,,,True,,,,,,1777.0,,o4-mini,,,,0.492,,,openai,,,,,,,,0.492,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:15.009611+00:00,,,,,,False,,False,0.0,False,0.0,0.492,0.492,Unknown,,,,,Undisclosed,Poor (<60%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.492,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),o4
,accuracy (o4-mini-high),,,tau-bench-retail,TAU-bench Retail,,,2025-07-19T19:56:14.980200+00:00,benchmark_result,,,,True,,,,,,1763.0,,o4-mini,,,,0.718,,,openai,,,,,,,,0.718,https://openai.com/index/introducing-o3-and-o4-mini/,,,,,,,,2025-07-19T19:56:14.980200+00:00,,,,,,False,,False,0.0,False,0.0,0.718,0.718,Unknown,,,,,Undisclosed,Good (70-79%),o4-mini,openai,OpenAI,,0.0,False,0.0,False,True,proprietary,Proprietary,2025-04-16,2025.0,4.0,2025-04,Undisclosed,"o4-mini is OpenAI's latest small o-series model, optimized for fast, effective reasoning with exceptionally efficient performance in coding and visual tasks. It is faster and more affordable than o3.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.718,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),o4
,10-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.111398+00:00,benchmark_result,,,,True,,,,,,13.0,,phi-3.5-mini-instruct,,,,0.846,,,microsoft,,,,,,,,0.846,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.111398+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.846,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.088299+00:00,benchmark_result,,,,True,,,,,,1448.0,,phi-3.5-mini-instruct,,,,0.37,,,microsoft,,,,,,,,0.37,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.088299+00:00,,,,,,False,,False,0.0,False,0.0,0.37,0.37,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.37,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,0-shot chain-of-thought,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.245591+00:00,benchmark_result,,,,True,,,,,,1078.0,,phi-3.5-mini-instruct,,,,0.69,,,microsoft,,,,,,,,0.69,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.245591+00:00,,,,,,False,,False,0.0,False,0.0,0.69,0.69,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.69,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,2-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.132882+00:00,benchmark_result,,,,True,,,,,,1025.0,,phi-3.5-mini-instruct,,,,0.78,,,microsoft,,,,,,,,0.78,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.132882+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.78,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,govreport,GovReport,,,2025-07-19T19:56:14.222697+00:00,benchmark_result,,,,True,,,,,,1504.0,,phi-3.5-mini-instruct,,,,0.259,,,microsoft,,,,,,,,0.259,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.222697+00:00,,,,,,False,,False,0.0,False,0.0,0.259,0.259,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",GovReport,"['summarization', 'long_context']",text,False,1.0,en,"A long document summarization dataset consisting of reports from government research agencies including Congressional Research Service and U.S. Government Accountability Office, with significantly longer documents and summaries than other datasets.",0.259,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,0-shot chain-of-thought,,,gpqa,GPQA,,,2025-07-19T19:56:11.651230+00:00,benchmark_result,,,,True,,,,,,285.0,,phi-3.5-mini-instruct,,,,0.304,,,microsoft,,,,,,,,0.304,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.651230+00:00,,,,,,False,,False,0.0,False,0.0,0.304,0.304,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.304,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,8-shot chain-of-thought,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.070240+00:00,benchmark_result,,,,True,,,,,,987.0,,phi-3.5-mini-instruct,,,,0.862,,,microsoft,,,,,,,,0.862,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.070240+00:00,,,,,,False,,False,0.0,False,0.0,0.862,0.862,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.862,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.173447+00:00,benchmark_result,,,,True,,,,,,43.0,,phi-3.5-mini-instruct,,,,0.694,,,microsoft,,,,,,,,0.694,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.173447+00:00,,,,,,False,,False,0.0,False,0.0,0.694,0.694,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.694,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.631199+00:00,benchmark_result,,,,True,,,,,,777.0,,phi-3.5-mini-instruct,,,,0.628,,,microsoft,,,,,,,,0.628,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:12.631199+00:00,,,,,,False,,False,0.0,False,0.0,0.628,0.628,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.628,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,0-shot chain-of-thought,,,math,MATH,,,2025-07-19T19:56:11.842901+00:00,benchmark_result,,,,True,,,,,,392.0,,phi-3.5-mini-instruct,,,,0.485,,,microsoft,,,,,,,,0.485,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.842901+00:00,,,,,,False,,False,0.0,False,0.0,0.485,0.485,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.485,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,3-shot,,,mbpp,MBPP,,,2025-07-19T19:56:13.481045+00:00,benchmark_result,,,,True,,,,,,1178.0,,phi-3.5-mini-instruct,,,,0.696,,,microsoft,,,,,,,,0.696,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.481045+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.006959999999999999,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,mega-mlqa,MEGA MLQA,,,2025-07-19T19:56:14.191909+00:00,benchmark_result,,,,True,,,,,,1494.0,,phi-3.5-mini-instruct,,,,0.617,,,microsoft,,,,,,,,0.617,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.191909+00:00,,,,,,False,,False,0.0,False,0.0,0.617,0.617,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MEGA MLQA,"['language', 'reasoning']",text,True,1.0,en,"MLQA as part of the MEGA (Multilingual Evaluation of Generative AI) benchmark suite. A multi-way aligned extractive QA evaluation benchmark for cross-lingual question answering across 7 languages (English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese) with over 12K QA instances in English and 5K in each other language.",0.617,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-tydi-qa,MEGA TyDi QA,,,2025-07-19T19:56:14.197084+00:00,benchmark_result,,,,True,,,,,,1496.0,,phi-3.5-mini-instruct,,,,0.622,,,microsoft,,,,,,,,0.622,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.197084+00:00,,,,,,False,,False,0.0,False,0.0,0.622,0.622,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MEGA TyDi QA,"['language', 'reasoning']",text,True,1.0,en,"TyDi QA as part of the MEGA benchmark suite. A question answering dataset covering 11 typologically diverse languages (Arabic, Bengali, English, Finnish, Indonesian, Japanese, Korean, Russian, Swahili, Telugu, and Thai) with 204K question-answer pairs. Features realistic information-seeking questions written by people who want to know the answer but don't know it yet.",0.622,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-udpos,MEGA UDPOS,,,2025-07-19T19:56:14.203616+00:00,benchmark_result,,,,True,,,,,,1498.0,,phi-3.5-mini-instruct,,,,0.465,,,microsoft,,,,,,,,0.465,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.203616+00:00,,,,,,False,,False,0.0,False,0.0,0.465,0.465,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MEGA UDPOS,['language'],text,True,1.0,en,"Universal Dependencies POS tagging as part of the MEGA benchmark suite. A multilingual part-of-speech tagging dataset based on Universal Dependencies treebanks, utilizing the universal POS tag set of 17 tags across 38 diverse languages from different language families. Used for evaluating multilingual POS tagging systems.",0.465,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,mega-xcopa,MEGA XCOPA,,,2025-07-19T19:56:14.210364+00:00,benchmark_result,,,,True,,,,,,1500.0,,phi-3.5-mini-instruct,,,,0.631,,,microsoft,,,,,,,,0.631,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.210364+00:00,,,,,,False,,False,0.0,False,0.0,0.631,0.631,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MEGA XCOPA,"['reasoning', 'language']",text,True,1.0,en,"XCOPA (Cross-lingual Choice of Plausible Alternatives) as part of the MEGA benchmark suite. A typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, including resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. Requires models to select which choice is the effect or cause of a given premise.",0.631,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-xstorycloze,MEGA XStoryCloze,,,2025-07-19T19:56:14.217597+00:00,benchmark_result,,,,True,,,,,,1502.0,,phi-3.5-mini-instruct,,,,0.735,,,microsoft,,,,,,,,0.735,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.217597+00:00,,,,,,False,,False,0.0,False,0.0,0.735,0.735,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MEGA XStoryCloze,"['reasoning', 'language']",text,True,1.0,en,"XStoryCloze as part of the MEGA benchmark suite. A cross-lingual story completion task that consists of professionally translated versions of the English StoryCloze dataset to 10 non-English languages. Requires models to predict the correct ending for a given four-sentence story, evaluating commonsense reasoning and narrative understanding.",0.735,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,0-shot chain-of-thought,,,mgsm,MGSM,,,2025-07-19T19:56:13.687534+00:00,benchmark_result,,,,True,,,,,,1282.0,,phi-3.5-mini-instruct,,,,0.479,,,microsoft,,,,,,,,0.479,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.687534+00:00,,,,,,False,,False,0.0,False,0.0,0.479,0.479,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.479,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,5-shot evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.240966+00:00,benchmark_result,,,,True,,,,,,75.0,,phi-3.5-mini-instruct,,,,0.69,,,microsoft,,,,,,,,0.69,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.240966+00:00,,,,,,False,,False,0.0,False,0.0,0.69,0.69,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.69,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,0-shot chain-of-thought,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.447960+00:00,benchmark_result,,,,True,,,,,,180.0,,phi-3.5-mini-instruct,,,,0.474,,,microsoft,,,,,,,,0.474,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.450171+00:00,,,,,,False,,False,0.0,False,0.0,0.474,0.474,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.474,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,5-shot evaluation,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.148935+00:00,benchmark_result,,,,True,,,,,,1476.0,,phi-3.5-mini-instruct,,,,0.554,,,microsoft,,,,,,,,0.554,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.148935+00:00,,,,,,False,,False,0.0,False,0.0,0.554,0.554,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.554,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,10-shot,,,openbookqa,OpenBookQA,,,2025-07-19T19:56:14.136354+00:00,benchmark_result,,,,True,,,,,,1471.0,,phi-3.5-mini-instruct,,,,0.792,,,microsoft,,,,,,,,0.792,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.136354+00:00,,,,,,False,,False,0.0,False,0.0,0.792,0.792,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",OpenBookQA,"['reasoning', 'general']",text,False,1.0,en,"OpenBookQA is a question-answering dataset modeled after open book exams for assessing human understanding. It contains 5,957 multiple-choice elementary-level science questions that probe understanding of 1,326 core science facts and their application to novel situations, requiring combination of open book facts with broad common knowledge through multi-hop reasoning.",0.792,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,5-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.154444+00:00,benchmark_result,,,,True,,,,,,1034.0,,phi-3.5-mini-instruct,,,,0.81,,,microsoft,,,,,,,,0.81,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.154444+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.81,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,qasper,Qasper,,,2025-07-19T19:56:14.173290+00:00,benchmark_result,,,,True,,,,,,1488.0,,phi-3.5-mini-instruct,,,,0.419,,,microsoft,,,,,,,,0.419,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.173290+00:00,,,,,,False,,False,0.0,False,0.0,0.419,0.419,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",Qasper,"['reasoning', 'long_context']",text,False,1.0,en,"QASPER is a dataset of 5,049 information-seeking questions and answers anchored in 1,585 NLP research papers. Questions are written by NLP practitioners who read only titles and abstracts, while answers require understanding the full paper text and provide supporting evidence. The dataset challenges models with complex reasoning across document sections for academic document question answering. Each question seeks information present in the full text and is answered by a separate set of NLP practitioners who also provide supporting evidence to answers.",0.419,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,qmsum,QMSum,,,2025-07-19T19:56:14.228389+00:00,benchmark_result,,,,True,,,,,,1506.0,,phi-3.5-mini-instruct,,,,0.213,,,microsoft,,,,,,,,0.213,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.228389+00:00,,,,,,False,,False,0.0,False,0.0,0.213,0.213,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",QMSum,"['summarization', 'long_context']",text,False,1.0,en,"QMSum is a benchmark for query-based multi-domain meeting summarization consisting of 1,808 query-summary pairs over 232 meetings across academic, product, and committee domains. The dataset enables models to select and summarize relevant spans of meetings in response to specific queries. Published at NAACL 2021, QMSum presents significant challenges in long meeting summarization where models must identify and summarize relevant content based on user queries.",0.213,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,average,,,repoqa,RepoQA,,,2025-07-19T19:56:14.186426+00:00,benchmark_result,,,,True,,,,,,1492.0,,phi-3.5-mini-instruct,,,,0.77,,,microsoft,,,,,,,,0.77,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.186426+00:00,,,,,,False,,False,0.0,False,0.0,0.77,0.77,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",RepoQA,"['long_context', 'reasoning', 'code']",text,True,1.0,en,"RepoQA is a benchmark for evaluating long-context code understanding capabilities of Large Language Models through the Searching Needle Function (SNF) task, where LLMs must locate specific functions in code repositories using natural language descriptions. The benchmark contains 500 code search tasks spanning 50 repositories across 5 modern programming languages (Python, Java, TypeScript, C++, and Rust), tested on 26 general and code-specific LLMs to assess their ability to comprehend and navigate code repositories.",0.77,False,True,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,128k,,,ruler,RULER,,,2025-07-19T19:56:14.179307+00:00,benchmark_result,,,,True,,,,,,1490.0,,phi-3.5-mini-instruct,,,,0.841,,,microsoft,,,,,,,,0.841,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.179307+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",RULER,"['long_context', 'reasoning']",text,False,1.0,en,"RULER (What's the Real Context Size of Your Long-Context Language Models?) is a synthetic benchmark designed to comprehensively evaluate the long-context capabilities of language models. It expands on needle-in-a-haystack (NIAH) testing by introducing new task categories including multi-hop tracing and aggregation tasks. The benchmark provides flexible configurations for customized sequence length and task complexity, evaluating 17 long-context language models across 13 representative tasks to reveal that despite models claiming 32K+ token context sizes, only half maintain satisfactory performance at 32K length.",0.841,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.177860+00:00,benchmark_result,,,,True,,,,,,1043.0,,phi-3.5-mini-instruct,,,,0.747,,,microsoft,,,,,,,,0.747,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.177860+00:00,,,,,,False,,False,0.0,False,0.0,0.747,0.747,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.747,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,squality,SQuALITY,,,2025-07-19T19:56:12.722570+00:00,benchmark_result,,,,True,,,,,,825.0,,phi-3.5-mini-instruct,,,,0.243,,,microsoft,,,,,,,,0.243,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:12.722570+00:00,,,,,,False,,False,0.0,False,0.0,0.243,0.243,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",SQuALITY,"['summarization', 'long_context', 'language']",text,False,1.0,en,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",0.243,False,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,summscreenfd,SummScreenFD,,,2025-07-19T19:56:14.234498+00:00,benchmark_result,,,,True,,,,,,1508.0,,phi-3.5-mini-instruct,,,,0.16,,,microsoft,,,,,,,,0.16,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:14.234498+00:00,,,,,,False,,False,0.0,False,0.0,0.16,0.16,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",SummScreenFD,"['summarization', 'long_context']",text,False,1.0,en,"SummScreenFD is the ForeverDreaming subset of the SummScreen dataset for abstractive screenplay summarization, comprising pairs of TV series transcripts and human-written recaps from 88 different shows. The dataset provides a challenging testbed for abstractive summarization where plot details are often expressed indirectly in character dialogues and scattered across the entirety of the transcript, requiring models to find and integrate these details to form succinct plot descriptions.",0.16,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,10-shot,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.346508+00:00,benchmark_result,,,,True,,,,,,134.0,,phi-3.5-mini-instruct,,,,0.64,,,microsoft,,,,,,,,0.64,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:11.346508+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.64,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:13.217697+00:00,benchmark_result,,,,True,,,,,,1063.0,,phi-3.5-mini-instruct,,,,0.685,,,microsoft,,,,,,,,0.685,https://huggingface.co/microsoft/Phi-3.5-mini-instruct,,,,,,,,2025-07-19T19:56:13.217697+00:00,,,,,,False,,False,0.0,False,0.0,0.685,0.685,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-mini-instruct,microsoft,Microsoft,3800000000.0,3800000000.0,True,3400000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-mini-instruct is a 3.8B-parameter model that supports up to 128K context tokens, with improved multilingual capabilities across over 20 languages. It underwent additional training and safety post-training to enhance instruction-following, reasoning, math, and code generation. Ideal for environments with memory or latency constraints, it uses an MIT license.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.685,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,10-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.108027+00:00,benchmark_result,,,,True,,,,,,12.0,,phi-3.5-moe-instruct,,,,0.91,,,microsoft,,,,,,,,0.91,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.108027+00:00,,,,,,False,,False,0.0,False,0.0,0.91,0.91,Unknown,,,,,Undisclosed,Excellent (90%+),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.91,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),phi
,standard evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.086453+00:00,benchmark_result,,,,True,,,,,,1447.0,,phi-3.5-moe-instruct,,,,0.379,,,microsoft,,,,,,,,0.379,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.086453+00:00,,,,,,False,,False,0.0,False,0.0,0.379,0.379,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.379,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,0-shot chain-of-thought,,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.244054+00:00,benchmark_result,,,,True,,,,,,1077.0,,phi-3.5-moe-instruct,,,,0.791,,,microsoft,,,,,,,,0.791,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.244054+00:00,,,,,,False,,False,0.0,False,0.0,0.791,0.791,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.791,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,2-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.130867+00:00,benchmark_result,,,,True,,,,,,1024.0,,phi-3.5-moe-instruct,,,,0.846,,,microsoft,,,,,,,,0.846,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.130867+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.846,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,govreport,GovReport,,,2025-07-19T19:56:14.221191+00:00,benchmark_result,,,,True,,,,,,1503.0,,phi-3.5-moe-instruct,,,,0.264,,,microsoft,,,,,,,,0.264,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.221191+00:00,,,,,,False,,False,0.0,False,0.0,0.264,0.264,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",GovReport,"['summarization', 'long_context']",text,False,1.0,en,"A long document summarization dataset consisting of reports from government research agencies including Congressional Research Service and U.S. Government Accountability Office, with significantly longer documents and summaries than other datasets.",0.264,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,0-shot chain-of-thought,,,gpqa,GPQA,,,2025-07-19T19:56:11.649286+00:00,benchmark_result,,,,True,,,,,,284.0,,phi-3.5-moe-instruct,,,,0.368,,,microsoft,,,,,,,,0.368,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.649286+00:00,,,,,,False,,False,0.0,False,0.0,0.368,0.368,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.368,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,8-shot chain-of-thought,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.068601+00:00,benchmark_result,,,,True,,,,,,986.0,,phi-3.5-moe-instruct,,,,0.887,,,microsoft,,,,,,,,0.887,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.068601+00:00,,,,,,False,,False,0.0,False,0.0,0.887,0.887,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.887,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.171621+00:00,benchmark_result,,,,True,,,,,,42.0,,phi-3.5-moe-instruct,,,,0.838,,,microsoft,,,,,,,,0.838,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.171621+00:00,,,,,,False,,False,0.0,False,0.0,0.838,0.838,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.838,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,0-shot,,,humaneval,HumanEval,,,2025-07-19T19:56:12.629465+00:00,benchmark_result,,,,True,,,,,,776.0,,phi-3.5-moe-instruct,,,,0.707,,,microsoft,,,,,,,,0.707,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:12.629465+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.707,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,0-shot chain-of-thought,,,math,MATH,,,2025-07-19T19:56:11.841295+00:00,benchmark_result,,,,True,,,,,,391.0,,phi-3.5-moe-instruct,,,,0.595,,,microsoft,,,,,,,,0.595,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.841295+00:00,,,,,,False,,False,0.0,False,0.0,0.595,0.595,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.595,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,3-shot,,,mbpp,MBPP,,,2025-07-19T19:56:13.479387+00:00,benchmark_result,,,,True,,,,,,1177.0,,phi-3.5-moe-instruct,,,,0.808,,,microsoft,,,,,,,,0.808,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.479387+00:00,,,,,,False,,False,0.0,False,0.0,0.808,0.808,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00808,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,mega-mlqa,MEGA MLQA,,,2025-07-19T19:56:14.190086+00:00,benchmark_result,,,,True,,,,,,1493.0,,phi-3.5-moe-instruct,,,,0.653,,,microsoft,,,,,,,,0.653,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.190086+00:00,,,,,,False,,False,0.0,False,0.0,0.653,0.653,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MEGA MLQA,"['language', 'reasoning']",text,True,1.0,en,"MLQA as part of the MEGA (Multilingual Evaluation of Generative AI) benchmark suite. A multi-way aligned extractive QA evaluation benchmark for cross-lingual question answering across 7 languages (English, Arabic, German, Spanish, Hindi, Vietnamese, and Simplified Chinese) with over 12K QA instances in English and 5K in each other language.",0.653,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-tydi-qa,MEGA TyDi QA,,,2025-07-19T19:56:14.195123+00:00,benchmark_result,,,,True,,,,,,1495.0,,phi-3.5-moe-instruct,,,,0.671,,,microsoft,,,,,,,,0.671,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.195123+00:00,,,,,,False,,False,0.0,False,0.0,0.671,0.671,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MEGA TyDi QA,"['language', 'reasoning']",text,True,1.0,en,"TyDi QA as part of the MEGA benchmark suite. A question answering dataset covering 11 typologically diverse languages (Arabic, Bengali, English, Finnish, Indonesian, Japanese, Korean, Russian, Swahili, Telugu, and Thai) with 204K question-answer pairs. Features realistic information-seeking questions written by people who want to know the answer but don't know it yet.",0.671,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-udpos,MEGA UDPOS,,,2025-07-19T19:56:14.201497+00:00,benchmark_result,,,,True,,,,,,1497.0,,phi-3.5-moe-instruct,,,,0.604,,,microsoft,,,,,,,,0.604,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.201497+00:00,,,,,,False,,False,0.0,False,0.0,0.604,0.604,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MEGA UDPOS,['language'],text,True,1.0,en,"Universal Dependencies POS tagging as part of the MEGA benchmark suite. A multilingual part-of-speech tagging dataset based on Universal Dependencies treebanks, utilizing the universal POS tag set of 17 tags across 38 diverse languages from different language families. Used for evaluating multilingual POS tagging systems.",0.604,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,standard evaluation,,,mega-xcopa,MEGA XCOPA,,,2025-07-19T19:56:14.208476+00:00,benchmark_result,,,,True,,,,,,1499.0,,phi-3.5-moe-instruct,,,,0.766,,,microsoft,,,,,,,,0.766,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.208476+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MEGA XCOPA,"['reasoning', 'language']",text,True,1.0,en,"XCOPA (Cross-lingual Choice of Plausible Alternatives) as part of the MEGA benchmark suite. A typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages, including resource-poor languages like Eastern Apurímac Quechua and Haitian Creole. Requires models to select which choice is the effect or cause of a given premise.",0.766,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,mega-xstorycloze,MEGA XStoryCloze,,,2025-07-19T19:56:14.214764+00:00,benchmark_result,,,,True,,,,,,1501.0,,phi-3.5-moe-instruct,,,,0.828,,,microsoft,,,,,,,,0.828,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.214764+00:00,,,,,,False,,False,0.0,False,0.0,0.828,0.828,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MEGA XStoryCloze,"['reasoning', 'language']",text,True,1.0,en,"XStoryCloze as part of the MEGA benchmark suite. A cross-lingual story completion task that consists of professionally translated versions of the English StoryCloze dataset to 10 non-English languages. Requires models to predict the correct ending for a given four-sentence story, evaluating commonsense reasoning and narrative understanding.",0.828,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,0-shot chain-of-thought,,,mgsm,MGSM,,,2025-07-19T19:56:13.686017+00:00,benchmark_result,,,,True,,,,,,1281.0,,phi-3.5-moe-instruct,,,,0.587,,,microsoft,,,,,,,,0.587,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.686017+00:00,,,,,,False,,False,0.0,False,0.0,0.587,0.587,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.587,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,5-shot evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.239087+00:00,benchmark_result,,,,True,,,,,,74.0,,phi-3.5-moe-instruct,,,,0.789,,,microsoft,,,,,,,,0.789,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.239087+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.789,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.444580+00:00,benchmark_result,,,,True,,,,,,178.0,,phi-3.5-moe-instruct,,,,0.453,,,microsoft,,,,,,,,0.453,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.446076+00:00,,,,,,False,,False,0.0,False,0.0,0.453,0.453,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.453,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,5-shot evaluation,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.147234+00:00,benchmark_result,,,,True,,,,,,1475.0,,phi-3.5-moe-instruct,,,,0.699,,,microsoft,,,,,,,,0.699,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.147234+00:00,,,,,,False,,False,0.0,False,0.0,0.699,0.699,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.699,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,10-shot,,,openbookqa,OpenBookQA,,,2025-07-19T19:56:14.134275+00:00,benchmark_result,,,,True,,,,,,1470.0,,phi-3.5-moe-instruct,,,,0.896,,,microsoft,,,,,,,,0.896,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.134275+00:00,,,,,,False,,False,0.0,False,0.0,0.896,0.896,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",OpenBookQA,"['reasoning', 'general']",text,False,1.0,en,"OpenBookQA is a question-answering dataset modeled after open book exams for assessing human understanding. It contains 5,957 multiple-choice elementary-level science questions that probe understanding of 1,326 core science facts and their application to novel situations, requiring combination of open book facts with broad common knowledge through multi-hop reasoning.",0.896,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.152199+00:00,benchmark_result,,,,True,,,,,,1033.0,,phi-3.5-moe-instruct,,,,0.886,,,microsoft,,,,,,,,0.886,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.152199+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.886,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,qasper,Qasper,,,2025-07-19T19:56:14.171579+00:00,benchmark_result,,,,True,,,,,,1487.0,,phi-3.5-moe-instruct,,,,0.4,,,microsoft,,,,,,,,0.4,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.171579+00:00,,,,,,False,,False,0.0,False,0.0,0.4,0.4,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",Qasper,"['reasoning', 'long_context']",text,False,1.0,en,"QASPER is a dataset of 5,049 information-seeking questions and answers anchored in 1,585 NLP research papers. Questions are written by NLP practitioners who read only titles and abstracts, while answers require understanding the full paper text and provide supporting evidence. The dataset challenges models with complex reasoning across document sections for academic document question answering. Each question seeks information present in the full text and is answered by a separate set of NLP practitioners who also provide supporting evidence to answers.",0.4,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,qmsum,QMSum,,,2025-07-19T19:56:14.226358+00:00,benchmark_result,,,,True,,,,,,1505.0,,phi-3.5-moe-instruct,,,,0.199,,,microsoft,,,,,,,,0.199,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.226358+00:00,,,,,,False,,False,0.0,False,0.0,0.199,0.199,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",QMSum,"['summarization', 'long_context']",text,False,1.0,en,"QMSum is a benchmark for query-based multi-domain meeting summarization consisting of 1,808 query-summary pairs over 232 meetings across academic, product, and committee domains. The dataset enables models to select and summarize relevant spans of meetings in response to specific queries. Published at NAACL 2021, QMSum presents significant challenges in long meeting summarization where models must identify and summarize relevant content based on user queries.",0.199,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,average,,,repoqa,RepoQA,,,2025-07-19T19:56:14.184432+00:00,benchmark_result,,,,True,,,,,,1491.0,,phi-3.5-moe-instruct,,,,0.85,,,microsoft,,,,,,,,0.85,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.184432+00:00,,,,,,False,,False,0.0,False,0.0,0.85,0.85,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",RepoQA,"['long_context', 'reasoning', 'code']",text,True,1.0,en,"RepoQA is a benchmark for evaluating long-context code understanding capabilities of Large Language Models through the Searching Needle Function (SNF) task, where LLMs must locate specific functions in code repositories using natural language descriptions. The benchmark contains 500 code search tasks spanning 50 repositories across 5 modern programming languages (Python, Java, TypeScript, C++, and Rust), tested on 26 general and code-specific LLMs to assess their ability to comprehend and navigate code repositories.",0.85,False,True,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,long context (128K) evaluation,,,ruler,RULER,,,2025-07-19T19:56:14.177557+00:00,benchmark_result,,,,True,,,,,,1489.0,,phi-3.5-moe-instruct,,,,0.871,,,microsoft,,,,,,,,0.871,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.177557+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",RULER,"['long_context', 'reasoning']",text,False,1.0,en,"RULER (What's the Real Context Size of Your Long-Context Language Models?) is a synthetic benchmark designed to comprehensively evaluate the long-context capabilities of language models. It expands on needle-in-a-haystack (NIAH) testing by introducing new task categories including multi-hop tracing and aggregation tasks. The benchmark provides flexible configurations for customized sequence length and task complexity, evaluating 17 long-context language models across 13 representative tasks to reveal that despite models claiming 32K+ token context sizes, only half maintain satisfactory performance at 32K length.",0.871,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.176106+00:00,benchmark_result,,,,True,,,,,,1042.0,,phi-3.5-moe-instruct,,,,0.78,,,microsoft,,,,,,,,0.78,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.176106+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.78,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,squality,SQuALITY,,,2025-07-19T19:56:12.720914+00:00,benchmark_result,,,,True,,,,,,824.0,,phi-3.5-moe-instruct,,,,0.241,,,microsoft,,,,,,,,0.241,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:12.720914+00:00,,,,,,False,,False,0.0,False,0.0,0.241,0.241,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",SQuALITY,"['summarization', 'long_context', 'language']",text,False,1.0,en,"SQuALITY (Summarization-format QUestion Answering with Long Input Texts, Yes!) is a long-document summarization dataset built by hiring highly-qualified contractors to read public-domain short stories (3000-6000 words) and write original summaries from scratch. Each document has five summaries: one overview and four question-focused summaries. Designed to address limitations in existing summarization datasets by providing high-quality, faithful summaries.",0.241,False,False,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,summscreenfd,SummScreenFD,,,2025-07-19T19:56:14.232655+00:00,benchmark_result,,,,True,,,,,,1507.0,,phi-3.5-moe-instruct,,,,0.169,,,microsoft,,,,,,,,0.169,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:14.232655+00:00,,,,,,False,,False,0.0,False,0.0,0.169,0.169,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",SummScreenFD,"['summarization', 'long_context']",text,False,1.0,en,"SummScreenFD is the ForeverDreaming subset of the SummScreen dataset for abstractive screenplay summarization, comprising pairs of TV series transcripts and human-written recaps from 88 different shows. The dataset provides a challenging testbed for abstractive summarization where plot details are often expressed indirectly in character dialogues and scattered across the entirety of the transcript, requiring models to find and integrate these details to form succinct plot descriptions.",0.169,False,False,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,10-shot,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.344788+00:00,benchmark_result,,,,True,,,,,,133.0,,phi-3.5-moe-instruct,,,,0.775,,,microsoft,,,,,,,,0.775,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:11.344788+00:00,,,,,,False,,False,0.0,False,0.0,0.775,0.775,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.775,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:13.215763+00:00,benchmark_result,,,,True,,,,,,1062.0,,phi-3.5-moe-instruct,,,,0.813,,,microsoft,,,,,,,,0.813,https://huggingface.co/microsoft/Phi-3.5-MoE-instruct,,,,,,,,2025-07-19T19:56:13.215763+00:00,,,,,,False,,False,0.0,False,0.0,0.813,0.813,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-MoE-instruct,microsoft,Microsoft,60000000000.0,60000000000.0,True,4900000000000.0,True,False,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-MoE-instruct is a mixture-of-experts model with ~42B total parameters (6.6B active) and a 128K context window. It excels at reasoning, math, coding, and multilingual tasks, outperforming larger dense models in many benchmarks. It underwent a thorough safety post-training process (SFT + DPO) and is licensed under MIT. This model is ideal for scenarios where efficiency and high performance are both required, particularly in multi-lingual or reasoning-intensive tasks.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.813,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.626694+00:00,benchmark_result,,,,True,,,,,,1250.0,,phi-3.5-vision-instruct,,,,0.781,,,microsoft,,,,,,,,0.781,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:13.626694+00:00,,,,,,False,,False,0.0,False,0.0,0.781,0.781,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.781,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),phi
,standard evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.795942+00:00,benchmark_result,,,,True,,,,,,858.0,,phi-3.5-vision-instruct,,,,0.818,,,microsoft,,,,,,,,0.818,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:12.795942+00:00,,,,,,False,,False,0.0,False,0.0,0.818,0.818,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.818,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,intergps,InterGPS,,,2025-07-19T19:56:14.261813+00:00,benchmark_result,,,,True,,,,,,1520.0,,phi-3.5-vision-instruct,,,,0.363,,,microsoft,,,,,,,,0.363,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:14.261813+00:00,,,,,,False,,False,0.0,False,0.0,0.363,0.363,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",InterGPS,"['math', 'spatial_reasoning']",text,False,1.0,en,"Interpretable Geometry Problem Solver (Inter-GPS) with Geometry3K dataset of 3,002 geometry problems with dense annotation in formal language using theorem knowledge and symbolic reasoning",0.363,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,mathvista,MathVista,,,2025-07-19T19:56:12.080462+00:00,benchmark_result,,,,True,,,,,,520.0,,phi-3.5-vision-instruct,,,,0.439,,,microsoft,,,,,,,,0.439,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:12.080462+00:00,,,,,,False,,False,0.0,False,0.0,0.439,0.439,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.439,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,mmbench,MMBench,,,2025-07-19T19:56:14.238017+00:00,benchmark_result,,,,True,,,,,,1509.0,,phi-3.5-vision-instruct,,,,0.819,,,microsoft,,,,,,,,0.819,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:14.238017+00:00,,,,,,False,,False,0.0,False,0.0,0.819,0.819,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.819,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,mmmu,MMMU,,,2025-07-19T19:56:12.158730+00:00,benchmark_result,,,,True,,,,,,563.0,,phi-3.5-vision-instruct,,,,0.43,,,microsoft,,,,,,,,0.43,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:12.158730+00:00,,,,,,False,,False,0.0,False,0.0,0.43,0.43,Unknown,,,,,Undisclosed,Poor (<60%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.43,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,standard evaluation,,,pope,POPE,,,2025-07-19T19:56:14.266959+00:00,benchmark_result,,,,True,,,,,,1522.0,,phi-3.5-vision-instruct,,,,0.861,,,microsoft,,,,,,,,0.861,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:14.266959+00:00,,,,,,False,,False,0.0,False,0.0,0.861,0.861,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",POPE,"['vision', 'safety', 'multimodal']",multimodal,False,1.0,en,"Polling-based Object Probing Evaluation (POPE) is a benchmark for evaluating object hallucination in Large Vision-Language Models (LVLMs). POPE addresses the problem where LVLMs generate objects inconsistent with target images by using a polling-based query method that asks yes/no questions about object presence in images, providing more stable and flexible evaluation of object hallucination.",0.861,False,False,False,False,False,True,True,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,standard evaluation,,,scienceqa,ScienceQA,,,2025-07-19T19:56:14.258220+00:00,benchmark_result,,,,True,,,,,,1519.0,,phi-3.5-vision-instruct,,,,0.913,,,microsoft,,,,,,,,0.913,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:14.258220+00:00,,,,,,False,,False,0.0,False,0.0,0.913,0.913,Unknown,,,,,Undisclosed,Excellent (90%+),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",ScienceQA,"['reasoning', 'math', 'multimodal']",multimodal,False,1.0,en,"ScienceQA is the first large-scale multimodal science question answering benchmark with 21,208 multiple-choice questions covering 3 subjects (natural science, language science, social science), 26 topics, 127 categories, and 379 skills. The benchmark includes both text and image modalities, featuring detailed explanations and Chain-of-Thought reasoning to diagnose multi-hop reasoning ability.",0.913,False,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),phi
,standard evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.888892+00:00,benchmark_result,,,,True,,,,,,906.0,,phi-3.5-vision-instruct,,,,0.72,,,microsoft,,,,,,,,0.72,https://huggingface.co/microsoft/Phi-3.5-vision-instruct,,,,,,,,2025-07-19T19:56:12.888892+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Phi-3.5-vision-instruct,microsoft,Microsoft,4200000000.0,4200000000.0,True,500000000000.0,True,True,mit,Open & Permissive,2024-08-23,2024.0,8.0,2024-08,Very Large (>70B),"Phi-3.5-vision-instruct is a 4.2B-parameter open multimodal model with up to 128K context tokens. It emphasizes multi-frame image understanding and reasoning, boosting performance on single-image benchmarks while enabling multi-image comparison, summarization, and even video analysis. The model underwent safety post-training for improved instruction-following, alignment, and robust handling of visual and text inputs, and is released under the MIT license.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.72,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),phi
,simple-evals,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.082804+00:00,benchmark_result,,,,True,,,,,,1445.0,,phi-4,,,,0.754,,,microsoft,,,,,,,,0.754,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:14.082804+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.754,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,simple-evals,,,drop,DROP,,,2025-07-19T19:56:12.999411+00:00,benchmark_result,,,,True,,,,,,947.0,,phi-4,,,,0.755,,,microsoft,,,,,,,,0.755,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:12.999411+00:00,,,,,,False,,False,0.0,False,0.0,0.755,0.755,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",DROP,"['reasoning', 'math']",text,False,1.0,en,"DROP (Discrete Reasoning Over Paragraphs) is a reading comprehension benchmark requiring discrete reasoning over paragraph content. It contains crowdsourced, adversarially-created questions that require resolving references and performing discrete operations like addition, counting, or sorting, demanding comprehensive paragraph understanding beyond paraphrase-and-entity-typing shortcuts.",0.755,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,simple-evals,,,gpqa,GPQA,,,2025-07-19T19:56:11.644574+00:00,benchmark_result,,,,True,,,,,,282.0,,phi-4,,,,0.561,,,microsoft,,,,,,,,0.561,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:11.644574+00:00,,,,,,False,,False,0.0,False,0.0,0.561,0.561,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.561,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,simple-evals,,,humaneval,HumanEval,,,2025-07-19T19:56:12.628035+00:00,benchmark_result,,,,True,,,,,,775.0,,phi-4,,,,0.826,,,microsoft,,,,,,,,0.826,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:12.628035+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.826,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,simple-evals,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.064824+00:00,benchmark_result,,,,True,,,,,,1437.0,,phi-4,,,,0.828,,,microsoft,,,,,,,,0.828,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:14.064824+00:00,,,,,,False,,False,0.0,False,0.0,0.828,0.828,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.828,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,simple-evals,,,ifeval,IFEval,,,2025-07-19T19:56:12.261770+00:00,benchmark_result,,,,True,,,,,,611.0,,phi-4,,,,0.63,,,microsoft,,,,,,,,0.63,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:12.261770+00:00,,,,,,False,,False,0.0,False,0.0,0.63,0.63,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.63,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,simple-evals,,,livebench,LiveBench,,,2025-07-19T19:56:12.569213+00:00,benchmark_result,,,,True,,,,,,746.0,,phi-4,,,,0.476,,,microsoft,,,,,,,,0.476,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:12.569213+00:00,,,,,,False,,False,0.0,False,0.0,0.476,0.476,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.476,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,simple-evals,,,math,MATH,,,2025-07-19T19:56:11.837602+00:00,benchmark_result,,,,True,,,,,,389.0,,phi-4,,,,0.804,,,microsoft,,,,,,,,0.804,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:11.837602+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.804,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,simple-evals,,,mgsm,MGSM,,,2025-07-19T19:56:13.681417+00:00,benchmark_result,,,,True,,,,,,1279.0,,phi-4,,,,0.806,,,microsoft,,,,,,,,0.806,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:13.681417+00:00,,,,,,False,,False,0.0,False,0.0,0.806,0.806,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.806,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,simple-evals,,,mmlu,MMLU,,,2025-07-19T19:56:11.236043+00:00,benchmark_result,,,,True,,,,,,72.0,,phi-4,,,,0.848,,,microsoft,,,,,,,,0.848,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:11.236043+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.848,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,simple-evals,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.441164+00:00,benchmark_result,,,,True,,,,,,176.0,,phi-4,,,,0.704,,,microsoft,,,,,,,,0.704,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:11.441164+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.704,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,simple-evals,,,phibench,PhiBench,,,2025-07-19T19:56:14.124860+00:00,benchmark_result,,,,True,,,,,,1466.0,,phi-4,,,,0.562,,,microsoft,,,,,,,,0.562,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:14.124860+00:00,,,,,,False,,False,0.0,False,0.0,0.562,0.562,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",PhiBench,"['reasoning', 'math', 'general']",text,False,1.0,en,"PhiBench is an internal benchmark designed to evaluate diverse skills and reasoning abilities of language models, covering a wide range of tasks including coding (debugging, extending incomplete code, explaining code snippets) and mathematics (identifying proof errors, generating related problems). Created by Microsoft's research team to address limitations of standard academic benchmarks and guide the development of the Phi-4 model.",0.562,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,simple-evals,,,simpleqa,SimpleQA,,,2025-07-19T19:56:11.546523+00:00,benchmark_result,,,,True,,,,,,233.0,,phi-4,,,,0.03,,,microsoft,,,,,,,,0.03,https://arxiv.org/pdf/2412.08905,,,,,,,,2025-07-19T19:56:11.546523+00:00,,,,,,False,,False,0.0,False,0.0,0.03,0.03,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4,microsoft,Microsoft,14700000000.0,14700000000.0,True,9800000000000.0,True,False,mit,Open & Permissive,2024-12-12,2024.0,12.0,2024-12,Very Large (>70B),"phi-4 is a state-of-the-art open model built to excel at advanced reasoning, coding, and knowledge tasks. It leverages a blend of synthetic data, filtered web data, academic texts, and supervised fine-tuning for precision, alignment, and safety.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.03,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,10-shot,,,arc-c,ARC-C,,,2025-07-19T19:56:11.105059+00:00,benchmark_result,,,,True,,,,,,11.0,,phi-4-mini,,,,0.837,,,microsoft,,,,,,,,0.837,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.105059+00:00,,,,,,False,,False,0.0,False,0.0,0.837,0.837,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.837,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,Standard evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.084727+00:00,benchmark_result,,,,True,,,,,,1446.0,,phi-4-mini,,,,0.328,,,microsoft,,,,,,,,0.328,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:14.084727+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.328,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,"0-shot, CoT",,,big-bench-hard,BIG-Bench Hard,,,2025-07-19T19:56:13.242363+00:00,benchmark_result,,,,True,,,,,,1076.0,,phi-4-mini,,,,0.704,,,microsoft,,,,,,,,0.704,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.242363+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",BIG-Bench Hard,"['reasoning', 'math', 'language']",text,False,1.0,en,"BIG-Bench Hard (BBH) is a subset of 23 challenging BIG-Bench tasks selected because prior language model evaluations did not outperform average human-rater performance. The benchmark contains 6,511 evaluation examples testing various forms of multi-step reasoning including arithmetic, logical reasoning (Boolean expressions, logical deduction), geometric reasoning, temporal reasoning, and language understanding. Tasks require capabilities such as causal judgment, object counting, navigation, pattern recognition, and complex problem solving.",0.704,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,2-shot,,,boolq,BoolQ,,,2025-07-19T19:56:13.129244+00:00,benchmark_result,,,,True,,,,,,1023.0,,phi-4-mini,,,,0.812,,,microsoft,,,,,,,,0.812,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.129244+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",BoolQ,"['language', 'reasoning']",text,False,1.0,en,"BoolQ is a reading comprehension dataset for yes/no questions containing 15,942 naturally occurring examples. Each example consists of a question, passage, and boolean answer, where questions are generated in unprompted and unconstrained settings. The dataset challenges models with complex, non-factoid information requiring entailment-like inference to solve.",0.812,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,"0-shot, CoT",,,gpqa,GPQA,,,2025-07-19T19:56:11.646470+00:00,benchmark_result,,,,True,,,,,,283.0,,phi-4-mini,,,,0.252,,,microsoft,,,,,,,,0.252,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.646470+00:00,,,,,,False,,False,0.0,False,0.0,0.252,0.252,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.252,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,"8-shot, CoT",,,gsm8k,GSM8k,,,2025-07-19T19:56:13.066927+00:00,benchmark_result,,,,True,,,,,,985.0,,phi-4-mini,,,,0.886,,,microsoft,,,,,,,,0.886,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.066927+00:00,,,,,,False,,False,0.0,False,0.0,0.886,0.886,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.886,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,5-shot,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.169983+00:00,benchmark_result,,,,True,,,,,,41.0,,phi-4-mini,,,,0.691,,,microsoft,,,,,,,,0.691,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.169983+00:00,,,,,,False,,False,0.0,False,0.0,0.691,0.691,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.691,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,"0-shot, CoT",,,math,MATH,,,2025-07-19T19:56:11.839081+00:00,benchmark_result,,,,True,,,,,,390.0,,phi-4-mini,,,,0.64,,,microsoft,,,,,,,,0.64,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.839081+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.64,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,5-shot,,,mgsm,MGSM,,,2025-07-19T19:56:13.683394+00:00,benchmark_result,,,,True,,,,,,1280.0,,phi-4-mini,,,,0.639,,,microsoft,,,,,,,,0.639,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.683394+00:00,,,,,,False,,False,0.0,False,0.0,0.639,0.639,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.639,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.237489+00:00,benchmark_result,,,,True,,,,,,73.0,,phi-4-mini,,,,0.673,,,microsoft,,,,,,,,0.673,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.237489+00:00,,,,,,False,,False,0.0,False,0.0,0.673,0.673,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.673,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,"0-shot, CoT",,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.443019+00:00,benchmark_result,,,,True,,,,,,177.0,,phi-4-mini,,,,0.528,,,microsoft,,,,,,,,0.528,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.443019+00:00,,,,,,False,,False,0.0,False,0.0,0.528,0.528,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.528,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,5-shot,,,multilingual-mmlu,Multilingual MMLU,,,2025-07-19T19:56:14.141886+00:00,benchmark_result,,,,True,,,,,,1473.0,,phi-4-mini,,,,0.493,,,microsoft,,,,,,,,0.493,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:14.141886+00:00,,,,,,False,,False,0.0,False,0.0,0.493,0.493,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",Multilingual MMLU,"['general', 'reasoning', 'language']",text,True,1.0,en,"MMLU-ProX is a comprehensive multilingual benchmark covering 29 typologically diverse languages, building upon MMLU-Pro. Each language version consists of 11,829 identical questions enabling direct cross-linguistic comparisons. The benchmark evaluates large language models' reasoning capabilities across linguistic and cultural boundaries through challenging, reasoning-focused questions with 10 answer choices.",0.493,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,10-shot,,,openbookqa,OpenBookQA,,,2025-07-19T19:56:14.132301+00:00,benchmark_result,,,,True,,,,,,1469.0,,phi-4-mini,,,,0.792,,,microsoft,,,,,,,,0.792,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:14.132301+00:00,,,,,,False,,False,0.0,False,0.0,0.792,0.792,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",OpenBookQA,"['reasoning', 'general']",text,False,1.0,en,"OpenBookQA is a question-answering dataset modeled after open book exams for assessing human understanding. It contains 5,957 multiple-choice elementary-level science questions that probe understanding of 1,326 core science facts and their application to novel situations, requiring combination of open book facts with broad common knowledge through multi-hop reasoning.",0.792,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,5-shot,,,piqa,PIQA,,,2025-07-19T19:56:13.150113+00:00,benchmark_result,,,,True,,,,,,1032.0,,phi-4-mini,,,,0.776,,,microsoft,,,,,,,,0.776,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.150113+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",PIQA,"['reasoning', 'physics', 'general']",text,False,1.0,en,"PIQA (Physical Interaction: Question Answering) is a benchmark dataset for physical commonsense reasoning in natural language. It tests AI systems' ability to answer questions requiring physical world knowledge through multiple choice questions with everyday situations, focusing on atypical solutions inspired by instructables.com. The dataset contains 21,000 multiple choice questions where models must choose the most appropriate solution for physical interactions.",0.776,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,5-shot,,,social-iqa,Social IQa,,,2025-07-19T19:56:13.172567+00:00,benchmark_result,,,,True,,,,,,1041.0,,phi-4-mini,,,,0.725,,,microsoft,,,,,,,,0.725,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:13.172567+00:00,,,,,,False,,False,0.0,False,0.0,0.725,0.725,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",Social IQa,"['reasoning', 'psychology']",text,False,1.0,en,"The first large-scale benchmark for commonsense reasoning about social situations. Contains 38,000 multiple choice questions probing emotional and social intelligence in everyday situations, testing commonsense understanding of social interactions and theory of mind reasoning about the implied emotions and behavior of others.",0.725,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,"MC2, 10-shot",,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.343180+00:00,benchmark_result,,,,True,,,,,,132.0,,phi-4-mini,,,,0.664,,,microsoft,,,,,,,,0.664,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.343180+00:00,,,,,,False,,False,0.0,False,0.0,0.664,0.664,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.664,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,5-shot,,,winogrande,Winogrande,,,2025-07-19T19:56:11.382335+00:00,benchmark_result,,,,True,,,,,,149.0,,phi-4-mini,,,,0.67,,,microsoft,,,,,,,,0.67,https://huggingface.co/microsoft/Phi-4-mini-instruct,,,,,,,,2025-07-19T19:56:11.382335+00:00,,,,,,False,,False,0.0,False,0.0,0.67,0.67,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Mini,microsoft,Microsoft,3840000000.0,3840000000.0,True,5000000000000.0,True,False,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi 4 Mini Instruct is a lightweight (3.8B parameters) open model built upon synthetic data and filtered web data, focusing on high-quality reasoning. It supports a 128K token context length and is enhanced for instruction adherence and safety via supervised fine-tuning and direct preference optimization.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.67,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,Standard evaluation,,,aime,AIME,,,2025-07-19T19:56:14.061299+00:00,benchmark_result,,,,True,,,,,,1436.0,,phi-4-mini-reasoning,,,,0.575,,,microsoft,,,,,,,,0.575,https://huggingface.co/microsoft/Phi-4-mini-reasoning,,,,,,,,2025-07-19T19:56:14.061299+00:00,,,,,,False,,False,0.0,False,0.0,0.575,0.575,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini Reasoning,microsoft,Microsoft,3800000000.0,3800000000.0,True,150000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.",AIME,"['math', 'reasoning']",text,False,1.0,en,American Invitational Mathematics Examination (AIME) benchmark for evaluating mathematical reasoning capabilities of large language models. Contains 30 challenging mathematical problems from AIME 2024 competition that require multi-step reasoning and advanced mathematical insight. Each problem has an integer answer between 000-999.,0.575,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.642870+00:00,benchmark_result,,,,True,,,,,,281.0,,phi-4-mini-reasoning,,,,0.52,,,microsoft,,,,,,,,0.52,https://huggingface.co/microsoft/Phi-4-mini-reasoning,,,,,,,,2025-07-19T19:56:11.642870+00:00,,,,,,False,,False,0.0,False,0.0,0.52,0.52,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Mini Reasoning,microsoft,Microsoft,3800000000.0,3800000000.0,True,150000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.52,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,Standard evaluation,,,math-500,MATH-500,,,2025-07-19T19:56:12.032863+00:00,benchmark_result,,,,True,,,,,,494.0,,phi-4-mini-reasoning,,,,0.946,,,microsoft,,,,,,,,0.946,https://huggingface.co/microsoft/Phi-4-mini-reasoning,,,,,,,,2025-07-19T19:56:12.032863+00:00,,,,,,False,,False,0.0,False,0.0,0.946,0.946,Unknown,,,,,Undisclosed,Excellent (90%+),Phi 4 Mini Reasoning,microsoft,Microsoft,3800000000.0,3800000000.0,True,150000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-mini-reasoning is designed for multi-step, logic-intensive mathematical problem-solving tasks under memory/compute constrained environments and latency bound scenarios. Some of the use cases include formal proof generation, symbolic computation, advanced word problems, and a wide range of mathematical reasoning scenarios. These models excel at maintaining context across steps, applying structured logic, and delivering accurate, reliable solutions in domains that require deep analytical thinking.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.946,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),phi
,Standard Evaluation,,,ai2d,AI2D,,,2025-07-19T19:56:13.628230+00:00,benchmark_result,,,,True,,,,,,1251.0,,phi-4-multimodal-instruct,,,,0.823,,,microsoft,,,,,,,,0.823,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:13.628230+00:00,,,,,,False,,False,0.0,False,0.0,0.823,0.823,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.823,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,Standard Evaluation,,,blink,BLINK,,,2025-07-19T19:56:14.329567+00:00,benchmark_result,,,,True,,,,,,1545.0,,phi-4-multimodal-instruct,,,,0.613,,,microsoft,,,,,,,,0.613,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.329567+00:00,,,,,,False,,False,0.0,False,0.0,0.613,0.613,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",BLINK,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"BLINK: Multimodal Large Language Models Can See but Not Perceive. A benchmark for multimodal language models focusing on core visual perception abilities. Reformats 14 classic computer vision tasks into 3,807 multiple-choice questions paired with single or multiple images and visual prompting. Tasks include relative depth estimation, visual correspondence, forensics detection, multi-view reasoning, counting, object localization, and spatial reasoning that humans can solve 'within a blink'.",0.613,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),phi
,Standard Evaluation,,,chartqa,ChartQA,,,2025-07-19T19:56:12.797898+00:00,benchmark_result,,,,True,,,,,,859.0,,phi-4-multimodal-instruct,,,,0.814,,,microsoft,,,,,,,,0.814,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:12.797898+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.814,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,Standard Evaluation,,,docvqa,DocVQA,,,2025-07-19T19:56:12.836095+00:00,benchmark_result,,,,True,,,,,,881.0,,phi-4-multimodal-instruct,,,,0.932,,,microsoft,,,,,,,,0.932,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:12.836095+00:00,,,,,,False,,False,0.0,False,0.0,0.932,0.932,Unknown,,,,,Undisclosed,Excellent (90%+),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.932,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),phi
,Standard Evaluation,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.609397+00:00,benchmark_result,,,,True,,,,,,1241.0,,phi-4-multimodal-instruct,,,,0.727,,,microsoft,,,,,,,,0.727,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:13.609397+00:00,,,,,,False,,False,0.0,False,0.0,0.727,0.727,Unknown,,,,,Undisclosed,Good (70-79%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.727,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),phi
,testmini,,,intergps,InterGPS,,,2025-07-19T19:56:14.263464+00:00,benchmark_result,,,,True,,,,,,1521.0,,phi-4-multimodal-instruct,,,,0.486,,,microsoft,,,,,,,,0.486,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.263464+00:00,,,,,,False,,False,0.0,False,0.0,0.486,0.486,Unknown,,,,,Undisclosed,Poor (<60%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",InterGPS,"['math', 'spatial_reasoning']",text,False,1.0,en,"Interpretable Geometry Problem Solver (Inter-GPS) with Geometry3K dataset of 3,002 geometry problems with dense annotation in formal language using theorem knowledge and symbolic reasoning",0.486,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,testmini,,,mathvista,MathVista,,,2025-07-19T19:56:12.082453+00:00,benchmark_result,,,,True,,,,,,521.0,,phi-4-multimodal-instruct,,,,0.624,,,microsoft,,,,,,,,0.624,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:12.082453+00:00,,,,,,False,,False,0.0,False,0.0,0.624,0.624,Unknown,,,,,Undisclosed,Fair (60-69%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.624,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),phi
,dev-en,,,mmbench,MMBench,,,2025-07-19T19:56:14.240071+00:00,benchmark_result,,,,True,,,,,,1510.0,,phi-4-multimodal-instruct,,,,0.867,,,microsoft,,,,,,,,0.867,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.240071+00:00,,,,,,False,,False,0.0,False,0.0,0.867,0.867,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.867,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,Standard Evaluation,,,mmmu,MMMU,,,2025-07-19T19:56:12.161302+00:00,benchmark_result,,,,True,,,,,,564.0,,phi-4-multimodal-instruct,,,,0.551,,,microsoft,,,,,,,,0.551,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:12.161302+00:00,,,,,,False,,False,0.0,False,0.0,0.551,0.551,Unknown,,,,,Undisclosed,Poor (<60%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.551,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,std/vision,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.285447+00:00,benchmark_result,,,,True,,,,,,1528.0,,phi-4-multimodal-instruct,,,,0.385,,,microsoft,,,,,,,,0.385,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.285447+00:00,,,,,,False,,False,0.0,False,0.0,0.385,0.385,Unknown,,,,,Undisclosed,Poor (<60%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.385,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),phi
,Standard Evaluation,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.309778+00:00,benchmark_result,,,,True,,,,,,1538.0,,phi-4-multimodal-instruct,,,,0.844,,,microsoft,,,,,,,,0.844,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.309778+00:00,,,,,,False,,False,0.0,False,0.0,0.844,0.844,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.844,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,Standard Evaluation,,,pope,POPE,,,2025-07-19T19:56:14.268923+00:00,benchmark_result,,,,True,,,,,,1523.0,,phi-4-multimodal-instruct,,,,0.856,,,microsoft,,,,,,,,0.856,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.268923+00:00,,,,,,False,,False,0.0,False,0.0,0.856,0.856,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",POPE,"['vision', 'safety', 'multimodal']",multimodal,False,1.0,en,"Polling-based Object Probing Evaluation (POPE) is a benchmark for evaluating object hallucination in Large Vision-Language Models (LVLMs). POPE addresses the problem where LVLMs generate objects inconsistent with target images by using a polling-based query method that asks yes/no questions about object presence in images, providing more stable and flexible evaluation of object hallucination.",0.856,False,False,False,False,False,True,True,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),phi
,img-test,,,scienceqa-visual,ScienceQA Visual,,,2025-07-19T19:56:14.303456+00:00,benchmark_result,,,,True,,,,,,1537.0,,phi-4-multimodal-instruct,,,,0.975,,,microsoft,,,,,,,,0.975,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:14.303456+00:00,,,,,,False,,False,0.0,False,0.0,0.975,0.975,Unknown,,,,,Undisclosed,Excellent (90%+),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",ScienceQA Visual,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"ScienceQA Visual is a multimodal science question answering benchmark consisting of 21,208 multiple-choice questions from elementary and high school science curricula. The dataset covers 3 subjects (natural science, language science, social science), 26 topics, 127 categories, and 379 skills. 48.7% of questions include image context requiring multimodal reasoning. Questions are annotated with lectures (83.9%) and explanations (90.5%) to support chain-of-thought reasoning for science question answering.",0.975,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),phi
,Standard Evaluation,,,textvqa,TextVQA,,,2025-07-19T19:56:12.890738+00:00,benchmark_result,,,,True,,,,,,907.0,,phi-4-multimodal-instruct,,,,0.756,,,microsoft,,,,,,,,0.756,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:12.890738+00:00,,,,,,False,,False,0.0,False,0.0,0.756,0.756,Unknown,,,,,Undisclosed,Good (70-79%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.756,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),phi
,16 frames,,,video-mme,Video-MME,,,2025-07-19T19:56:13.911859+00:00,benchmark_result,,,,True,,,,,,1383.0,,phi-4-multimodal-instruct,,,,0.55,,,microsoft,,,,,,,,0.55,https://huggingface.co/microsoft/Phi-4-multimodal-instruct,,,,,,,,2025-07-19T19:56:13.911859+00:00,,,,,,False,,False,0.0,False,0.0,0.55,0.55,Unknown,,,,,Undisclosed,Poor (<60%),Phi-4-multimodal-instruct,microsoft,Microsoft,5600000000.0,5600000000.0,True,5000000000000.0,True,True,mit,Open & Permissive,2025-02-01,2025.0,2.0,2025-02,Very Large (>70B),"Phi-4-multimodal-instruct is a lightweight (5.57B parameters) open multimodal foundation model that leverages research and datasets from Phi-3.5 and 4.0. It processes text, image, and audio inputs to generate text outputs, supporting a 128K token context length. Enhanced via SFT, DPO, and RLHF for instruction following and safety.",Video-MME,"['multimodal', 'vision', 'reasoning']",multimodal,True,1.0,en,"Video-MME is the first-ever comprehensive evaluation benchmark of Multi-modal Large Language Models (MLLMs) in video analysis. It features 900 videos totaling 254 hours with 2,700 human-annotated question-answer pairs across 6 primary visual domains (Knowledge, Film & Television, Sports Competition, Life Record, Multilingual, and others) and 30 subfields. The benchmark evaluates models across diverse temporal dimensions (11 seconds to 1 hour), integrates multi-modal inputs including video frames, subtitles, and audio, and uses rigorous manual labeling by expert annotators for precise assessment.",0.55,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),phi
,Standard evaluation,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.955706+00:00,benchmark_result,,,,True,,,,,,450.0,,phi-4-reasoning,,,,0.753,,,microsoft,,,,,,,,0.753,https://huggingface.co/microsoft/Phi-4-reasoning,,,,,,,,2025-07-19T19:56:11.955706+00:00,,,,,,False,,False,0.0,False,0.0,0.753,0.753,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.753,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Standard evaluation,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.444086+00:00,benchmark_result,,,,True,,,,,,688.0,,phi-4-reasoning,,,,0.629,,,microsoft,,,,,,,,0.629,https://huggingface.co/microsoft/Phi-4-reasoning,,,,,,,,2025-07-19T19:56:12.444086+00:00,,,,,,False,,False,0.0,False,0.0,0.629,0.629,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.629,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,Standard evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.091856+00:00,benchmark_result,,,,True,,,,,,1450.0,,phi-4-reasoning,,,,0.733,,,microsoft,,,,,,,,0.733,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.091856+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.733,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,3K-token subset,,,flenqa,FlenQA,,,2025-07-19T19:56:14.281300+00:00,benchmark_result,,,,True,,,,,,1527.0,,phi-4-reasoning,,,,0.977,,,microsoft,,,,,,,,0.977,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.281300+00:00,,,,,,False,,False,0.0,False,0.0,0.977,0.977,Unknown,,,,,Undisclosed,Excellent (90%+),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",FlenQA,"['reasoning', 'long_context']",text,False,1.0,en,"Flexible Length Question Answering dataset for evaluating the impact of input length on reasoning performance of language models, featuring True/False questions embedded in contexts of varying lengths (250-3000 tokens) across three reasoning tasks: Monotone Relations, People In Rooms, and simplified Ruletaker",0.977,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,SOTA (95%+),phi
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.654843+00:00,benchmark_result,,,,True,,,,,,287.0,,phi-4-reasoning,,,,0.658,,,microsoft,,,,,,,,0.658,https://huggingface.co/microsoft/Phi-4-reasoning,,,,,,,,2025-07-19T19:56:11.654843+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.658,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,Standard evaluation,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.068831+00:00,benchmark_result,,,,True,,,,,,1439.0,,phi-4-reasoning,,,,0.929,,,microsoft,,,,,,,,0.929,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.068831+00:00,,,,,,False,,False,0.0,False,0.0,0.929,0.929,Unknown,,,,,Undisclosed,Excellent (90%+),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.929,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),phi
,Strict,,,ifeval,IFEval,,,2025-07-19T19:56:12.265033+00:00,benchmark_result,,,,True,,,,,,613.0,,phi-4-reasoning,,,,0.834,,,microsoft,,,,,,,,0.834,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:12.265033+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.834,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,8/1/24–2/1/25,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.324523+00:00,benchmark_result,,,,True,,,,,,1114.0,,phi-4-reasoning,,,,0.538,,,microsoft,,,,,,,,0.538,https://huggingface.co/microsoft/Phi-4-reasoning,,,,,,,,2025-07-19T19:56:13.324523+00:00,,,,,,False,,False,0.0,False,0.0,0.538,0.538,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.538,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,Standard evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.453150+00:00,benchmark_result,,,,True,,,,,,183.0,,phi-4-reasoning,,,,0.743,,,microsoft,,,,,,,,0.743,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:11.453150+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.743,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Standard evaluation,,,omnimath,OmniMath,,,2025-07-19T19:56:14.276205+00:00,benchmark_result,,,,True,,,,,,1525.0,,phi-4-reasoning,,,,0.766,,,microsoft,,,,,,,,0.766,https://huggingface.co/microsoft/Phi-4-reasoning,,,,,,,,2025-07-19T19:56:14.276205+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",OmniMath,"['math', 'reasoning']",text,False,1.0,en,"A Universal Olympiad Level Mathematic Benchmark for Large Language Models containing 4,428 competition-level problems with rigorous human annotation, categorized into over 33 sub-domains and spanning more than 10 distinct difficulty levels",0.766,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,2.21,,,phibench,PhiBench,,,2025-07-19T19:56:14.127989+00:00,benchmark_result,,,,True,,,,,,1468.0,,phi-4-reasoning,,,,0.706,,,microsoft,,,,,,,,0.706,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.127989+00:00,,,,,,False,,False,0.0,False,0.0,0.706,0.706,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning on a dataset of chain-of-thought traces and reinforcement learning. It focuses on math, science, and coding skills.",PhiBench,"['reasoning', 'math', 'general']",text,False,1.0,en,"PhiBench is an internal benchmark designed to evaluate diverse skills and reasoning abilities of language models, covering a wide range of tasks including coding (debugging, extending incomplete code, explaining code snippets) and mathematics (identifying proof errors, generating related problems). Created by Microsoft's research team to address limitations of standard academic benchmarks and guide the development of the Phi-4 model.",0.706,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Standard evaluation,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.953709+00:00,benchmark_result,,,,True,,,,,,449.0,,phi-4-reasoning-plus,,,,0.813,,,microsoft,,,,,,,,0.813,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:11.953709+00:00,,,,,,False,,False,0.0,False,0.0,0.813,0.813,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.813,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,Standard evaluation,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.440995+00:00,benchmark_result,,,,True,,,,,,687.0,,phi-4-reasoning-plus,,,,0.78,,,microsoft,,,,,,,,0.78,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:12.440995+00:00,,,,,,False,,False,0.0,False,0.0,0.78,0.78,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.78,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Standard evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.090173+00:00,benchmark_result,,,,True,,,,,,1449.0,,phi-4-reasoning-plus,,,,0.79,,,microsoft,,,,,,,,0.79,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.090173+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.79,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,3K-token subset,,,flenqa,FlenQA,,,2025-07-19T19:56:14.279654+00:00,benchmark_result,,,,True,,,,,,1526.0,,phi-4-reasoning-plus,,,,0.979,,,microsoft,,,,,,,,0.979,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.279654+00:00,,,,,,False,,False,0.0,False,0.0,0.979,0.979,Unknown,,,,,Undisclosed,Excellent (90%+),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",FlenQA,"['reasoning', 'long_context']",text,False,1.0,en,"Flexible Length Question Answering dataset for evaluating the impact of input length on reasoning performance of language models, featuring True/False questions embedded in contexts of varying lengths (250-3000 tokens) across three reasoning tasks: Monotone Relations, People In Rooms, and simplified Ruletaker",0.979,False,False,False,True,False,False,False,True,False,False,False,False,False,False,False,False,False,SOTA (95%+),phi
,Diamond,,,gpqa,GPQA,,,2025-07-19T19:56:11.652983+00:00,benchmark_result,,,,True,,,,,,286.0,,phi-4-reasoning-plus,,,,0.689,,,microsoft,,,,,,,,0.689,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:11.652983+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.689,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),phi
,Standard evaluation,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.066904+00:00,benchmark_result,,,,True,,,,,,1438.0,,phi-4-reasoning-plus,,,,0.923,,,microsoft,,,,,,,,0.923,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.066904+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.923,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),phi
,Strict,,,ifeval,IFEval,,,2025-07-19T19:56:12.263243+00:00,benchmark_result,,,,True,,,,,,612.0,,phi-4-reasoning-plus,,,,0.849,,,microsoft,,,,,,,,0.849,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:12.263243+00:00,,,,,,False,,False,0.0,False,0.0,0.849,0.849,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.849,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,8/1/24–2/1/25,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.322076+00:00,benchmark_result,,,,True,,,,,,1113.0,,phi-4-reasoning-plus,,,,0.531,,,microsoft,,,,,,,,0.531,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:13.322076+00:00,,,,,,False,,False,0.0,False,0.0,0.531,0.531,Unknown,,,,,Undisclosed,Poor (<60%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.531,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),phi
,Standard evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.451685+00:00,benchmark_result,,,,True,,,,,,182.0,,phi-4-reasoning-plus,,,,0.76,,,microsoft,,,,,,,,0.76,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:11.451685+00:00,,,,,,False,,False,0.0,False,0.0,0.76,0.76,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.76,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Standard evaluation,,,omnimath,OmniMath,,,2025-07-19T19:56:14.274539+00:00,benchmark_result,,,,True,,,,,,1524.0,,phi-4-reasoning-plus,,,,0.819,,,microsoft,,,,,,,,0.819,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.274539+00:00,,,,,,False,,False,0.0,False,0.0,0.819,0.819,Unknown,,,,,Undisclosed,Very Good (80-89%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",OmniMath,"['math', 'reasoning']",text,False,1.0,en,"A Universal Olympiad Level Mathematic Benchmark for Large Language Models containing 4,428 competition-level problems with rigorous human annotation, categorized into over 33 sub-domains and spanning more than 10 distinct difficulty levels",0.819,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),phi
,2.21,,,phibench,PhiBench,,,2025-07-19T19:56:14.126449+00:00,benchmark_result,,,,True,,,,,,1467.0,,phi-4-reasoning-plus,,,,0.742,,,microsoft,,,,,,,,0.742,https://huggingface.co/microsoft/Phi-4-reasoning-plus,,,,,,,,2025-07-19T19:56:14.126449+00:00,,,,,,False,,False,0.0,False,0.0,0.742,0.742,Unknown,,,,,Undisclosed,Good (70-79%),Phi 4 Reasoning Plus,microsoft,Microsoft,14000000000.0,14000000000.0,True,16000000000.0,True,False,mit,Open & Permissive,2025-04-30,2025.0,4.0,2025-04,Very Large (>70B),"Phi-4-reasoning-plus is a state-of-the-art open-weight reasoning model finetuned from Phi-4 using supervised fine-tuning and reinforcement learning. It focuses on math, science, and coding skills. This 'plus' version has higher accuracy due to additional RL training but may have higher latency.",PhiBench,"['reasoning', 'math', 'general']",text,False,1.0,en,"PhiBench is an internal benchmark designed to evaluate diverse skills and reasoning abilities of language models, covering a wide range of tasks including coding (debugging, extending incomplete code, explaining code snippets) and mathematics (identifying proof errors, generating related problems). Created by Microsoft's research team to address limitations of standard academic benchmarks and guide the development of the Phi-4 model.",0.742,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),phi
,Chain of Thought (CoT),,,chartqa,ChartQA,,,2025-07-19T19:56:12.822444+00:00,benchmark_result,,,,True,,,,,,874.0,,pixtral-12b-2409,,,,0.818,,,mistral,,,,,,,,0.818,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.822444+00:00,,,,,,False,,False,0.0,False,0.0,0.818,0.818,Unknown,,,,,Undisclosed,Very Good (80-89%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.818,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),pixtral
,ANLS,,,docvqa,DocVQA,,,2025-07-19T19:56:12.871485+00:00,benchmark_result,,,,True,,,,,,899.0,,pixtral-12b-2409,,,,0.907,,,mistral,,,,,,,,0.907,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.871485+00:00,,,,,,False,,False,0.0,False,0.0,0.907,0.907,Unknown,,,,,Undisclosed,Excellent (90%+),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.907,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),pixtral
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.684555+00:00,benchmark_result,,,,True,,,,,,808.0,,pixtral-12b-2409,,,,0.72,,,mistral,,,,,,,,0.72,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.684555+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.72,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),pixtral
,Text Instruction Following Score,,,ifeval,IFEval,,,2025-07-19T19:56:12.297384+00:00,benchmark_result,,,,True,,,,,,631.0,,pixtral-12b-2409,,,,0.613,,,mistral,,,,,,,,0.613,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.297384+00:00,,,,,,False,,False,0.0,False,0.0,0.613,0.613,Unknown,,,,,Undisclosed,Fair (60-69%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.613,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),pixtral
,Pass@1,,,math,MATH,,,2025-07-19T19:56:11.900275+00:00,benchmark_result,,,,True,,,,,,425.0,,pixtral-12b-2409,,,,0.481,,,mistral,,,,,,,,0.481,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:11.900275+00:00,,,,,,False,,False,0.0,False,0.0,0.481,0.481,Unknown,,,,,Undisclosed,Poor (<60%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.481,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,Chain of Thought (CoT),,,mathvista,MathVista,,,2025-07-19T19:56:12.111272+00:00,benchmark_result,,,,True,,,,,,537.0,,pixtral-12b-2409,,,,0.58,,,mistral,,,,,,,,0.58,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.111272+00:00,,,,,,False,,False,0.0,False,0.0,0.58,0.58,Unknown,,,,,Undisclosed,Poor (<60%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.58,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),pixtral
,Multimodal Instruction Following Score,,,mm-if-eval,MM IF-Eval,,,2025-07-19T19:56:15.145578+00:00,benchmark_result,,,,True,,,,,,1822.0,,pixtral-12b-2409,,,,0.527,,,mistral,,,,,,,,0.527,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:15.145578+00:00,,,,,,False,,False,0.0,False,0.0,0.527,0.527,Unknown,,,,,Undisclosed,Poor (<60%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MM IF-Eval,"['multimodal', 'reasoning']",multimodal,False,1.0,en,"A challenging multimodal instruction-following benchmark that includes both compose-level constraints for output responses and perception-level constraints tied to input images, with comprehensive evaluation pipeline.",0.527,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,Multimodal MT-Bench Score,,,mm-mt-bench,MM-MT-Bench,,,2025-07-19T19:56:14.887276+00:00,benchmark_result,,,,True,,,,,,1733.0,,pixtral-12b-2409,,,,0.605,,,mistral,,,,,,,,0.605,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:14.887276+00:00,,,,,,False,,False,0.0,False,0.0,0.605,0.605,Unknown,,,,,Undisclosed,Fair (60-69%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MM-MT-Bench,"['multimodal', 'communication']",multimodal,False,100.0,en,A multi-turn LLM-as-a-judge evaluation benchmark for testing multimodal instruction-tuned models' ability to follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner.,0.00605,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,5-shot,,,mmlu,MMLU,,,2025-07-19T19:56:11.314507+00:00,benchmark_result,,,,True,,,,,,115.0,,pixtral-12b-2409,,,,0.692,,,mistral,,,,,,,,0.692,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:11.314507+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.692,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),pixtral
,Chain of Thought (CoT),,,mmmu,MMMU,,,2025-07-19T19:56:12.209409+00:00,benchmark_result,,,,True,,,,,,588.0,,pixtral-12b-2409,,,,0.525,,,mistral,,,,,,,,0.525,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:12.209409+00:00,,,,,,False,,False,0.0,False,0.0,0.525,0.525,Unknown,,,,,Undisclosed,Poor (<60%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.525,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,Text MT-Bench Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.539185+00:00,benchmark_result,,,,True,,,,,,1614.0,,pixtral-12b-2409,,,,0.768,,,mistral,,,,,,,,0.768,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:14.539185+00:00,,,,,,False,,False,0.0,False,0.0,0.768,0.768,Unknown,,,,,Undisclosed,Good (70-79%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00768,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,VQA Match,,,vqav2,VQAv2,,,2025-07-19T19:56:14.416120+00:00,benchmark_result,,,,True,,,,,,1575.0,,pixtral-12b-2409,,,,0.786,,,mistral,,,,,,,,0.786,https://mistral.ai/news/pixtral-12b/,,,,,,,,2025-07-19T19:56:14.416120+00:00,,,,,,False,,False,0.0,False,0.0,0.786,0.786,Unknown,,,,,Undisclosed,Good (70-79%),Pixtral-12B,mistral,Mistral AI,12400000000.0,12400000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2024-09-17,2024.0,9.0,2024-09,Very Large (>70B),"A 12B parameter multimodal model with a 400M parameter vision encoder, capable of understanding both natural images and documents. Excels at multimodal tasks while maintaining strong text-only performance. Supports variable image sizes and multiple images in context.",VQAv2,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset that addresses language bias by providing complementary images for each question, forcing models to rely on visual understanding rather than language priors. It contains approximately twice the number of image-question pairs compared to the original VQA dataset.",0.786,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),pixtral
,BBox,,,ai2d,AI2D,,,2025-07-19T19:56:13.645378+00:00,benchmark_result,,,,True,,,,,,1261.0,,pixtral-large,,,,0.938,,,mistral,,,,,,,,0.938,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:13.645378+00:00,,,,,,False,,False,0.0,False,0.0,0.938,0.938,Unknown,,,,,Undisclosed,Excellent (90%+),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.938,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),pixtral
,CoT,,,chartqa,ChartQA,,,2025-07-19T19:56:12.820802+00:00,benchmark_result,,,,True,,,,,,873.0,,pixtral-large,,,,0.881,,,mistral,,,,,,,,0.881,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:12.820802+00:00,,,,,,False,,False,0.0,False,0.0,0.881,0.881,Unknown,,,,,Undisclosed,Very Good (80-89%),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.881,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),pixtral
,ANLS,,,docvqa,DocVQA,,,2025-07-19T19:56:12.869454+00:00,benchmark_result,,,,True,,,,,,898.0,,pixtral-large,,,,0.933,,,mistral,,,,,,,,0.933,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:12.869454+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.933,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),pixtral
,CoT,,,mathvista,MathVista,,,2025-07-19T19:56:12.109764+00:00,benchmark_result,,,,True,,,,,,536.0,,pixtral-large,,,,0.694,,,mistral,,,,,,,,0.694,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:12.109764+00:00,,,,,,False,,False,0.0,False,0.0,0.694,0.694,Unknown,,,,,Undisclosed,Fair (60-69%),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.694,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),pixtral
,GPT-4o Judge,,,mm-mt-bench,MM-MT-Bench,,,2025-07-19T19:56:14.885715+00:00,benchmark_result,,,,True,,,,,,1732.0,,pixtral-large,,,,0.74,,,mistral,,,,,,,,0.74,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:14.885715+00:00,,,,,,False,,False,0.0,False,0.0,0.74,0.74,Unknown,,,,,Undisclosed,Good (70-79%),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",MM-MT-Bench,"['multimodal', 'communication']",multimodal,False,100.0,en,A multi-turn LLM-as-a-judge evaluation benchmark for testing multimodal instruction-tuned models' ability to follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner.,0.0074,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),pixtral
,CoT,,,mmmu,MMMU,,,2025-07-19T19:56:12.205240+00:00,benchmark_result,,,,True,,,,,,586.0,,pixtral-large,,,,0.64,,,mistral,,,,,,,,0.64,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:12.205240+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.64,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),pixtral
,VQA Match,,,vqav2,VQAv2,,,2025-07-19T19:56:14.414450+00:00,benchmark_result,,,,True,,,,,,1574.0,,pixtral-large,,,,0.809,,,mistral,,,,,,,,0.809,https://mistral.ai/news/pixtral-large/,,,,,,,,2025-07-19T19:56:14.414450+00:00,,,,,,False,,False,0.0,False,0.0,0.809,0.809,Unknown,,,,,Undisclosed,Very Good (80-89%),Pixtral Large,mistral,Mistral AI,124000000000.0,124000000000.0,True,0.0,False,True,mistral_research_license_(mrl)_for_research;_mistral_commercial_license_for_commercial_use,Restricted/Community,2024-11-18,2024.0,11.0,2024-11,Very Large (>70B),"A 124B parameter multimodal model built on top of Mistral Large 2, featuring frontier-level image understanding capabilities. Excels at understanding documents, charts, and natural images while maintaining strong text-only performance. Features a 123B multimodal decoder and 1B parameter vision encoder with a 128K context window supporting up to 30 high-resolution images.",VQAv2,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"VQAv2 is a balanced Visual Question Answering dataset that addresses language bias by providing complementary images for each question, forcing models to rely on visual understanding rather than language priors. It contains approximately twice the number of image-question pairs compared to the original VQA dataset.",0.809,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),pixtral
,full,,,mathvision,MathVision,,,2025-07-19T19:56:14.700746+00:00,benchmark_result,,,,True,,,,,,1675.0,,qvq-72b-preview,,,,0.359,,,qwen,,,,,,,,0.359,https://huggingface.co/Qwen/QVQ-72B-Preview,,,,,,,,2025-07-19T19:56:14.700746+00:00,,,,,,False,,False,0.0,False,0.0,0.359,0.359,Unknown,,,,,Undisclosed,Poor (<60%),QvQ-72B-Preview,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,qwen,Restricted/Community,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops.",MathVision,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",0.359,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qvq
,mini,,,mathvista,MathVista,,,2025-07-19T19:56:12.092107+00:00,benchmark_result,,,,True,,,,,,526.0,,qvq-72b-preview,,,,0.714,,,qwen,,,,,,,,0.714,https://huggingface.co/Qwen/QVQ-72B-Preview,,,,,,,,2025-07-19T19:56:12.092107+00:00,,,,,,False,,False,0.0,False,0.0,0.714,0.714,Unknown,,,,,Undisclosed,Good (70-79%),QvQ-72B-Preview,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,qwen,Restricted/Community,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.714,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qvq
,val,,,mmmu,MMMU,,,2025-07-19T19:56:12.173084+00:00,benchmark_result,,,,True,,,,,,570.0,,qvq-72b-preview,,,,0.703,,,qwen,,,,,,,,0.703,https://huggingface.co/Qwen/QVQ-72B-Preview,,,,,,,,2025-07-19T19:56:12.173084+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),QvQ-72B-Preview,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,qwen,Restricted/Community,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.703,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qvq
,full,,,olympiadbench,OlympiadBench,,,2025-07-19T19:56:14.824642+00:00,benchmark_result,,,,True,,,,,,1716.0,,qvq-72b-preview,,,,0.204,,,qwen,,,,,,,,0.204,https://huggingface.co/Qwen/QVQ-72B-Preview,,,,,,,,2025-07-19T19:56:14.824642+00:00,,,,,,False,,False,0.0,False,0.0,0.204,0.204,Unknown,,,,,Undisclosed,Poor (<60%),QvQ-72B-Preview,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,qwen,Restricted/Community,2024-12-25,2024.0,12.0,2024-12,Very Large (>70B),"An experimental research model focusing on advanced visual reasoning and step-by-step cognitive capabilities. Achieves strong performance on multi-modal science and mathematics tasks, though exhibits some limitations such as potential language mixing and recursive reasoning loops.",OlympiadBench,"['math', 'reasoning', 'physics', 'multimodal']",multimodal,True,1.0,en,"A challenging benchmark for promoting AGI with Olympiad-level bilingual multimodal scientific problems. Comprises 8,476 math and physics problems from international and Chinese Olympiads and the Chinese college entrance exam, featuring expert-level annotations for step-by-step reasoning. Includes both text-only and multimodal problems in English and Chinese.",0.204,False,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qvq
,ARC-C benchmark evaluation,,,arc-c,ARC-C,,,2025-07-19T19:56:11.127541+00:00,benchmark_result,,,,True,,,,,,21.0,,qwen-2.5-14b-instruct,,,,0.673,,,qwen,,,,,,,,0.673,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.127541+00:00,,,,,,False,,False,0.0,False,0.0,0.673,0.673,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.673,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,BBH benchmark evaluation,,,bbh,BBH,,,2025-07-19T19:56:13.042167+00:00,benchmark_result,,,,True,,,,,,971.0,,qwen-2.5-14b-instruct,,,,0.782,,,qwen,,,,,,,,0.782,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.042167+00:00,,,,,,False,,False,0.0,False,0.0,0.782,0.782,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.782,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,GPQA benchmark evaluation,,,gpqa,GPQA,,,2025-07-19T19:56:11.677954+00:00,benchmark_result,,,,True,,,,,,301.0,,qwen-2.5-14b-instruct,,,,0.455,,,qwen,,,,,,,,0.455,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.677954+00:00,,,,,,False,,False,0.0,False,0.0,0.455,0.455,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.455,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,GSM8K benchmark evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.082212+00:00,benchmark_result,,,,True,,,,,,994.0,,qwen-2.5-14b-instruct,,,,0.948,,,qwen,,,,,,,,0.948,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.082212+00:00,,,,,,False,,False,0.0,False,0.0,0.948,0.948,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.948,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen
,HumanEval benchmark evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.646500+00:00,benchmark_result,,,,True,,,,,,786.0,,qwen-2.5-14b-instruct,,,,0.835,,,qwen,,,,,,,,0.835,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.646500+00:00,,,,,,False,,False,0.0,False,0.0,0.835,0.835,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.835,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,HumanEval+ benchmark evaluation,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.071967+00:00,benchmark_result,,,,True,,,,,,1441.0,,qwen-2.5-14b-instruct,,,,0.512,,,qwen,,,,,,,,0.512,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.071967+00:00,,,,,,False,,False,0.0,False,0.0,0.512,0.512,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.512,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MATH benchmark evaluation,,,math,MATH,,,2025-07-19T19:56:11.862254+00:00,benchmark_result,,,,True,,,,,,404.0,,qwen-2.5-14b-instruct,,,,0.8,,,qwen,,,,,,,,0.8,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.862254+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.8,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MBPP benchmark evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.497488+00:00,benchmark_result,,,,True,,,,,,1185.0,,qwen-2.5-14b-instruct,,,,0.82,,,qwen,,,,,,,,0.82,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.497488+00:00,,,,,,False,,False,0.0,False,0.0,0.82,0.82,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.008199999999999999,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MBPP+ benchmark evaluation,,,mbpp+,MBPP+,,,2025-07-19T19:56:14.507421+00:00,benchmark_result,,,,True,,,,,,1602.0,,qwen-2.5-14b-instruct,,,,0.632,,,qwen,,,,,,,,0.632,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.507421+00:00,,,,,,False,,False,0.0,False,0.0,0.632,0.632,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MBPP+,"['reasoning', 'general']",text,False,1.0,en,"MBPP+ is an enhanced version of MBPP (Mostly Basic Python Problems) with significantly more test cases (35x) for more rigorous evaluation. MBPP is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality.",0.632,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,MMLU benchmark evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.269091+00:00,benchmark_result,,,,True,,,,,,89.0,,qwen-2.5-14b-instruct,,,,0.797,,,qwen,,,,,,,,0.797,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.269091+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.797,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,MMLU-Pro benchmark evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.471047+00:00,benchmark_result,,,,True,,,,,,194.0,,qwen-2.5-14b-instruct,,,,0.637,,,qwen,,,,,,,,0.637,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.471047+00:00,,,,,,False,,False,0.0,False,0.0,0.637,0.637,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.637,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,MMLU-redux benchmark evaluation,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.538944+00:00,benchmark_result,,,,True,,,,,,731.0,,qwen-2.5-14b-instruct,,,,0.8,,,qwen,,,,,,,,0.8,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.538944+00:00,,,,,,False,,False,0.0,False,0.0,0.8,0.8,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.8,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MMLU-STEM benchmark evaluation,,,mmlu-stem,MMLU-STEM,,,2025-07-19T19:56:14.500528+00:00,benchmark_result,,,,True,,,,,,1600.0,,qwen-2.5-14b-instruct,,,,0.764,,,qwen,,,,,,,,0.764,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.500528+00:00,,,,,,False,,False,0.0,False,0.0,0.764,0.764,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU-STEM,"['math', 'reasoning', 'physics', 'chemistry']",text,False,1.0,en,"STEM-focused subset of the Massive Multitask Language Understanding benchmark, evaluating language models on science, technology, engineering, and mathematics topics including physics, chemistry, mathematics, and other technical subjects.",0.764,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,MultiPL-E benchmark evaluation,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.319213+00:00,benchmark_result,,,,True,,,,,,642.0,,qwen-2.5-14b-instruct,,,,0.728,,,qwen,,,,,,,,0.728,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.319213+00:00,,,,,,False,,False,0.0,False,0.0,0.728,0.728,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.728,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,TheoremQA benchmark evaluation,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.492163+00:00,benchmark_result,,,,True,,,,,,1597.0,,qwen-2.5-14b-instruct,,,,0.43,,,qwen,,,,,,,,0.43,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.492163+00:00,,,,,,False,,False,0.0,False,0.0,0.43,0.43,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.43,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,TruthfulQA benchmark evaluation,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.355004+00:00,benchmark_result,,,,True,,,,,,138.0,,qwen-2.5-14b-instruct,,,,0.584,,,qwen,,,,,,,,0.584,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.355004+00:00,,,,,,False,,False,0.0,False,0.0,0.584,0.584,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 14B Instruct,qwen,Alibaba Cloud / Qwen Team,14700000000.0,14700000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-14B-Instruct is an instruction-tuned 14.7B parameter language model, part of the Qwen2.5 series. It features significant improvements in instruction following, long text generation (8K+ tokens), structured data understanding, and JSON output generation. The model supports a 128K token context length and multilingual capabilities across 29+ languages including Chinese, English, French, Spanish, and more.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.584,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,ARC-C benchmark evaluation,,,arc-c,ARC-C,,,2025-07-19T19:56:11.121747+00:00,benchmark_result,,,,True,,,,,,18.0,,qwen-2.5-32b-instruct,,,,0.704,,,qwen,,,,,,,,0.704,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.121747+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.704,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,BBH benchmark evaluation,,,bbh,BBH,,,2025-07-19T19:56:13.040428+00:00,benchmark_result,,,,True,,,,,,970.0,,qwen-2.5-32b-instruct,,,,0.845,,,qwen,,,,,,,,0.845,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.040428+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.845,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,GPQA benchmark evaluation,,,gpqa,GPQA,,,2025-07-19T19:56:11.671178+00:00,benchmark_result,,,,True,,,,,,297.0,,qwen-2.5-32b-instruct,,,,0.495,,,qwen,,,,,,,,0.495,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.671178+00:00,,,,,,False,,False,0.0,False,0.0,0.495,0.495,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.495,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,GSM8K benchmark evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.074870+00:00,benchmark_result,,,,True,,,,,,990.0,,qwen-2.5-32b-instruct,,,,0.959,,,qwen,,,,,,,,0.959,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.074870+00:00,,,,,,False,,False,0.0,False,0.0,0.959,0.959,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.959,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),qwen
,HellaSwag benchmark evaluation,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.178158+00:00,benchmark_result,,,,True,,,,,,45.0,,qwen-2.5-32b-instruct,,,,0.852,,,qwen,,,,,,,,0.852,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.178158+00:00,,,,,,False,,False,0.0,False,0.0,0.852,0.852,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.852,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,HumanEval benchmark evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.639922+00:00,benchmark_result,,,,True,,,,,,782.0,,qwen-2.5-32b-instruct,,,,0.884,,,qwen,,,,,,,,0.884,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.639922+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.884,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,HumanEval+ benchmark evaluation,,,humaneval+,HumanEval+,,,2025-07-19T19:56:14.070409+00:00,benchmark_result,,,,True,,,,,,1440.0,,qwen-2.5-32b-instruct,,,,0.524,,,qwen,,,,,,,,0.524,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.070409+00:00,,,,,,False,,False,0.0,False,0.0,0.524,0.524,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",HumanEval+,['reasoning'],text,False,1.0,en,"Enhanced version of HumanEval that extends the original test cases by 80x using EvalPlus framework for rigorous evaluation of LLM-synthesized code functional correctness, detecting previously undetected wrong code",0.524,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MATH benchmark evaluation,,,math,MATH,,,2025-07-19T19:56:11.856115+00:00,benchmark_result,,,,True,,,,,,400.0,,qwen-2.5-32b-instruct,,,,0.831,,,qwen,,,,,,,,0.831,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.856115+00:00,,,,,,False,,False,0.0,False,0.0,0.831,0.831,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.831,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MBPP benchmark evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.489427+00:00,benchmark_result,,,,True,,,,,,1181.0,,qwen-2.5-32b-instruct,,,,0.84,,,qwen,,,,,,,,0.84,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.489427+00:00,,,,,,False,,False,0.0,False,0.0,0.84,0.84,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0084,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MBPP+ benchmark evaluation,,,mbpp+,MBPP+,,,2025-07-19T19:56:14.504915+00:00,benchmark_result,,,,True,,,,,,1601.0,,qwen-2.5-32b-instruct,,,,0.672,,,qwen,,,,,,,,0.672,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.504915+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MBPP+,"['reasoning', 'general']",text,False,1.0,en,"MBPP+ is an enhanced version of MBPP (Mostly Basic Python Problems) with significantly more test cases (35x) for more rigorous evaluation. MBPP is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers, covering programming fundamentals and standard library functionality.",0.672,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,MMLU benchmark evaluation,,,mmlu,MMLU,,,2025-07-19T19:56:11.261705+00:00,benchmark_result,,,,True,,,,,,85.0,,qwen-2.5-32b-instruct,,,,0.833,,,qwen,,,,,,,,0.833,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.261705+00:00,,,,,,False,,False,0.0,False,0.0,0.833,0.833,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.833,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MMLU-Pro benchmark evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.465052+00:00,benchmark_result,,,,True,,,,,,190.0,,qwen-2.5-32b-instruct,,,,0.69,,,qwen,,,,,,,,0.69,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.465052+00:00,,,,,,False,,False,0.0,False,0.0,0.69,0.69,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.69,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,MMLU-redux benchmark evaluation,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.533630+00:00,benchmark_result,,,,True,,,,,,728.0,,qwen-2.5-32b-instruct,,,,0.839,,,qwen,,,,,,,,0.839,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.533630+00:00,,,,,,False,,False,0.0,False,0.0,0.839,0.839,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.839,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MMLU-STEM benchmark evaluation,,,mmlu-stem,MMLU-STEM,,,2025-07-19T19:56:14.498255+00:00,benchmark_result,,,,True,,,,,,1599.0,,qwen-2.5-32b-instruct,,,,0.809,,,qwen,,,,,,,,0.809,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.498255+00:00,,,,,,False,,False,0.0,False,0.0,0.809,0.809,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU-STEM,"['math', 'reasoning', 'physics', 'chemistry']",text,False,1.0,en,"STEM-focused subset of the Massive Multitask Language Understanding benchmark, evaluating language models on science, technology, engineering, and mathematics topics including physics, chemistry, mathematics, and other technical subjects.",0.809,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MultiPL-E benchmark evaluation,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.316384+00:00,benchmark_result,,,,True,,,,,,640.0,,qwen-2.5-32b-instruct,,,,0.754,,,qwen,,,,,,,,0.754,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.316384+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.754,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,TheoremQA benchmark evaluation,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.482526+00:00,benchmark_result,,,,True,,,,,,1593.0,,qwen-2.5-32b-instruct,,,,0.441,,,qwen,,,,,,,,0.441,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.482526+00:00,,,,,,False,,False,0.0,False,0.0,0.441,0.441,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.441,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,TruthfulQA benchmark evaluation,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.349397+00:00,benchmark_result,,,,True,,,,,,135.0,,qwen-2.5-32b-instruct,,,,0.578,,,qwen,,,,,,,,0.578,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.349397+00:00,,,,,,False,,False,0.0,False,0.0,0.578,0.578,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.578,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,Winogrande benchmark evaluation,,,winogrande,Winogrande,,,2025-07-19T19:56:11.384431+00:00,benchmark_result,,,,True,,,,,,150.0,,qwen-2.5-32b-instruct,,,,0.82,,,qwen,,,,,,,,0.82,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.384431+00:00,,,,,,False,,False,0.0,False,0.0,0.82,0.82,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-32B-Instruct is an instruction-tuned 32 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.82,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,AlignBench v1.1 benchmark evaluation,,,alignbench,AlignBench,,,2025-07-19T19:56:14.546122+00:00,benchmark_result,,,,True,,,,,,1617.0,,qwen-2.5-72b-instruct,,,,0.816,,,qwen,,,,,,,,0.816,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:14.546122+00:00,,,,,,False,,False,0.0,False,0.0,0.816,0.816,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",AlignBench,"['general', 'language', 'math', 'reasoning', 'roleplay']",text,True,1.0,en,"AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",0.816,True,False,True,True,True,False,False,False,True,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,Arena Hard benchmark evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.097075+00:00,benchmark_result,,,,True,,,,,,1453.0,,qwen-2.5-72b-instruct,,,,0.812,,,qwen,,,,,,,,0.812,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:14.097075+00:00,,,,,,False,,False,0.0,False,0.0,0.812,0.812,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.812,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,GPQA benchmark evaluation,,,gpqa,GPQA,,,2025-07-19T19:56:11.681073+00:00,benchmark_result,,,,True,,,,,,303.0,,qwen-2.5-72b-instruct,,,,0.49,,,qwen,,,,,,,,0.49,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:11.681073+00:00,,,,,,False,,False,0.0,False,0.0,0.49,0.49,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.49,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,GSM8K benchmark evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.085236+00:00,benchmark_result,,,,True,,,,,,996.0,,qwen-2.5-72b-instruct,,,,0.958,,,qwen,,,,,,,,0.958,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:13.085236+00:00,,,,,,False,,False,0.0,False,0.0,0.958,0.958,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.958,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),qwen
,HumanEval benchmark evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.648406+00:00,benchmark_result,,,,True,,,,,,787.0,,qwen-2.5-72b-instruct,,,,0.866,,,qwen,,,,,,,,0.866,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:12.648406+00:00,,,,,,False,,False,0.0,False,0.0,0.866,0.866,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.866,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,IFEval strict-prompt benchmark evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.277303+00:00,benchmark_result,,,,True,,,,,,620.0,,qwen-2.5-72b-instruct,,,,0.841,,,qwen,,,,,,,,0.841,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:12.277303+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.841,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,LiveBench benchmark evaluation,,,livebench,LiveBench,,,2025-07-19T19:56:12.577555+00:00,benchmark_result,,,,True,,,,,,750.0,,qwen-2.5-72b-instruct,,,,0.523,,,qwen,,,,,,,,0.523,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:12.577555+00:00,,,,,,False,,False,0.0,False,0.0,0.523,0.523,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.523,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,LiveCodeBench benchmark evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.346315+00:00,benchmark_result,,,,True,,,,,,1124.0,,qwen-2.5-72b-instruct,,,,0.555,,,qwen,,,,,,,,0.555,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:13.346315+00:00,,,,,,False,,False,0.0,False,0.0,0.555,0.555,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.555,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MATH benchmark evaluation,,,math,MATH,,,2025-07-19T19:56:11.865721+00:00,benchmark_result,,,,True,,,,,,406.0,,qwen-2.5-72b-instruct,,,,0.831,,,qwen,,,,,,,,0.831,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:11.865721+00:00,,,,,,False,,False,0.0,False,0.0,0.831,0.831,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.831,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MBPP benchmark evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.503069+00:00,benchmark_result,,,,True,,,,,,1187.0,,qwen-2.5-72b-instruct,,,,0.882,,,qwen,,,,,,,,0.882,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:13.503069+00:00,,,,,,False,,False,0.0,False,0.0,0.882,0.882,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00882,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MMLU-Pro benchmark evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.475182+00:00,benchmark_result,,,,True,,,,,,196.0,,qwen-2.5-72b-instruct,,,,0.711,,,qwen,,,,,,,,0.711,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.475182+00:00,,,,,,False,,False,0.0,False,0.0,0.711,0.711,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.711,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,MMLU-redux benchmark evaluation,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.542364+00:00,benchmark_result,,,,True,,,,,,733.0,,qwen-2.5-72b-instruct,,,,0.868,,,qwen,,,,,,,,0.868,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:12.542364+00:00,,,,,,False,,False,0.0,False,0.0,0.868,0.868,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.868,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,MT-bench benchmark evaluation,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.521232+00:00,benchmark_result,,,,True,,,,,,1606.0,,qwen-2.5-72b-instruct,,,,0.935,,,qwen,,,,,,,,0.935,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:14.521232+00:00,,,,,,False,,False,0.0,False,0.0,0.935,0.935,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00935,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MultiPL-E benchmark evaluation,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.322800+00:00,benchmark_result,,,,True,,,,,,644.0,,qwen-2.5-72b-instruct,,,,0.751,,,qwen,,,,,,,,0.751,https://qwenlm.github.io/blog/qwen2.5/,,,,,,,,2025-07-19T19:56:12.322800+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72700000000.0,72700000000.0,True,18000000000000.0,True,False,qwen,Restricted/Community,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-72B-Instruct is an instruction-tuned 72 billion parameter language model, part of the Qwen2.5 series. It is designed to follow instructions, generate long texts (over 8K tokens), understand structured data (e.g., tables), and generate structured outputs, especially JSON. The model supports multilingual capabilities across over 29 languages.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.751,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,AlignBench v1.1 benchmark evaluation,,,alignbench,AlignBench,,,2025-07-19T19:56:14.548680+00:00,benchmark_result,,,,True,,,,,,1618.0,,qwen-2.5-7b-instruct,,,,0.733,,,qwen,,,,,,,,0.733,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.548680+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",AlignBench,"['general', 'language', 'math', 'reasoning', 'roleplay']",text,True,1.0,en,"AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",0.733,True,False,True,True,True,False,False,False,True,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,Arena Hard benchmark evaluation,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.100766+00:00,benchmark_result,,,,True,,,,,,1455.0,,qwen-2.5-7b-instruct,,,,0.52,,,qwen,,,,,,,,0.52,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.100766+00:00,,,,,,False,,False,0.0,False,0.0,0.52,0.52,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.52,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,GPQA benchmark evaluation,,,gpqa,GPQA,,,2025-07-19T19:56:11.685965+00:00,benchmark_result,,,,True,,,,,,306.0,,qwen-2.5-7b-instruct,,,,0.364,,,qwen,,,,,,,,0.364,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.685965+00:00,,,,,,False,,False,0.0,False,0.0,0.364,0.364,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.364,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,GSM8K benchmark evaluation,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.088027+00:00,benchmark_result,,,,True,,,,,,998.0,,qwen-2.5-7b-instruct,,,,0.916,,,qwen,,,,,,,,0.916,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.088027+00:00,,,,,,False,,False,0.0,False,0.0,0.916,0.916,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.916,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen
,HumanEval benchmark evaluation,,,humaneval,HumanEval,,,2025-07-19T19:56:12.651744+00:00,benchmark_result,,,,True,,,,,,789.0,,qwen-2.5-7b-instruct,,,,0.848,,,qwen,,,,,,,,0.848,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.651744+00:00,,,,,,False,,False,0.0,False,0.0,0.848,0.848,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.848,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,IFEval strict-prompt benchmark evaluation,,,ifeval,IFEval,,,2025-07-19T19:56:12.278867+00:00,benchmark_result,,,,True,,,,,,621.0,,qwen-2.5-7b-instruct,,,,0.712,,,qwen,,,,,,,,0.712,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.278867+00:00,,,,,,False,,False,0.0,False,0.0,0.712,0.712,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.712,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,LiveBench 0831 benchmark evaluation,,,livebench,LiveBench,,,2025-07-19T19:56:12.584018+00:00,benchmark_result,,,,True,,,,,,753.0,,qwen-2.5-7b-instruct,,,,0.359,,,qwen,,,,,,,,0.359,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.584018+00:00,,,,,,False,,False,0.0,False,0.0,0.359,0.359,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.359,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,LiveCodeBench 2305-2409 benchmark evaluation,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.352497+00:00,benchmark_result,,,,True,,,,,,1126.0,,qwen-2.5-7b-instruct,,,,0.287,,,qwen,,,,,,,,0.287,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.352497+00:00,,,,,,False,,False,0.0,False,0.0,0.287,0.287,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.287,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MATH benchmark evaluation,,,math,MATH,,,2025-07-19T19:56:11.869960+00:00,benchmark_result,,,,True,,,,,,408.0,,qwen-2.5-7b-instruct,,,,0.755,,,qwen,,,,,,,,0.755,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.869960+00:00,,,,,,False,,False,0.0,False,0.0,0.755,0.755,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.755,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,MBPP benchmark evaluation,,,mbpp,MBPP,,,2025-07-19T19:56:13.506947+00:00,benchmark_result,,,,True,,,,,,1189.0,,qwen-2.5-7b-instruct,,,,0.792,,,qwen,,,,,,,,0.792,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:13.506947+00:00,,,,,,False,,False,0.0,False,0.0,0.792,0.792,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00792,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MMLU-Pro benchmark evaluation,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.479104+00:00,benchmark_result,,,,True,,,,,,198.0,,qwen-2.5-7b-instruct,,,,0.563,,,qwen,,,,,,,,0.563,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:11.479104+00:00,,,,,,False,,False,0.0,False,0.0,0.563,0.563,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.563,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MMLU-redux benchmark evaluation,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.545338+00:00,benchmark_result,,,,True,,,,,,735.0,,qwen-2.5-7b-instruct,,,,0.754,,,qwen,,,,,,,,0.754,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.545338+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.754,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,MT-bench benchmark evaluation,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.523567+00:00,benchmark_result,,,,True,,,,,,1607.0,,qwen-2.5-7b-instruct,,,,0.875,,,qwen,,,,,,,,0.875,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:14.523567+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.00875,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,MultiPL-E benchmark evaluation,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.325846+00:00,benchmark_result,,,,True,,,,,,646.0,,qwen-2.5-7b-instruct,,,,0.704,,,qwen,,,,,,,,0.704,https://qwenlm.github.io/blog/qwen2.5-llm/,,,,,,,,2025-07-19T19:56:12.325846+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7610000000.0,7610000000.0,True,18000000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-7B-Instruct is an instruction-tuned 7B parameter language model that excels at following instructions, generating long texts (over 8K tokens), understanding structured data, and generating structured outputs like JSON. The model features enhanced capabilities in mathematics, coding, and multilingual support across 29+ languages including Chinese, English, French, Spanish, and more.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.704,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.123905+00:00,benchmark_result,,,,True,,,,,,19.0,,qwen-2.5-coder-32b-instruct,,,,0.705,,,qwen,,,,,,,,0.705,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.123905+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.705,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,accuracy,,,bigcodebench-full,BigCodeBench-Full,,,2025-07-19T19:56:14.511653+00:00,benchmark_result,,,,True,,,,,,1603.0,,qwen-2.5-coder-32b-instruct,,,,0.496,,,qwen,,,,,,,,0.496,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.511653+00:00,,,,,,False,,False,0.0,False,0.0,0.496,0.496,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",BigCodeBench-Full,"['general', 'reasoning']",text,False,1.0,en,"A comprehensive benchmark that evaluates large language models' ability to solve complex, practical programming tasks via code generation. Contains 1,140 fine-grained tasks across 7 domains using function calls from 139 libraries. Challenges LLMs to invoke multiple function calls as tools and handle complex instructions for realistic software engineering and general-purpose reasoning tasks.",0.496,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,bigcodebench-hard,BigCodeBench-Hard,,,2025-07-19T19:56:14.515099+00:00,benchmark_result,,,,True,,,,,,1604.0,,qwen-2.5-coder-32b-instruct,,,,0.27,,,qwen,,,,,,,,0.27,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.515099+00:00,,,,,,False,,False,0.0,False,0.0,0.27,0.27,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",BigCodeBench-Hard,"['general', 'reasoning']",text,False,1.0,en,"BigCodeBench-Hard is a subset of 148 challenging programming tasks from BigCodeBench, designed to evaluate large language models' ability to solve complex, real-world programming problems. These tasks require diverse function calls from multiple libraries across 7 domains including computation, networking, data analysis, and visualization. The benchmark tests compositional reasoning and the ability to implement complex instructions that span 139 libraries with an average of 2.8 libraries per task.",0.27,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.076453+00:00,benchmark_result,,,,True,,,,,,991.0,,qwen-2.5-coder-32b-instruct,,,,0.911,,,qwen,,,,,,,,0.911,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.076453+00:00,,,,,,False,,False,0.0,False,0.0,0.911,0.911,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.911,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen
,accuracy,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.180700+00:00,benchmark_result,,,,True,,,,,,46.0,,qwen-2.5-coder-32b-instruct,,,,0.83,,,qwen,,,,,,,,0.83,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.180700+00:00,,,,,,False,,False,0.0,False,0.0,0.83,0.83,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.83,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.641672+00:00,benchmark_result,,,,True,,,,,,783.0,,qwen-2.5-coder-32b-instruct,,,,0.927,,,qwen,,,,,,,,0.927,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:12.641672+00:00,,,,,,False,,False,0.0,False,0.0,0.927,0.927,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.927,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen
,pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.329968+00:00,benchmark_result,,,,True,,,,,,1117.0,,qwen-2.5-coder-32b-instruct,,,,0.314,,,qwen,,,,,,,,0.314,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.329968+00:00,,,,,,False,,False,0.0,False,0.0,0.314,0.314,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.314,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,math,MATH,,,2025-07-19T19:56:11.857514+00:00,benchmark_result,,,,True,,,,,,401.0,,qwen-2.5-coder-32b-instruct,,,,0.572,,,qwen,,,,,,,,0.572,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.857514+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.572,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.491369+00:00,benchmark_result,,,,True,,,,,,1182.0,,qwen-2.5-coder-32b-instruct,,,,0.902,,,qwen,,,,,,,,0.902,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.491369+00:00,,,,,,False,,False,0.0,False,0.0,0.902,0.902,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00902,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.263438+00:00,benchmark_result,,,,True,,,,,,86.0,,qwen-2.5-coder-32b-instruct,,,,0.751,,,qwen,,,,,,,,0.751,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.263438+00:00,,,,,,False,,False,0.0,False,0.0,0.751,0.751,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.751,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.466410+00:00,benchmark_result,,,,True,,,,,,191.0,,qwen-2.5-coder-32b-instruct,,,,0.504,,,qwen,,,,,,,,0.504,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.466410+00:00,,,,,,False,,False,0.0,False,0.0,0.504,0.504,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.504,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.535302+00:00,benchmark_result,,,,True,,,,,,729.0,,qwen-2.5-coder-32b-instruct,,,,0.775,,,qwen,,,,,,,,0.775,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:12.535302+00:00,,,,,,False,,False,0.0,False,0.0,0.775,0.775,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.775,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,accuracy,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.485084+00:00,benchmark_result,,,,True,,,,,,1594.0,,qwen-2.5-coder-32b-instruct,,,,0.431,,,qwen,,,,,,,,0.431,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.485084+00:00,,,,,,False,,False,0.0,False,0.0,0.431,0.431,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.431,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.351250+00:00,benchmark_result,,,,True,,,,,,136.0,,qwen-2.5-coder-32b-instruct,,,,0.542,,,qwen,,,,,,,,0.542,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.351250+00:00,,,,,,False,,False,0.0,False,0.0,0.542,0.542,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.542,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,winogrande,Winogrande,,,2025-07-19T19:56:13.219435+00:00,benchmark_result,,,,True,,,,,,1064.0,,qwen-2.5-coder-32b-instruct,,,,0.808,,,qwen,,,,,,,,0.808,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.219435+00:00,,,,,,False,,False,0.0,False,0.0,0.808,0.808,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Coder 32B Instruct,qwen,Alibaba Cloud / Qwen Team,32000000000.0,32000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, repair, and multi-programming tasks while maintaining strong performance in mathematics and general capabilities.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.808,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,pass@1,,,aider,Aider,,,2025-07-19T19:56:14.569369+00:00,benchmark_result,,,,True,,,,,,1624.0,,qwen-2.5-coder-7b-instruct,,,,0.556,,,qwen,,,,,,,,0.556,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.569369+00:00,,,,,,False,,False,0.0,False,0.0,0.556,0.556,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",Aider,"['reasoning', 'code']",text,False,1.0,en,"Aider is a comprehensive code editing benchmark based on 133 practice exercises from Exercism's Python repository, designed to evaluate AI models' ability to translate natural language coding requests into executable code that passes unit tests. The benchmark measures end-to-end code editing capabilities, including GPT's ability to edit existing code and format code changes for automated saving to local files. The Aider Polyglot variant extends this evaluation across 225 challenging exercises spanning C++, Go, Java, JavaScript, Python, and Rust, making it a standard benchmark for assessing multilingual code editing performance in AI research.",0.556,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.126002+00:00,benchmark_result,,,,True,,,,,,20.0,,qwen-2.5-coder-7b-instruct,,,,0.609,,,qwen,,,,,,,,0.609,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.126002+00:00,,,,,,False,,False,0.0,False,0.0,0.609,0.609,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.609,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,accuracy,,,bigcodebench,BigCodeBench,,,2025-07-19T19:56:14.052666+00:00,benchmark_result,,,,True,,,,,,1434.0,,qwen-2.5-coder-7b-instruct,,,,0.41,,,qwen,,,,,,,,0.41,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.052666+00:00,,,,,,False,,False,0.0,False,0.0,0.41,0.41,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",BigCodeBench,"['general', 'reasoning']",text,False,1.0,en,"A benchmark that challenges LLMs to invoke multiple function calls as tools from 139 libraries and 7 domains for 1,140 fine-grained programming tasks. Evaluates code generation with diverse function calls and complex instructions, featuring two variants: Complete (code completion based on comprehensive docstrings) and Instruct (generating code from natural language instructions).",0.41,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,cruxeval-input-cot,CRUXEval-Input-CoT,,,2025-07-19T19:56:14.554528+00:00,benchmark_result,,,,True,,,,,,1620.0,,qwen-2.5-coder-7b-instruct,,,,0.565,,,qwen,,,,,,,,0.565,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.554528+00:00,,,,,,False,,False,0.0,False,0.0,0.565,0.565,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",CRUXEval-Input-CoT,['reasoning'],text,False,1.0,en,"CRUXEval input prediction task with Chain of Thought (CoT) prompting. Part of the CRUXEval benchmark for code reasoning, understanding, and execution evaluation. Given a Python function and its expected output, the task is to predict the appropriate input using chain-of-thought reasoning. Consists of 800 Python functions (3-13 lines) designed to evaluate code comprehension and reasoning capabilities.",0.565,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,cruxeval-output-cot,CRUXEval-Output-CoT,,,2025-07-19T19:56:14.558251+00:00,benchmark_result,,,,True,,,,,,1621.0,,qwen-2.5-coder-7b-instruct,,,,0.56,,,qwen,,,,,,,,0.56,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.558251+00:00,,,,,,False,,False,0.0,False,0.0,0.56,0.56,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",CRUXEval-Output-CoT,['reasoning'],text,False,1.0,en,"CRUXEval-O (output prediction) with Chain-of-Thought prompting. Part of the CRUXEval benchmark consisting of 800 Python functions (3-13 lines) designed to evaluate code reasoning, understanding, and execution capabilities. The output prediction task requires models to predict the output of a given Python function with specific inputs, evaluated using chain-of-thought reasoning methodology.",0.56,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.080381+00:00,benchmark_result,,,,True,,,,,,993.0,,qwen-2.5-coder-7b-instruct,,,,0.839,,,qwen,,,,,,,,0.839,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.080381+00:00,,,,,,False,,False,0.0,False,0.0,0.839,0.839,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.839,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,accuracy,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.182466+00:00,benchmark_result,,,,True,,,,,,47.0,,qwen-2.5-coder-7b-instruct,,,,0.768,,,qwen,,,,,,,,0.768,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.182466+00:00,,,,,,False,,False,0.0,False,0.0,0.768,0.768,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.768,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.644936+00:00,benchmark_result,,,,True,,,,,,785.0,,qwen-2.5-coder-7b-instruct,,,,0.884,,,qwen,,,,,,,,0.884,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:12.644936+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.884,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen
,pass@1,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.340042+00:00,benchmark_result,,,,True,,,,,,1121.0,,qwen-2.5-coder-7b-instruct,,,,0.182,,,qwen,,,,,,,,0.182,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.340042+00:00,,,,,,False,,False,0.0,False,0.0,0.182,0.182,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.182,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,math,MATH,,,2025-07-19T19:56:11.860821+00:00,benchmark_result,,,,True,,,,,,403.0,,qwen-2.5-coder-7b-instruct,,,,0.466,,,qwen,,,,,,,,0.466,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.860821+00:00,,,,,,False,,False,0.0,False,0.0,0.466,0.466,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.466,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.495284+00:00,benchmark_result,,,,True,,,,,,1184.0,,qwen-2.5-coder-7b-instruct,,,,0.835,,,qwen,,,,,,,,0.835,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.495284+00:00,,,,,,False,,False,0.0,False,0.0,0.835,0.835,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00835,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.267319+00:00,benchmark_result,,,,True,,,,,,88.0,,qwen-2.5-coder-7b-instruct,,,,0.676,,,qwen,,,,,,,,0.676,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.267319+00:00,,,,,,False,,False,0.0,False,0.0,0.676,0.676,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.676,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,accuracy,,,mmlu-base,MMLU-Base,,,2025-07-19T19:56:14.565292+00:00,benchmark_result,,,,True,,,,,,1623.0,,qwen-2.5-coder-7b-instruct,,,,0.68,,,qwen,,,,,,,,0.68,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.565292+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MMLU-Base,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"Base version of the Massive Multitask Language Understanding benchmark, evaluating language models across 57 tasks including elementary mathematics, US history, computer science, law, and other professional and academic subjects. Designed to comprehensively measure the breadth and depth of a model's academic and professional understanding.",0.68,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.469384+00:00,benchmark_result,,,,True,,,,,,193.0,,qwen-2.5-coder-7b-instruct,,,,0.401,,,qwen,,,,,,,,0.401,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.469384+00:00,,,,,,False,,False,0.0,False,0.0,0.401,0.401,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.401,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.537049+00:00,benchmark_result,,,,True,,,,,,730.0,,qwen-2.5-coder-7b-instruct,,,,0.666,,,qwen,,,,,,,,0.666,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:12.537049+00:00,,,,,,False,,False,0.0,False,0.0,0.666,0.666,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.666,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen
,accuracy,,,stem,STEM,,,2025-07-19T19:56:14.561469+00:00,benchmark_result,,,,True,,,,,,1622.0,,qwen-2.5-coder-7b-instruct,,,,0.34,,,qwen,,,,,,,,0.34,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.561469+00:00,,,,,,False,,False,0.0,False,0.0,0.34,0.34,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",STEM,"['math', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"A comprehensive multimodal benchmark dataset with 448 skills and 1,073,146 questions spanning all STEM subjects (Science, Technology, Engineering, Mathematics), designed to test neural models' vision-language STEM skills based on K-12 curriculum. Unlike existing datasets that focus on expert-level ability, this dataset includes fundamental skills designed around educational standards.",0.34,False,False,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.489921+00:00,benchmark_result,,,,True,,,,,,1596.0,,qwen-2.5-coder-7b-instruct,,,,0.34,,,qwen,,,,,,,,0.34,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:14.489921+00:00,,,,,,False,,False,0.0,False,0.0,0.34,0.34,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.34,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.353301+00:00,benchmark_result,,,,True,,,,,,137.0,,qwen-2.5-coder-7b-instruct,,,,0.506,,,qwen,,,,,,,,0.506,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:11.353301+00:00,,,,,,False,,False,0.0,False,0.0,0.506,0.506,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.506,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen
,accuracy,,,winogrande,Winogrande,,,2025-07-19T19:56:13.221874+00:00,benchmark_result,,,,True,,,,,,1065.0,,qwen-2.5-coder-7b-instruct,,,,0.729,,,qwen,,,,,,,,0.729,https://arxiv.org/abs/2409.12186,,,,,,,,2025-07-19T19:56:13.221874+00:00,,,,,,False,,False,0.0,False,0.0,0.729,0.729,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Coder 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,5500000000000.0,True,False,apache_2_0,Open & Permissive,2024-09-19,2024.0,9.0,2024-09,Very Large (>70B),"Qwen2.5-Coder is a specialized coding model trained on 5.5 trillion tokens of code data, supporting 92 programming languages with a 128K context window. It excels in code generation, completion, and repair while maintaining strong performance in math and general tasks. The model demonstrates exceptional capabilities in multi-programming language tasks and code reasoning.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.729,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen
,Accuracy,,,arc-c,ARC-C,,,2025-07-19T19:56:11.129146+00:00,benchmark_result,,,,True,,,,,,22.0,,qwen2-72b-instruct,,,,0.689,,,qwen,,,,,,,,0.689,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.129146+00:00,,,,,,False,,False,0.0,False,0.0,0.689,0.689,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",ARC-C,"['reasoning', 'general']",text,False,1.0,en,"The AI2 Reasoning Challenge (ARC) Challenge Set is a multiple-choice question-answering benchmark containing grade-school level science questions that require advanced reasoning capabilities. ARC-C specifically contains questions that were answered incorrectly by both retrieval-based and word co-occurrence algorithms, making it a particularly challenging subset designed to test commonsense reasoning abilities in AI systems.",0.689,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2
,Accuracy,,,bbh,BBH,,,2025-07-19T19:56:13.045120+00:00,benchmark_result,,,,True,,,,,,973.0,,qwen2-72b-instruct,,,,0.824,,,qwen,,,,,,,,0.824,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:13.045120+00:00,,,,,,False,,False,0.0,False,0.0,0.824,0.824,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.824,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Accuracy,,,c-eval,C-Eval,,,2025-07-19T19:56:11.926225+00:00,benchmark_result,,,,True,,,,,,437.0,,qwen2-72b-instruct,,,,0.838,,,qwen,,,,,,,,0.838,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.926225+00:00,,,,,,False,,False,0.0,False,0.0,0.838,0.838,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",C-Eval,"['general', 'reasoning']",text,True,1.0,en,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",0.838,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Accuracy,,,cmmlu,CMMLU,,,2025-07-19T19:56:14.943893+00:00,benchmark_result,,,,True,,,,,,1749.0,,qwen2-72b-instruct,,,,0.901,,,qwen,,,,,,,,0.901,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:14.943893+00:00,,,,,,False,,False,0.0,False,0.0,0.901,0.901,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",CMMLU,"['language', 'reasoning', 'general']",text,True,1.0,en,"CMMLU (Chinese Massive Multitask Language Understanding) is a comprehensive Chinese benchmark that evaluates the knowledge and reasoning capabilities of large language models across 67 different subject topics. The benchmark covers natural sciences, social sciences, engineering, and humanities with multiple-choice questions ranging from basic to advanced professional levels.",0.901,True,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2
,Pass@1,,,evalplus,EvalPlus,,,2025-07-19T19:56:11.802955+00:00,benchmark_result,,,,True,,,,,,372.0,,qwen2-72b-instruct,,,,0.79,,,qwen,,,,,,,,0.79,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.802955+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",EvalPlus,"['reasoning', 'code']",text,False,100.0,en,"A rigorous code synthesis evaluation framework that augments existing datasets with extensive test cases generated by LLM and mutation-based strategies to better assess functional correctness of generated code, including HumanEval+ with 80x more test cases",0.0079,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.687633+00:00,benchmark_result,,,,True,,,,,,307.0,,qwen2-72b-instruct,,,,0.424,,,qwen,,,,,,,,0.424,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.687633+00:00,,,,,,False,,False,0.0,False,0.0,0.424,0.424,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.424,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.089706+00:00,benchmark_result,,,,True,,,,,,999.0,,qwen2-72b-instruct,,,,0.911,,,qwen,,,,,,,,0.911,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:13.089706+00:00,,,,,,False,,False,0.0,False,0.0,0.911,0.911,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.911,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2
,Accuracy,,,hellaswag,HellaSwag,,,2025-07-19T19:56:11.184833+00:00,benchmark_result,,,,True,,,,,,48.0,,qwen2-72b-instruct,,,,0.876,,,qwen,,,,,,,,0.876,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.184833+00:00,,,,,,False,,False,0.0,False,0.0,0.876,0.876,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",HellaSwag,['reasoning'],text,False,1.0,en,"A challenging commonsense natural language inference dataset that uses Adversarial Filtering to create questions trivial for humans (>95% accuracy) but difficult for state-of-the-art models, requiring completion of sentence endings based on physical situations and everyday activities",0.876,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.653267+00:00,benchmark_result,,,,True,,,,,,790.0,,qwen2-72b-instruct,,,,0.86,,,qwen,,,,,,,,0.86,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:12.653267+00:00,,,,,,False,,False,0.0,False,0.0,0.86,0.86,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.86,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.871582+00:00,benchmark_result,,,,True,,,,,,409.0,,qwen2-72b-instruct,,,,0.597,,,qwen,,,,,,,,0.597,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.871582+00:00,,,,,,False,,False,0.0,False,0.0,0.597,0.597,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.597,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.508406+00:00,benchmark_result,,,,True,,,,,,1190.0,,qwen2-72b-instruct,,,,0.802,,,qwen,,,,,,,,0.802,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:13.508406+00:00,,,,,,False,,False,0.0,False,0.0,0.802,0.802,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.008020000000000001,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.272629+00:00,benchmark_result,,,,True,,,,,,91.0,,qwen2-72b-instruct,,,,0.823,,,qwen,,,,,,,,0.823,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.272629+00:00,,,,,,False,,False,0.0,False,0.0,0.823,0.823,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.823,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.480879+00:00,benchmark_result,,,,True,,,,,,199.0,,qwen2-72b-instruct,,,,0.644,,,qwen,,,,,,,,0.644,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.480879+00:00,,,,,,False,,False,0.0,False,0.0,0.644,0.644,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.644,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2
,Pass@1,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.327331+00:00,benchmark_result,,,,True,,,,,,647.0,,qwen2-72b-instruct,,,,0.692,,,qwen,,,,,,,,0.692,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:12.327331+00:00,,,,,,False,,False,0.0,False,0.0,0.692,0.692,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.692,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2
,Accuracy,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.494165+00:00,benchmark_result,,,,True,,,,,,1598.0,,qwen2-72b-instruct,,,,0.444,,,qwen,,,,,,,,0.444,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:14.494165+00:00,,,,,,False,,False,0.0,False,0.0,0.444,0.444,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.444,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,truthfulqa,TruthfulQA,,,2025-07-19T19:56:11.356602+00:00,benchmark_result,,,,True,,,,,,139.0,,qwen2-72b-instruct,,,,0.548,,,qwen,,,,,,,,0.548,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.356602+00:00,,,,,,False,,False,0.0,False,0.0,0.548,0.548,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",TruthfulQA,"['general', 'reasoning', 'legal', 'healthcare', 'finance']",text,False,1.0,en,"TruthfulQA is a benchmark to measure whether language models are truthful in generating answers to questions. It comprises 817 questions that span 38 categories, including health, law, finance and politics. The questions are crafted such that some humans would answer falsely due to a false belief or misconception, testing models' ability to avoid generating false answers learned from human texts.",0.548,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,winogrande,Winogrande,,,2025-07-19T19:56:11.386216+00:00,benchmark_result,,,,True,,,,,,151.0,,qwen2-72b-instruct,,,,0.851,,,qwen,,,,,,,,0.851,https://huggingface.co/Qwen/Qwen2-72B,,,,,,,,2025-07-19T19:56:11.386216+00:00,,,,,,False,,False,0.0,False,0.0,0.851,0.851,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,False,tongyi_qianwen,Restricted/Community,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-72B-Instruct is an instruction-tuned language model with 72 billion parameters, supporting a context length of up to 131,072 tokens. It's part of the new Qwen2 series, which has surpassed most open-source models and demonstrates competitiveness against proprietary models across various benchmarks.",Winogrande,"['reasoning', 'language']",text,False,1.0,en,"WinoGrande: An Adversarial Winograd Schema Challenge at Scale. A large-scale dataset of 44,000 pronoun resolution problems designed to test machine commonsense reasoning. Uses adversarial filtering to reduce spurious biases and provides a more robust evaluation of whether AI systems truly understand commonsense or exploit statistical shortcuts. Current best AI methods achieve 59.4-79.1% accuracy, significantly below human performance of 94.0%.",0.851,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Score,,,alignbench,AlignBench,,,2025-07-19T19:56:14.544441+00:00,benchmark_result,,,,True,,,,,,1616.0,,qwen2-7b-instruct,,,,0.721,,,qwen,,,,,,,,0.721,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:14.544441+00:00,,,,,,False,,False,0.0,False,0.0,0.721,0.721,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",AlignBench,"['general', 'language', 'math', 'reasoning', 'roleplay']",text,True,1.0,en,"AlignBench is a comprehensive multi-dimensional benchmark for evaluating Chinese alignment of Large Language Models. It contains 8 main categories: Fundamental Language Ability, Advanced Chinese Understanding, Open-ended Questions, Writing Ability, Logical Reasoning, Mathematics, Task-oriented Role Play, and Professional Knowledge. The benchmark includes 683 real-scenario rooted queries with human-verified references and uses a rule-calibrated multi-dimensional LLM-as-Judge approach with Chain-of-Thought for evaluation.",0.721,True,False,True,True,True,False,False,False,True,False,False,False,False,False,False,False,False,Good (70-80%),qwen2
,Accuracy,,,c-eval,C-Eval,,,2025-07-19T19:56:11.924104+00:00,benchmark_result,,,,True,,,,,,436.0,,qwen2-7b-instruct,,,,0.772,,,qwen,,,,,,,,0.772,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.924104+00:00,,,,,,False,,False,0.0,False,0.0,0.772,0.772,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",C-Eval,"['general', 'reasoning']",text,True,1.0,en,"C-Eval is a comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. It comprises 13,948 multiple-choice questions across 52 diverse disciplines spanning humanities, science, and engineering, with four difficulty levels: middle school, high school, college, and professional. The benchmark includes C-Eval Hard, a subset of very challenging subjects requiring advanced reasoning abilities.",0.772,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2
,Pass@1,,,evalplus,EvalPlus,,,2025-07-19T19:56:11.799094+00:00,benchmark_result,,,,True,,,,,,370.0,,qwen2-7b-instruct,,,,0.703,,,qwen,,,,,,,,0.703,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.799094+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",EvalPlus,"['reasoning', 'code']",text,False,100.0,en,"A rigorous code synthesis evaluation framework that augments existing datasets with extensive test cases generated by LLM and mutation-based strategies to better assess functional correctness of generated code, including HumanEval+ with 80x more test cases",0.00703,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.674412+00:00,benchmark_result,,,,True,,,,,,299.0,,qwen2-7b-instruct,,,,0.253,,,qwen,,,,,,,,0.253,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.674412+00:00,,,,,,False,,False,0.0,False,0.0,0.253,0.253,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.253,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.078833+00:00,benchmark_result,,,,True,,,,,,992.0,,qwen2-7b-instruct,,,,0.823,,,qwen,,,,,,,,0.823,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:13.078833+00:00,,,,,,False,,False,0.0,False,0.0,0.823,0.823,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.823,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2
,Pass@1,,,humaneval,HumanEval,,,2025-07-19T19:56:12.643272+00:00,benchmark_result,,,,True,,,,,,784.0,,qwen2-7b-instruct,,,,0.799,,,qwen,,,,,,,,0.799,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:12.643272+00:00,,,,,,False,,False,0.0,False,0.0,0.799,0.799,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.799,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2
,Score,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.335377+00:00,benchmark_result,,,,True,,,,,,1119.0,,qwen2-7b-instruct,,,,0.266,,,qwen,,,,,,,,0.266,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:13.335377+00:00,,,,,,False,,False,0.0,False,0.0,0.266,0.266,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.266,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.859120+00:00,benchmark_result,,,,True,,,,,,402.0,,qwen2-7b-instruct,,,,0.496,,,qwen,,,,,,,,0.496,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.859120+00:00,,,,,,False,,False,0.0,False,0.0,0.496,0.496,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.496,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Pass@1,,,mbpp,MBPP,,,2025-07-19T19:56:13.493272+00:00,benchmark_result,,,,True,,,,,,1183.0,,qwen2-7b-instruct,,,,0.672,,,qwen,,,,,,,,0.672,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:13.493272+00:00,,,,,,False,,False,0.0,False,0.0,0.672,0.672,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00672,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.265352+00:00,benchmark_result,,,,True,,,,,,87.0,,qwen2-7b-instruct,,,,0.705,,,qwen,,,,,,,,0.705,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.265352+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.705,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2
,Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.467957+00:00,benchmark_result,,,,True,,,,,,192.0,,qwen2-7b-instruct,,,,0.441,,,qwen,,,,,,,,0.441,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:11.467957+00:00,,,,,,False,,False,0.0,False,0.0,0.441,0.441,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.441,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Score,,,mt-bench,MT-Bench,,,2025-07-19T19:56:14.519120+00:00,benchmark_result,,,,True,,,,,,1605.0,,qwen2-7b-instruct,,,,0.841,,,qwen,,,,,,,,0.841,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:14.519120+00:00,,,,,,False,,False,0.0,False,0.0,0.841,0.841,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MT-Bench,"['communication', 'reasoning', 'general', 'roleplay']",text,False,100.0,en,"MT-Bench is a challenging multi-turn benchmark that measures the ability of large language models to engage in coherent, informative, and engaging conversations. It uses strong LLMs as judges for scalable and explainable evaluation of multi-turn dialogue capabilities.",0.008409999999999999,True,False,False,True,False,False,False,False,True,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Pass@1,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.317803+00:00,benchmark_result,,,,True,,,,,,641.0,,qwen2-7b-instruct,,,,0.591,,,qwen,,,,,,,,0.591,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:12.317803+00:00,,,,,,False,,False,0.0,False,0.0,0.591,0.591,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.591,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,Accuracy,,,theoremqa,TheoremQA,,,2025-07-19T19:56:14.487702+00:00,benchmark_result,,,,True,,,,,,1595.0,,qwen2-7b-instruct,,,,0.253,,,qwen,,,,,,,,0.253,https://huggingface.co/Qwen/Qwen2-7B-Instruct,,,,,,,,2025-07-19T19:56:14.487702+00:00,,,,,,False,,False,0.0,False,0.0,0.253,0.253,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2 7B Instruct,qwen,Alibaba Cloud / Qwen Team,7620000000.0,7620000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-07-23,2024.0,7.0,2024-07,Very Large (>70B),"Qwen2-7B-Instruct is an instruction-tuned language model with 7 billion parameters, supporting a context length of up to 131,072 tokens.",TheoremQA,"['math', 'reasoning', 'physics', 'finance']",text,False,1.0,en,"A theorem-driven question answering dataset containing 800 high-quality questions covering 350+ theorems from Math, Physics, EE&CS, and Finance. Designed to evaluate AI models' capabilities to apply theorems to solve challenging university-level science problems.",0.253,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2
,score,,,chartqa,ChartQA,,,2025-07-19T19:56:12.806635+00:00,benchmark_result,,,,True,,,,,,864.0,,qwen2-vl-72b,,,,0.883,,,qwen,,,,,,,,0.883,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:12.806635+00:00,,,,,,False,,False,0.0,False,0.0,0.883,0.883,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.883,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2
,score,,,docvqatest,DocVQAtest,,,2025-07-19T19:56:14.582058+00:00,benchmark_result,,,,True,,,,,,1629.0,,qwen2-vl-72b,,,,0.965,,,qwen,,,,,,,,0.965,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.582058+00:00,,,,,,False,,False,0.0,False,0.0,0.965,0.965,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",DocVQAtest,"['vision', 'multimodal']",multimodal,False,1.0,en,"DocVQA is a Visual Question Answering benchmark on document images containing 50,000 questions defined on 12,000+ document images. The benchmark focuses on understanding document structure and content to answer questions about various document types including letters, memos, notes, and reports from the UCSF Industry Documents Library.",0.965,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),qwen2
,score,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.928297+00:00,benchmark_result,,,,True,,,,,,923.0,,qwen2-vl-72b,,,,0.779,,,qwen,,,,,,,,0.779,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:12.928297+00:00,,,,,,False,,False,0.0,False,0.0,0.779,0.779,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.779,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2
,score,,,infovqatest,InfoVQAtest,,,2025-07-19T19:56:14.586477+00:00,benchmark_result,,,,True,,,,,,1630.0,,qwen2-vl-72b,,,,0.845,,,qwen,,,,,,,,0.845,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.586477+00:00,,,,,,False,,False,0.0,False,0.0,0.845,0.845,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",InfoVQAtest,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA test set with infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.845,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2
,score,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.662750+00:00,benchmark_result,,,,True,,,,,,1269.0,,qwen2-vl-72b,,,,0.705,,,qwen,,,,,,,,0.705,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:13.662750+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.705,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2
,score,,,mmbench-test,MMBench_test,,,2025-07-19T19:56:14.610292+00:00,benchmark_result,,,,True,,,,,,1639.0,,qwen2-vl-72b,,,,0.865,,,qwen,,,,,,,,0.865,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.610292+00:00,,,,,,False,,False,0.0,False,0.0,0.865,0.865,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MMBench_test,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"Test set of MMBench, a bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",0.865,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2
,score,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.292395+00:00,benchmark_result,,,,True,,,,,,1532.0,,qwen2-vl-72b,,,,0.462,,,qwen,,,,,,,,0.462,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.292395+00:00,,,,,,False,,False,0.0,False,0.0,0.462,0.462,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.462,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2
,score,,,mmmuval,MMMUval,,,2025-07-19T19:56:14.578458+00:00,benchmark_result,,,,True,,,,,,1628.0,,qwen2-vl-72b,,,,0.645,,,qwen,,,,,,,,0.645,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.578458+00:00,,,,,,False,,False,0.0,False,0.0,0.645,0.645,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MMMUval,"['vision', 'general', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"Validation set for MMMU (Massive Multi-discipline Multimodal Understanding and Reasoning) benchmark, designed to evaluate multimodal models on massive multi-discipline tasks demanding college-level subject knowledge and deliberate reasoning across Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering.",0.645,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2
,score,,,mmvetgpt4turbo,MMVetGPT4Turbo,,,2025-07-19T19:56:14.613913+00:00,benchmark_result,,,,True,,,,,,1640.0,,qwen2-vl-72b,,,,0.74,,,qwen,,,,,,,,0.74,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.613913+00:00,,,,,,False,,False,0.0,False,0.0,0.74,0.74,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MMVetGPT4Turbo,"['vision', 'multimodal', 'reasoning', 'general', 'spatial_reasoning', 'math']",multimodal,False,1.0,en,"MM-Vet evaluation using GPT-4 Turbo for scoring. This variant of MM-Vet examines large multimodal models on complicated multimodal tasks requiring integrated capabilities across six core vision-language abilities: recognition, knowledge, spatial awareness, language generation, OCR, and math.",0.74,True,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2
,score,,,mtvqa,MTVQA,,,2025-07-19T19:56:14.590936+00:00,benchmark_result,,,,True,,,,,,1631.0,,qwen2-vl-72b,,,,0.309,,,qwen,,,,,,,,0.309,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.590936+00:00,,,,,,False,,False,0.0,False,0.0,0.309,0.309,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MTVQA,"['vision', 'multimodal', 'text-to-image']",multimodal,True,1.0,en,"MTVQA (Multilingual Text-Centric Visual Question Answering) is the first benchmark featuring high-quality human expert annotations across 9 diverse languages, consisting of 6,778 question-answer pairs across 2,116 images. It addresses visual-textual misalignment problems in multilingual text-centric VQA.",0.309,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2
,score,,,mvbench,MVBench,,,2025-07-19T19:56:14.618622+00:00,benchmark_result,,,,True,,,,,,1641.0,,qwen2-vl-72b,,,,0.736,,,qwen,,,,,,,,0.736,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.618622+00:00,,,,,,False,,False,0.0,False,0.0,0.736,0.736,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",MVBench,"['vision', 'video', 'multimodal', 'spatial_reasoning', 'reasoning']",multimodal,False,1.0,en,"A comprehensive multi-modal video understanding benchmark covering 20 challenging video tasks that require temporal understanding beyond single-frame analysis. Tasks span from perception to cognition, including action recognition, temporal reasoning, spatial reasoning, object interaction, scene transition, and counterfactual inference. Uses a novel static-to-dynamic method to systematically generate video tasks from existing annotations.",0.736,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2
,score,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.311748+00:00,benchmark_result,,,,True,,,,,,1539.0,,qwen2-vl-72b,,,,0.877,,,qwen,,,,,,,,0.877,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.311748+00:00,,,,,,False,,False,0.0,False,0.0,0.877,0.877,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.877,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2
,score,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.597450+00:00,benchmark_result,,,,True,,,,,,1633.0,,qwen2-vl-72b,,,,0.778,,,qwen,,,,,,,,0.778,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.597450+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.778,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2
,score,,,textvqa,TextVQA,,,2025-07-19T19:56:12.894922+00:00,benchmark_result,,,,True,,,,,,909.0,,qwen2-vl-72b,,,,0.855,,,qwen,,,,,,,,0.855,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:12.894922+00:00,,,,,,False,,False,0.0,False,0.0,0.855,0.855,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.855,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2
,score,,,vcr-en-easy,VCR_en_easy,,,2025-07-19T19:56:14.594379+00:00,benchmark_result,,,,True,,,,,,1632.0,,qwen2-vl-72b,,,,0.9193,,,qwen,,,,,,,,0.9193,https://github.com/QwenLM/Qwen2,,,,,,,,2025-07-19T19:56:14.594379+00:00,,,,,,False,,False,0.0,False,0.0,0.9193,0.9193,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2-VL-72B-Instruct,qwen,Alibaba Cloud / Qwen Team,73400000000.0,73400000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2024-08-29,2024.0,8.0,2024-08,Very Large (>70B),"An instruction-tuned, large multimodal model that excels at visual understanding and step-by-step reasoning. It supports image and video input, with dynamic resolution handling and improved positional embeddings (M-ROPE), enabling advanced capabilities such as complex problem solving, multilingual text recognition in images, and agent-like interactions in video contexts.",VCR_en_easy,"['vision', 'reasoning']",multimodal,False,1.0,en,"Visual Commonsense Reasoning (VCR) benchmark that tests higher-order cognition and commonsense reasoning beyond simple object recognition. Models must answer challenging questions about images and provide rationales justifying their answers. The benchmark measures the ability to infer people's actions, goals, and mental states from visual context.",0.9193,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),qwen2
,Score,,,ai2d,AI2D,,,2025-07-19T19:56:13.633399+00:00,benchmark_result,,,,True,,,,,,1254.0,,qwen2.5-omni-7b,,,,0.832,,,qwen,,,,,,,,0.832,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:13.633399+00:00,,,,,,False,,False,0.0,False,0.0,0.832,0.832,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.832,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,chartqa,ChartQA,,,2025-07-19T19:56:12.809953+00:00,benchmark_result,,,,True,,,,,,866.0,,qwen2.5-omni-7b,,,,0.853,,,qwen,,,,,,,,0.853,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.809953+00:00,,,,,,False,,False,0.0,False,0.0,0.853,0.853,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.853,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,WER,,,common-voice-15,Common Voice 15,,,2025-07-19T19:56:14.833534+00:00,benchmark_result,,,,True,,,,,,1718.0,,qwen2.5-omni-7b,,,,0.076,,,qwen,,,,,,,,0.076,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.833534+00:00,,,,,,False,,False,0.0,False,0.0,0.076,0.076,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",Common Voice 15,"['audio', 'speech-to-text', 'language']",audio,True,100.0,en,"Common Voice is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Version 15.0 contains 28,750 recorded hours across 114 languages, consisting of crowdsourced voice recordings with corresponding transcriptions.",0.0007599999999999999,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,BLEU,,,covost2-en-zh,CoVoST2 en-zh,,,2025-07-19T19:56:14.828460+00:00,benchmark_result,,,,True,,,,,,1717.0,,qwen2.5-omni-7b,,,,0.414,,,qwen,,,,,,,,0.414,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.828460+00:00,,,,,,False,,False,0.0,False,0.0,0.414,0.414,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",CoVoST2 en-zh,"['audio', 'speech-to-text', 'language']",audio,True,100.0,en,CoVoST 2 English-to-Chinese subset is part of the large-scale multilingual speech translation corpus derived from Common Voice. This subset focuses specifically on English to Chinese speech translation tasks within the broader CoVoST 2 dataset.,0.00414,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,crperelation,CRPErelation,,,2025-07-19T19:56:14.837425+00:00,benchmark_result,,,,True,,,,,,1719.0,,qwen2.5-omni-7b,,,,0.765,,,qwen,,,,,,,,0.765,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.837425+00:00,,,,,,False,,False,0.0,False,0.0,0.765,0.765,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",CRPErelation,"['healthcare', 'reasoning']",text,False,1.0,en,Clinical reasoning problems evaluation benchmark for assessing diagnostic reasoning and medical knowledge application capabilities.,0.765,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,docvqa,DocVQA,,,2025-07-19T19:56:12.846061+00:00,benchmark_result,,,,True,,,,,,887.0,,qwen2.5-omni-7b,,,,0.952,,,qwen,,,,,,,,0.952,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.846061+00:00,,,,,,False,,False,0.0,False,0.0,0.952,0.952,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.952,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),qwen2.5
,Score,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.931056+00:00,benchmark_result,,,,True,,,,,,924.0,,qwen2.5-omni-7b,,,,0.686,,,qwen,,,,,,,,0.686,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.931056+00:00,,,,,,False,,False,0.0,False,0.0,0.686,0.686,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.686,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,WER,,,fleurs,FLEURS,,,2025-07-19T19:56:13.953081+00:00,benchmark_result,,,,True,,,,,,1401.0,,qwen2.5-omni-7b,,,,0.041,,,qwen,,,,,,,,0.041,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:13.953081+00:00,,,,,,False,,False,0.0,False,0.0,0.041,0.041,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",FLEURS,"['language', 'speech-to-text']",audio,True,100.0,en,"Few-shot Learning Evaluation of Universal Representations of Speech - a parallel speech dataset in 102 languages built on FLoRes-101 with approximately 12 hours of speech supervision per language for tasks including ASR, speech language identification, translation and retrieval",0.00041,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,giantsteps-tempo,GiantSteps Tempo,,,2025-07-19T19:56:14.841583+00:00,benchmark_result,,,,True,,,,,,1720.0,,qwen2.5-omni-7b,,,,0.88,,,qwen,,,,,,,,0.88,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.841583+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",GiantSteps Tempo,['audio'],audio,False,1.0,en,"A dataset for tempo estimation in electronic dance music containing 664 2-minute audio previews from Beatport, annotated from user corrections for evaluating automatic tempo estimation algorithms.",0.88,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,gpqa,GPQA,,,2025-07-19T19:56:11.684328+00:00,benchmark_result,,,,True,,,,,,305.0,,qwen2.5-omni-7b,,,,0.308,,,qwen,,,,,,,,0.308,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:11.684328+00:00,,,,,,False,,False,0.0,False,0.0,0.308,0.308,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.308,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.086524+00:00,benchmark_result,,,,True,,,,,,997.0,,qwen2.5-omni-7b,,,,0.887,,,qwen,,,,,,,,0.887,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:13.086524+00:00,,,,,,False,,False,0.0,False,0.0,0.887,0.887,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.887,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,humaneval,HumanEval,,,2025-07-19T19:56:12.650243+00:00,benchmark_result,,,,True,,,,,,788.0,,qwen2.5-omni-7b,,,,0.787,,,qwen,,,,,,,,0.787,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.650243+00:00,,,,,,False,,False,0.0,False,0.0,0.787,0.787,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.787,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,livebench,LiveBench,,,2025-07-19T19:56:12.581448+00:00,benchmark_result,,,,True,,,,,,752.0,,qwen2.5-omni-7b,,,,0.296,,,qwen,,,,,,,,0.296,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.581448+00:00,,,,,,False,,False,0.0,False,0.0,0.296,0.296,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.296,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,math,MATH,,,2025-07-19T19:56:11.867189+00:00,benchmark_result,,,,True,,,,,,407.0,,qwen2.5-omni-7b,,,,0.715,,,qwen,,,,,,,,0.715,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:11.867189+00:00,,,,,,False,,False,0.0,False,0.0,0.715,0.715,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.715,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mathvision,MathVision,,,2025-07-19T19:56:14.702750+00:00,benchmark_result,,,,True,,,,,,1676.0,,qwen2.5-omni-7b,,,,0.25,,,qwen,,,,,,,,0.25,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.702750+00:00,,,,,,False,,False,0.0,False,0.0,0.25,0.25,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MathVision,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",0.25,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvista,MathVista,,,2025-07-19T19:56:12.094090+00:00,benchmark_result,,,,True,,,,,,527.0,,qwen2.5-omni-7b,,,,0.679,,,qwen,,,,,,,,0.679,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.094090+00:00,,,,,,False,,False,0.0,False,0.0,0.679,0.679,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MathVista,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista evaluates mathematical reasoning of foundation models in visual contexts. It consists of 6,141 examples derived from 28 existing multimodal datasets and 3 newly created datasets (IQTest, FunctionQA, and PaperQA), combining challenges from diverse mathematical and visual tasks to assess models' ability to understand complex figures and perform rigorous reasoning.",0.679,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mbpp,MBPP,,,2025-07-19T19:56:13.504920+00:00,benchmark_result,,,,True,,,,,,1188.0,,qwen2.5-omni-7b,,,,0.732,,,qwen,,,,,,,,0.732,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:13.504920+00:00,,,,,,False,,False,0.0,False,0.0,0.732,0.732,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00732,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,meld,Meld,,,2025-07-19T19:56:14.845437+00:00,benchmark_result,,,,True,,,,,,1721.0,,qwen2.5-omni-7b,,,,0.57,,,qwen,,,,,,,,0.57,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.845437+00:00,,,,,,False,,False,0.0,False,0.0,0.57,0.57,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",Meld,"['multimodal', 'psychology']",multimodal,False,1.0,en,"MELD (Multimodal EmotionLines Dataset) is a multimodal multi-party dataset for emotion recognition in conversations. Contains approximately 13,000 utterances from 1,433 dialogues extracted from the TV series Friends. Each utterance is annotated with emotion (Anger, Disgust, Sadness, Joy, Neutral, Surprise, Fear) and sentiment labels across audio, visual, and textual modalities.",0.57,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mm-mt-bench,MM-MT-Bench,,,2025-07-19T19:56:14.883880+00:00,benchmark_result,,,,True,,,,,,1731.0,,qwen2.5-omni-7b,,,,0.06,,,qwen,,,,,,,,0.06,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.883880+00:00,,,,,,False,,False,0.0,False,0.0,0.06,0.06,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MM-MT-Bench,"['multimodal', 'communication']",multimodal,False,100.0,en,A multi-turn LLM-as-a-judge evaluation benchmark for testing multimodal instruction-tuned models' ability to follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner.,0.0006,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmau,MMAU,,,2025-07-19T19:56:14.849392+00:00,benchmark_result,,,,True,,,,,,1722.0,,qwen2.5-omni-7b,,,,0.656,,,qwen,,,,,,,,0.656,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.849392+00:00,,,,,,False,,False,0.0,False,0.0,0.656,0.656,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMAU,"['audio', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"A massive multi-task audio understanding and reasoning benchmark comprising 10,000 carefully curated audio clips paired with human-annotated natural language questions spanning speech, environmental sounds, and music. Requires expert-level knowledge and complex reasoning across 27 distinct skills.",0.656,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmau-music,MMAU Music,,,2025-07-19T19:56:14.854098+00:00,benchmark_result,,,,True,,,,,,1723.0,,qwen2.5-omni-7b,,,,0.6916,,,qwen,,,,,,,,0.6916,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.854098+00:00,,,,,,False,,False,0.0,False,0.0,0.6916,0.6916,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMAU Music,"['audio', 'multimodal', 'reasoning']",multimodal,False,1.0,en,A subset of the MMAU benchmark focused specifically on music understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across music audio clips.,0.6916,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmau-sound,MMAU Sound,,,2025-07-19T19:56:14.862523+00:00,benchmark_result,,,,True,,,,,,1724.0,,qwen2.5-omni-7b,,,,0.6787,,,qwen,,,,,,,,0.6787,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.862523+00:00,,,,,,False,,False,0.0,False,0.0,0.6787,0.6787,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMAU Sound,"['audio', 'multimodal', 'reasoning']",multimodal,False,1.0,en,A subset of the MMAU benchmark focused specifically on environmental sound understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across environmental sound clips.,0.6787,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmau-speech,MMAU Speech,,,2025-07-19T19:56:14.867393+00:00,benchmark_result,,,,True,,,,,,1725.0,,qwen2.5-omni-7b,,,,0.5976,,,qwen,,,,,,,,0.5976,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.867393+00:00,,,,,,False,,False,0.0,False,0.0,0.5976,0.5976,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMAU Speech,"['audio', 'multimodal', 'reasoning', 'speech-to-text']",multimodal,False,1.0,en,A subset of the MMAU benchmark focused specifically on speech understanding and reasoning tasks. Part of a comprehensive multimodal audio understanding benchmark that evaluates models on expert-level knowledge and complex reasoning across speech audio clips.,0.5976,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmbench-v1.1,MMBench-V1.1,,,2025-07-19T19:56:14.871500+00:00,benchmark_result,,,,True,,,,,,1726.0,,qwen2.5-omni-7b,,,,0.818,,,qwen,,,,,,,,0.818,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.871500+00:00,,,,,,False,,False,0.0,False,0.0,0.818,0.818,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMBench-V1.1,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"Version 1.1 of MMBench, an improved bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks.",0.818,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,mme-realworld,MME-RealWorld,,,2025-07-19T19:56:14.879804+00:00,benchmark_result,,,,True,,,,,,1730.0,,qwen2.5-omni-7b,,,,0.616,,,qwen,,,,,,,,0.616,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.879804+00:00,,,,,,False,,False,0.0,False,0.0,0.616,0.616,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MME-RealWorld,"['vision', 'multimodal', 'general']",multimodal,False,1.0,en,"A comprehensive evaluation benchmark for Multimodal Large Language Models featuring over 13,366 high-resolution images and 29,429 question-answer pairs across 43 subtasks and 5 real-world scenarios. The largest manually annotated multimodal benchmark to date, designed to test MLLMs on challenging high-resolution real-world scenarios.",0.616,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.477278+00:00,benchmark_result,,,,True,,,,,,197.0,,qwen2.5-omni-7b,,,,0.47,,,qwen,,,,,,,,0.47,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:11.477278+00:00,,,,,,False,,False,0.0,False,0.0,0.47,0.47,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.47,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.544013+00:00,benchmark_result,,,,True,,,,,,734.0,,qwen2.5-omni-7b,,,,0.71,,,qwen,,,,,,,,0.71,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.544013+00:00,,,,,,False,,False,0.0,False,0.0,0.71,0.71,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.71,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmmu,MMMU,,,2025-07-19T19:56:12.175251+00:00,benchmark_result,,,,True,,,,,,571.0,,qwen2.5-omni-7b,,,,0.592,,,qwen,,,,,,,,0.592,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.175251+00:00,,,,,,False,,False,0.0,False,0.0,0.592,0.592,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.592,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.296124+00:00,benchmark_result,,,,True,,,,,,1534.0,,qwen2.5-omni-7b,,,,0.366,,,qwen,,,,,,,,0.366,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.296124+00:00,,,,,,False,,False,0.0,False,0.0,0.366,0.366,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.366,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmstar,MMStar,,,2025-07-19T19:56:14.664551+00:00,benchmark_result,,,,True,,,,,,1660.0,,qwen2.5-omni-7b,,,,0.64,,,qwen,,,,,,,,0.64,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.664551+00:00,,,,,,False,,False,0.0,False,0.0,0.64,0.64,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.64,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,muirbench,MuirBench,,,2025-07-19T19:56:14.891075+00:00,benchmark_result,,,,True,,,,,,1734.0,,qwen2.5-omni-7b,,,,0.592,,,qwen,,,,,,,,0.592,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.891075+00:00,,,,,,False,,False,0.0,False,0.0,0.592,0.592,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MuirBench,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"A comprehensive benchmark for robust multi-image understanding capabilities of multimodal LLMs. Consists of 12 diverse multi-image tasks involving 10 categories of multi-image relations (e.g., multiview, temporal relations, narrative, complementary). Comprises 11,264 images and 2,600 multiple-choice questions created in a pairwise manner, where each standard instance is paired with an unanswerable variant for reliable assessment.",0.592,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.324318+00:00,benchmark_result,,,,True,,,,,,645.0,,qwen2.5-omni-7b,,,,0.658,,,qwen,,,,,,,,0.658,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.324318+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.658,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,musiccaps,MusicCaps,,,2025-07-19T19:56:14.894342+00:00,benchmark_result,,,,True,,,,,,1735.0,,qwen2.5-omni-7b,,,,0.328,,,qwen,,,,,,,,0.328,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.894342+00:00,,,,,,False,,False,0.0,False,0.0,0.328,0.328,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MusicCaps,"['audio', 'multimodal']",multimodal,False,1.0,en,"MusicCaps is a dataset composed of 5,521 music examples, each labeled with an English aspect list and a free text caption written by musicians. The dataset contains 10-second music clips from AudioSet paired with rich textual descriptions that capture sonic qualities and musical elements like genre, mood, tempo, instrumentation, and rhythm. Created to support research in music-text understanding and generation tasks.",0.328,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mvbench,MVBench,,,2025-07-19T19:56:14.621841+00:00,benchmark_result,,,,True,,,,,,1643.0,,qwen2.5-omni-7b,,,,0.703,,,qwen,,,,,,,,0.703,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.621841+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",MVBench,"['vision', 'video', 'multimodal', 'spatial_reasoning', 'reasoning']",multimodal,False,1.0,en,"A comprehensive multi-modal video understanding benchmark covering 20 challenging video tasks that require temporal understanding beyond single-frame analysis. Tasks span from perception to cognition, including action recognition, temporal reasoning, spatial reasoning, object interaction, scene transition, and counterfactual inference. Uses a novel static-to-dynamic method to systematically generate video tasks from existing annotations.",0.703,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,NMOS,,,nmos,NMOS,,,2025-07-19T19:56:14.897653+00:00,benchmark_result,,,,True,,,,,,1736.0,,qwen2.5-omni-7b,,,,0.0451,,,qwen,,,,,,,,0.0451,https://qwenlm.github.io/blog/qwen2.5-omni/,,,,,,,,2025-07-19T19:56:14.897653+00:00,,,,,,False,,False,0.0,False,0.0,0.0451,0.0451,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",NMOS,['general'],text,False,100.0,en,NMOS evaluation benchmark for assessing model performance on specialized tasks,0.000451,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,ocrbench-v2,OCRBench_V2,,,2025-07-19T19:56:14.901546+00:00,benchmark_result,,,,True,,,,,,1737.0,,qwen2.5-omni-7b,,,,0.578,,,qwen,,,,,,,,0.578,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.901546+00:00,,,,,,False,,False,0.0,False,0.0,0.578,0.578,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",OCRBench_V2,"['vision', 'image-to-text']",multimodal,True,1.0,en,"OCRBench v2: Enhanced large-scale bilingual benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with 10,000 human-verified question-answering pairs across 8 core OCR capabilities",0.578,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,odinw,ODinW,,,2025-07-19T19:56:14.905294+00:00,benchmark_result,,,,True,,,,,,1738.0,,qwen2.5-omni-7b,,,,0.424,,,qwen,,,,,,,,0.424,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.905294+00:00,,,,,,False,,False,0.0,False,0.0,0.424,0.424,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",ODinW,['vision'],image,False,1.0,en,Object Detection in the Wild (ODinW) benchmark for evaluating object detection models' task-level transfer ability across diverse real-world datasets in terms of prediction accuracy and adaptation efficiency,0.424,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,omnibench,OmniBench,,,2025-07-19T19:56:14.909979+00:00,benchmark_result,,,,True,,,,,,1739.0,,qwen2.5-omni-7b,,,,0.5613,,,qwen,,,,,,,,0.5613,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.909979+00:00,,,,,,False,,False,0.0,False,0.0,0.5613,0.5613,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",OmniBench,"['multimodal', 'reasoning']",multimodal,False,1.0,en,"A novel multimodal benchmark designed to evaluate large language models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. Comprises 1,142 question-answer pairs covering 8 task categories from basic perception to complex inference, with a unique constraint that accurate responses require integrated understanding of all three modalities.",0.5613,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,omnibench-music,OmniBench Music,,,2025-07-19T19:56:14.913742+00:00,benchmark_result,,,,True,,,,,,1740.0,,qwen2.5-omni-7b,,,,0.5283,,,qwen,,,,,,,,0.5283,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.913742+00:00,,,,,,False,,False,0.0,False,0.0,0.5283,0.5283,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",OmniBench Music,"['multimodal', 'audio']",multimodal,False,1.0,en,"Music component of OmniBench, a comprehensive benchmark for evaluating omni-language models' ability to recognize, interpret, and reason across visual, acoustic, and textual inputs simultaneously. The music category includes various compositions and performances that require integrated understanding across text, image, and audio modalities.",0.5283,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,pointgrounding,PointGrounding,,,2025-07-19T19:56:14.918183+00:00,benchmark_result,,,,True,,,,,,1741.0,,qwen2.5-omni-7b,,,,0.665,,,qwen,,,,,,,,0.665,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.918183+00:00,,,,,,False,,False,0.0,False,0.0,0.665,0.665,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",PointGrounding,"['vision', 'spatial_reasoning', 'multimodal']",multimodal,False,1.0,en,"PointArena is a comprehensive platform for evaluating multimodal pointing across diverse reasoning scenarios. It includes Point-Bench, a curated dataset of ~1,000 pointing tasks across five categories: Spatial (positional references), Affordance (functional part identification), Counting (attribute-based grouping), Steerable (relative pointing), and Reasoning (open-ended visual inference). The benchmark evaluates language-guided pointing capabilities in vision-language models.",0.665,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,realworldqa,RealWorldQA,,,2025-07-19T19:56:14.599392+00:00,benchmark_result,,,,True,,,,,,1634.0,,qwen2.5-omni-7b,,,,0.703,,,qwen,,,,,,,,0.703,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.599392+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",RealWorldQA,"['vision', 'spatial_reasoning']",multimodal,False,1.0,en,"RealWorldQA is a benchmark designed to evaluate basic real-world spatial understanding capabilities of multimodal models. The initial release consists of over 700 anonymized images taken from vehicles and other real-world scenarios, each accompanied by a question and easily verifiable answer. Released by xAI as part of their Grok-1.5 Vision preview to test models' ability to understand natural scenes and spatial relationships in everyday visual contexts.",0.703,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,textvqa,TextVQA,,,2025-07-19T19:56:12.899579+00:00,benchmark_result,,,,True,,,,,,911.0,,qwen2.5-omni-7b,,,,0.844,,,qwen,,,,,,,,0.844,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:12.899579+00:00,,,,,,False,,False,0.0,False,0.0,0.844,0.844,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.844,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,videomme-w-sub.,VideoMME w sub.,,,2025-07-19T19:56:14.727965+00:00,benchmark_result,,,,True,,,,,,1685.0,,qwen2.5-omni-7b,,,,0.724,,,qwen,,,,,,,,0.724,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.727965+00:00,,,,,,False,,False,0.0,False,0.0,0.724,0.724,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",VideoMME w sub.,"['vision', 'multimodal', 'video']",multimodal,False,1.0,en,"The first-ever comprehensive evaluation benchmark of Multi-modal LLMs in Video analysis. Features 900 videos (254 hours) with 2,700 question-answer pairs covering 6 primary visual domains and 30 subfields. Evaluates temporal understanding across short (11 seconds) to long (1 hour) videos with multi-modal inputs including video frames, subtitles, and audio.",0.724,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,vocalsound,VocalSound,,,2025-07-19T19:56:14.921505+00:00,benchmark_result,,,,True,,,,,,1742.0,,qwen2.5-omni-7b,,,,0.939,,,qwen,,,,,,,,0.939,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.921505+00:00,,,,,,False,,False,0.0,False,0.0,0.939,0.939,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",VocalSound,['audio'],audio,False,1.0,en,"A dataset for improving human vocal sounds recognition, containing over 21,000 crowdsourced recordings of laughter, sighs, coughs, throat clearing, sneezes, and sniffs from 3,365 unique subjects. Used for audio event classification and recognition of human non-speech vocalizations.",0.939,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,Score,,,voicebench-avg,VoiceBench Avg,,,2025-07-19T19:56:14.925208+00:00,benchmark_result,,,,True,,,,,,1743.0,,qwen2.5-omni-7b,,,,0.7412,,,qwen,,,,,,,,0.7412,https://github.com/QwenLM/Qwen2.5-Omni,,,,,,,,2025-07-19T19:56:14.925208+00:00,,,,,,False,,False,0.0,False,0.0,0.7412,0.7412,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5-Omni-7B,qwen,Alibaba Cloud / Qwen Team,7000000000.0,7000000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-03-27,2025.0,3.0,2025-03,Very Large (>70B),"Qwen2.5-Omni is the flagship end-to-end multimodal model in the Qwen series. It processes diverse inputs including text, images, audio, and video, delivering real-time streaming responses through text generation and natural speech synthesis using a novel Thinker-Talker architecture.",VoiceBench Avg,"['general', 'reasoning', 'safety', 'communication']",multimodal,False,1.0,en,"VoiceBench is the first benchmark designed to provide a multi-faceted evaluation of LLM-based voice assistants, evaluating capabilities including general knowledge, instruction-following, reasoning, and safety using both synthetic and real spoken instruction data with diverse speaker characteristics and environmental conditions.",0.7412,True,False,False,True,False,False,True,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,EM,,,aitz-em,AITZ_EM,,,2025-07-19T19:56:14.791493+00:00,benchmark_result,,,,True,,,,,,1704.0,,qwen2.5-vl-32b,,,,0.831,,,qwen,,,,,,,,0.831,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.791493+00:00,,,,,,False,,False,0.0,False,0.0,0.831,0.831,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",AITZ_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,"Android-In-The-Zoo (AitZ) benchmark for evaluating autonomous GUI agents on smartphones. Contains 18,643 screen-action pairs with chain-of-action-thought annotations spanning over 70 Android apps. Designed to connect perception (screen layouts and UI elements) with cognition (action decision-making) for natural language-triggered smartphone task completion.",0.831,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,EM,,,android-control-high-em,Android Control High_EM,,,2025-07-19T19:56:14.798431+00:00,benchmark_result,,,,True,,,,,,1707.0,,qwen2.5-vl-32b,,,,0.696,,,qwen,,,,,,,,0.696,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.798431+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",Android Control High_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android device control benchmark using high exact match evaluation metric for assessing agent performance on mobile interface tasks,0.696,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,EM,,,android-control-low-em,Android Control Low_EM,,,2025-07-19T19:56:14.807428+00:00,benchmark_result,,,,True,,,,,,1710.0,,qwen2.5-vl-32b,,,,0.933,,,qwen,,,,,,,,0.933,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.807428+00:00,,,,,,False,,False,0.0,False,0.0,0.933,0.933,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",Android Control Low_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android control benchmark evaluating autonomous agents on mobile device interaction tasks with low exact match scoring criteria,0.933,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,SR,,,androidworld-sr,AndroidWorld_SR,,,2025-07-19T19:56:14.815734+00:00,benchmark_result,,,,True,,,,,,1713.0,,qwen2.5-vl-32b,,,,0.22,,,qwen,,,,,,,,0.22,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.815734+00:00,,,,,,False,,False,0.0,False,0.0,0.22,0.22,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",AndroidWorld_SR,"['general', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"AndroidWorld Success Rate (SR) benchmark - A dynamic benchmarking environment for autonomous agents operating on Android devices. Evaluates agents on 116 programmatic tasks across 20 real-world Android apps using multimodal inputs (screen screenshots, accessibility trees, and natural language instructions). Measures success rate of agents completing tasks like sending messages, creating calendar events, and navigating mobile interfaces. Published at ICLR 2025. Best current performance: 30.6% success rate (M3A agent) vs 80.0% human performance.",0.22,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,cc-ocr,CC-OCR,,,2025-07-19T19:56:14.659496+00:00,benchmark_result,,,,True,,,,,,1658.0,,qwen2.5-vl-32b,,,,0.771,,,qwen,,,,,,,,0.771,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.659496+00:00,,,,,,False,,False,0.0,False,0.0,0.771,0.771,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",CC-OCR,"['vision', 'multimodal', 'text-to-image']",multimodal,True,1.0,en,"A comprehensive OCR benchmark for evaluating Large Multimodal Models (LMMs) in literacy. Comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. Contains 39 subsets with 7,058 fully annotated images, 41% sourced from real applications. Tests capabilities including text grounding, multi-orientation text recognition, and detecting hallucination/repetition across diverse visual challenges.",0.771,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,charadessta,CharadesSTA,,,2025-07-19T19:56:14.765807+00:00,benchmark_result,,,,True,,,,,,1695.0,,qwen2.5-vl-32b,,,,0.542,,,qwen,,,,,,,,0.542,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.765807+00:00,,,,,,False,,False,0.0,False,0.0,0.542,0.542,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",CharadesSTA,"['video', 'language', 'multimodal']",multimodal,False,1.0,en,"Charades-STA is a benchmark dataset for temporal activity localization via language queries, extending the Charades dataset with sentence temporal annotations. It contains 12,408 training and 3,720 testing segment-sentence pairs from videos with natural language descriptions and precise temporal boundaries for localizing activities based on language queries.",0.542,False,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,docvqa,DocVQA,,,2025-07-19T19:56:12.850117+00:00,benchmark_result,,,,True,,,,,,889.0,,qwen2.5-vl-32b,,,,0.948,,,qwen,,,,,,,,0.948,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:12.850117+00:00,,,,,,False,,False,0.0,False,0.0,0.948,0.948,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.948,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Excellent (90-95%),qwen2.5
,Score,,,gpqa,GPQA,,,2025-07-19T19:56:14.953480+00:00,benchmark_result,,,,True,,,,,,1751.0,,qwen2.5-vl-32b,,,,0.46,,,qwen,,,,,,,,0.46,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.953480+00:00,,,,,,False,,False,0.0,False,0.0,0.46,0.46,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.46,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,humaneval,HumanEval,,,2025-07-19T19:56:12.655022+00:00,benchmark_result,,,,True,,,,,,791.0,,qwen2.5-vl-32b,,,,0.915,,,qwen,,,,,,,,0.915,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:12.655022+00:00,,,,,,False,,False,0.0,False,0.0,0.915,0.915,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",HumanEval,"['reasoning', 'code']",text,False,1.0,en,"A benchmark that measures functional correctness for synthesizing programs from docstrings, consisting of 164 original programming problems assessing language comprehension, algorithms, and simple mathematics",0.915,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,Score,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.612560+00:00,benchmark_result,,,,True,,,,,,1243.0,,qwen2.5-vl-32b,,,,0.834,,,qwen,,,,,,,,0.834,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:13.612560+00:00,,,,,,False,,False,0.0,False,0.0,0.834,0.834,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.834,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,lvbench,LVBench,,,2025-07-19T19:56:12.733525+00:00,benchmark_result,,,,True,,,,,,830.0,,qwen2.5-vl-32b,,,,0.49,,,qwen,,,,,,,,0.49,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:12.733525+00:00,,,,,,False,,False,0.0,False,0.0,0.49,0.49,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",LVBench,"['vision', 'multimodal', 'long_context']",multimodal,False,1.0,en,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",0.49,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,math,MATH,,,2025-07-19T19:56:11.873375+00:00,benchmark_result,,,,True,,,,,,410.0,,qwen2.5-vl-32b,,,,0.822,,,qwen,,,,,,,,0.822,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:11.873375+00:00,,,,,,False,,False,0.0,False,0.0,0.822,0.822,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.822,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,mathvision,MathVision,,,2025-07-19T19:56:14.707439+00:00,benchmark_result,,,,True,,,,,,1678.0,,qwen2.5-vl-32b,,,,0.384,,,qwen,,,,,,,,0.384,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.707439+00:00,,,,,,False,,False,0.0,False,0.0,0.384,0.384,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MathVision,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",0.384,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.668155+00:00,benchmark_result,,,,True,,,,,,1272.0,,qwen2.5-vl-32b,,,,0.747,,,qwen,,,,,,,,0.747,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:13.668155+00:00,,,,,,False,,False,0.0,False,0.0,0.747,0.747,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.747,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mbpp,MBPP,,,2025-07-19T19:56:13.509907+00:00,benchmark_result,,,,True,,,,,,1191.0,,qwen2.5-vl-32b,,,,0.84,,,qwen,,,,,,,,0.84,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:13.509907+00:00,,,,,,False,,False,0.0,False,0.0,0.84,0.84,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.0084,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmbench-video,MMBench-Video,,,2025-07-19T19:56:14.747059+00:00,benchmark_result,,,,True,,,,,,1690.0,,qwen2.5-vl-32b,,,,0.0193,,,qwen,,,,,,,,0.0193,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.747059+00:00,,,,,,False,,False,0.0,False,0.0,0.0193,0.0193,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMBench-Video,"['video', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"A long-form multi-shot benchmark for holistic video understanding that incorporates approximately 600 web videos from YouTube spanning 16 major categories, with each video ranging from 30 seconds to 6 minutes. Includes roughly 2,000 original question-answer pairs covering 26 fine-grained capabilities.",0.0193,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmlu,MMLU,,,2025-07-19T19:56:11.274441+00:00,benchmark_result,,,,True,,,,,,92.0,,qwen2.5-vl-32b,,,,0.784,,,qwen,,,,,,,,0.784,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:11.274441+00:00,,,,,,False,,False,0.0,False,0.0,0.784,0.784,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.784,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.482355+00:00,benchmark_result,,,,True,,,,,,200.0,,qwen2.5-vl-32b,,,,0.688,,,qwen,,,,,,,,0.688,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:11.482355+00:00,,,,,,False,,False,0.0,False,0.0,0.688,0.688,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.688,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmmu,MMMU,,,2025-07-19T19:56:12.179390+00:00,benchmark_result,,,,True,,,,,,573.0,,qwen2.5-vl-32b,,,,0.7,,,qwen,,,,,,,,0.7,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:12.179390+00:00,,,,,,False,,False,0.0,False,0.0,0.7,0.7,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.7,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.299391+00:00,benchmark_result,,,,True,,,,,,1536.0,,qwen2.5-vl-32b,,,,0.495,,,qwen,,,,,,,,0.495,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.299391+00:00,,,,,,False,,False,0.0,False,0.0,0.495,0.495,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.495,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmstar,MMStar,,,2025-07-19T19:56:14.668445+00:00,benchmark_result,,,,True,,,,,,1662.0,,qwen2.5-vl-32b,,,,0.695,,,qwen,,,,,,,,0.695,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.668445+00:00,,,,,,False,,False,0.0,False,0.0,0.695,0.695,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.695,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,ocrbench-v2-(en),OCRBench-V2 (en),,,2025-07-19T19:56:14.930331+00:00,benchmark_result,,,,True,,,,,,1745.0,,qwen2.5-vl-32b,,,,0.572,,,qwen,,,,,,,,0.572,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.930331+00:00,,,,,,False,,False,0.0,False,0.0,0.572,0.572,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",OCRBench-V2 (en),"['vision', 'image-to-text']",multimodal,False,1.0,en,OCRBench v2 English subset: Enhanced benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with English text content,0.572,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,ocrbench-v2-(zh),OCRBench-V2 (zh),,,2025-07-19T19:56:14.947420+00:00,benchmark_result,,,,True,,,,,,1750.0,,qwen2.5-vl-32b,,,,0.591,,,qwen,,,,,,,,0.591,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.947420+00:00,,,,,,False,,False,0.0,False,0.0,0.591,0.591,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",OCRBench-V2 (zh),"['vision', 'image-to-text']",multimodal,False,1.0,zh,OCRBench v2 Chinese subset: Enhanced benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with Chinese text content,0.591,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,osworld,OSWorld,,,2025-07-19T19:56:14.939263+00:00,benchmark_result,,,,True,,,,,,1748.0,,qwen2.5-vl-32b,,,,0.0592,,,qwen,,,,,,,,0.0592,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.939263+00:00,,,,,,False,,False,0.0,False,0.0,0.0592,0.0592,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",OSWorld,"['multimodal', 'general', 'vision']",multimodal,False,1.0,en,"OSWorld: The first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across Ubuntu, Windows, and macOS with 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows",0.0592,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,screenspot,ScreenSpot,,,2025-07-19T19:56:14.775538+00:00,benchmark_result,,,,True,,,,,,1698.0,,qwen2.5-vl-32b,,,,0.885,,,qwen,,,,,,,,0.885,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.775538+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",ScreenSpot,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot is the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. The dataset comprises over 1,200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element types (text and icon/widget), designed to evaluate visual GUI agents' ability to accurately locate screen elements based on natural language instructions.",0.885,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,screenspot-pro,ScreenSpot Pro,,,2025-07-19T19:56:14.783897+00:00,benchmark_result,,,,True,,,,,,1701.0,,qwen2.5-vl-32b,,,,0.394,,,qwen,,,,,,,,0.394,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.783897+00:00,,,,,,False,,False,0.0,False,0.0,0.394,0.394,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",ScreenSpot Pro,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot-Pro is a novel GUI grounding benchmark designed to rigorously evaluate the grounding capabilities of multimodal large language models (MLLMs) in professional high-resolution computing environments. The benchmark comprises 1,581 instructions across 23 applications spanning 5 industries and 3 operating systems, featuring authentic high-resolution images from professional domains with expert annotations. Unlike previous benchmarks that focus on cropped screenshots in consumer applications, ScreenSpot-Pro addresses the complexity and diversity of real-world professional software scenarios, revealing significant performance gaps in current MLLM GUI perception capabilities.",0.394,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,videomme-w-o-sub.,VideoMME w/o sub.,,,2025-07-19T19:56:14.722056+00:00,benchmark_result,,,,True,,,,,,1683.0,,qwen2.5-vl-32b,,,,0.705,,,qwen,,,,,,,,0.705,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.722056+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",VideoMME w/o sub.,"['multimodal', 'video', 'vision']",multimodal,False,1.0,en,"Video-MME is a comprehensive evaluation benchmark for multi-modal large language models in video analysis. It features 900 videos across 6 primary visual domains with 30 subfields, ranging from 11 seconds to 1 hour in duration, with 2,700 question-answer pairs. The benchmark evaluates MLLMs' capabilities in processing sequential visual data and multi-modal content including video frames, subtitles, and audio.",0.705,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,videomme-w-sub.,VideoMME w sub.,,,2025-07-19T19:56:14.729388+00:00,benchmark_result,,,,True,,,,,,1686.0,,qwen2.5-vl-32b,,,,0.779,,,qwen,,,,,,,,0.779,https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct,,,,,,,,2025-07-19T19:56:14.729388+00:00,,,,,,False,,False,0.0,False,0.0,0.779,0.779,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 32B Instruct,qwen,Alibaba Cloud / Qwen Team,33500000000.0,33500000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-02-28,2025.0,2.0,2025-02,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",VideoMME w sub.,"['vision', 'multimodal', 'video']",multimodal,False,1.0,en,"The first-ever comprehensive evaluation benchmark of Multi-modal LLMs in Video analysis. Features 900 videos (254 hours) with 2,700 question-answer pairs covering 6 primary visual domains and 30 subfields. Evaluates temporal understanding across short (11 seconds) to long (1 hour) videos with multi-modal inputs including video frames, subtitles, and audio.",0.779,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,ai2d,AI2D,,,2025-07-19T19:56:13.635049+00:00,benchmark_result,,,,True,,,,,,1255.0,,qwen2.5-vl-72b,,,,0.884,,,qwen,,,,,,,,0.884,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:13.635049+00:00,,,,,,False,,False,0.0,False,0.0,0.884,0.884,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",AI2D,"['vision', 'reasoning', 'multimodal']",multimodal,False,1.0,en,"AI2D is a dataset of 4,903 illustrative diagrams from grade school natural sciences (such as food webs, human physiology, and life cycles) with over 15,000 multiple choice questions and answers. The benchmark evaluates diagram understanding and visual reasoning capabilities, requiring models to interpret diagrammatic elements, relationships, and structure to answer questions about scientific concepts represented in visual form.",0.884,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,EM,,,aitz-em,AITZ_EM,,,2025-07-19T19:56:14.789425+00:00,benchmark_result,,,,True,,,,,,1703.0,,qwen2.5-vl-72b,,,,0.832,,,qwen,,,,,,,,0.832,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.789425+00:00,,,,,,False,,False,0.0,False,0.0,0.832,0.832,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",AITZ_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,"Android-In-The-Zoo (AitZ) benchmark for evaluating autonomous GUI agents on smartphones. Contains 18,643 screen-action pairs with chain-of-action-thought annotations spanning over 70 Android apps. Designed to connect perception (screen layouts and UI elements) with cognition (action decision-making) for natural language-triggered smartphone task completion.",0.832,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,EM,,,android-control-high-em,Android Control High_EM,,,2025-07-19T19:56:14.796411+00:00,benchmark_result,,,,True,,,,,,1706.0,,qwen2.5-vl-72b,,,,0.6736,,,qwen,,,,,,,,0.6736,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.796411+00:00,,,,,,False,,False,0.0,False,0.0,0.6736,0.6736,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",Android Control High_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android device control benchmark using high exact match evaluation metric for assessing agent performance on mobile interface tasks,0.6736,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,EM,,,android-control-low-em,Android Control Low_EM,,,2025-07-19T19:56:14.805303+00:00,benchmark_result,,,,True,,,,,,1709.0,,qwen2.5-vl-72b,,,,0.937,,,qwen,,,,,,,,0.937,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.805303+00:00,,,,,,False,,False,0.0,False,0.0,0.937,0.937,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",Android Control Low_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android control benchmark evaluating autonomous agents on mobile device interaction tasks with low exact match scoring criteria,0.937,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,SR,,,androidworld-sr,AndroidWorld_SR,,,2025-07-19T19:56:14.813492+00:00,benchmark_result,,,,True,,,,,,1712.0,,qwen2.5-vl-72b,,,,0.35,,,qwen,,,,,,,,0.35,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.813492+00:00,,,,,,False,,False,0.0,False,0.0,0.35,0.35,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",AndroidWorld_SR,"['general', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"AndroidWorld Success Rate (SR) benchmark - A dynamic benchmarking environment for autonomous agents operating on Android devices. Evaluates agents on 116 programmatic tasks across 20 real-world Android apps using multimodal inputs (screen screenshots, accessibility trees, and natural language instructions). Measures success rate of agents completing tasks like sending messages, creating calendar events, and navigating mobile interfaces. Published at ICLR 2025. Best current performance: 30.6% success rate (M3A agent) vs 80.0% human performance.",0.35,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,cc-ocr,CC-OCR,,,2025-07-19T19:56:14.657333+00:00,benchmark_result,,,,True,,,,,,1657.0,,qwen2.5-vl-72b,,,,0.798,,,qwen,,,,,,,,0.798,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.657333+00:00,,,,,,False,,False,0.0,False,0.0,0.798,0.798,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",CC-OCR,"['vision', 'multimodal', 'text-to-image']",multimodal,True,1.0,en,"A comprehensive OCR benchmark for evaluating Large Multimodal Models (LMMs) in literacy. Comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. Contains 39 subsets with 7,058 fully annotated images, 41% sourced from real applications. Tests capabilities including text grounding, multi-orientation text recognition, and detecting hallucination/repetition across diverse visual challenges.",0.798,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,chartqa,ChartQA,,,2025-07-19T19:56:12.811401+00:00,benchmark_result,,,,True,,,,,,867.0,,qwen2.5-vl-72b,,,,0.895,,,qwen,,,,,,,,0.895,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:12.811401+00:00,,,,,,False,,False,0.0,False,0.0,0.895,0.895,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.895,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,docvqa,DocVQA,,,2025-07-19T19:56:12.848273+00:00,benchmark_result,,,,True,,,,,,888.0,,qwen2.5-vl-72b,,,,0.964,,,qwen,,,,,,,,0.964,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:12.848273+00:00,,,,,,False,,False,0.0,False,0.0,0.964,0.964,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.964,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),qwen2.5
,Score,,,egoschema,EgoSchema,,,2025-07-19T19:56:12.933582+00:00,benchmark_result,,,,True,,,,,,925.0,,qwen2.5-vl-72b,,,,0.762,,,qwen,,,,,,,,0.762,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:12.933582+00:00,,,,,,False,,False,0.0,False,0.0,0.762,0.762,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",EgoSchema,"['vision', 'reasoning', 'long_context']",video,False,1.0,en,"A diagnostic benchmark for very long-form video language understanding consisting of over 5000 human curated multiple choice questions based on 3-minute video clips from Ego4D, covering a broad range of natural human activities and behaviors",0.762,False,False,False,True,False,False,False,True,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,hallusion-bench,Hallusion Bench,,,2025-07-19T19:56:14.694733+00:00,benchmark_result,,,,True,,,,,,1673.0,,qwen2.5-vl-72b,,,,0.5516,,,qwen,,,,,,,,0.5516,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.694733+00:00,,,,,,False,,False,0.0,False,0.0,0.5516,0.5516,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",Hallusion Bench,"['vision', 'reasoning']",multimodal,False,1.0,en,"A comprehensive benchmark designed to evaluate image-context reasoning in large visual-language models (LVLMs) by challenging models with 346 images and 1,129 carefully crafted questions to assess language hallucination and visual illusion",0.5516,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,lvbench,LVBench,,,2025-07-19T19:56:12.731476+00:00,benchmark_result,,,,True,,,,,,829.0,,qwen2.5-vl-72b,,,,0.473,,,qwen,,,,,,,,0.473,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:12.731476+00:00,,,,,,False,,False,0.0,False,0.0,0.473,0.473,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",LVBench,"['vision', 'multimodal', 'long_context']",multimodal,False,1.0,en,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",0.473,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvision,MathVision,,,2025-07-19T19:56:14.705119+00:00,benchmark_result,,,,True,,,,,,1677.0,,qwen2.5-vl-72b,,,,0.381,,,qwen,,,,,,,,0.381,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.705119+00:00,,,,,,False,,False,0.0,False,0.0,0.381,0.381,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MathVision,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",0.381,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.666379+00:00,benchmark_result,,,,True,,,,,,1271.0,,qwen2.5-vl-72b,,,,0.748,,,qwen,,,,,,,,0.748,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:13.666379+00:00,,,,,,False,,False,0.0,False,0.0,0.748,0.748,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.748,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mlvu-m,MLVU-M,,,2025-07-19T19:56:14.934328+00:00,benchmark_result,,,,True,,,,,,1746.0,,qwen2.5-vl-72b,,,,0.746,,,qwen,,,,,,,,0.746,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.934328+00:00,,,,,,False,,False,0.0,False,0.0,0.746,0.746,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MLVU-M,['general'],text,False,1.0,en,MLVU-M benchmark,0.746,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmbench,MMBench,,,2025-07-19T19:56:14.243543+00:00,benchmark_result,,,,True,,,,,,1512.0,,qwen2.5-vl-72b,,,,0.88,,,qwen,,,,,,,,0.88,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.243543+00:00,,,,,,False,,False,0.0,False,0.0,0.88,0.88,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.88,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,mmbench-video,MMBench-Video,,,2025-07-19T19:56:14.744558+00:00,benchmark_result,,,,True,,,,,,1689.0,,qwen2.5-vl-72b,,,,0.0202,,,qwen,,,,,,,,0.0202,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.744558+00:00,,,,,,False,,False,0.0,False,0.0,0.0202,0.0202,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMBench-Video,"['video', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"A long-form multi-shot benchmark for holistic video understanding that incorporates approximately 600 web videos from YouTube spanning 16 major categories, with each video ranging from 30 seconds to 6 minutes. Includes roughly 2,000 original question-answer pairs covering 26 fine-grained capabilities.",0.0202,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmmu,MMMU,,,2025-07-19T19:56:12.177290+00:00,benchmark_result,,,,True,,,,,,572.0,,qwen2.5-vl-72b,,,,0.702,,,qwen,,,,,,,,0.702,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:12.177290+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.702,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.297757+00:00,benchmark_result,,,,True,,,,,,1535.0,,qwen2.5-vl-72b,,,,0.511,,,qwen,,,,,,,,0.511,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.297757+00:00,,,,,,False,,False,0.0,False,0.0,0.511,0.511,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.511,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmstar,MMStar,,,2025-07-19T19:56:14.666719+00:00,benchmark_result,,,,True,,,,,,1661.0,,qwen2.5-vl-72b,,,,0.708,,,qwen,,,,,,,,0.708,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.666719+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.708,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmvet,MMVet,,,2025-07-19T19:56:14.688513+00:00,benchmark_result,,,,True,,,,,,1671.0,,qwen2.5-vl-72b,,,,0.7619,,,qwen,,,,,,,,0.7619,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.688513+00:00,,,,,,False,,False,0.0,False,0.0,0.7619,0.7619,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MMVet,"['vision', 'multimodal', 'reasoning', 'general', 'spatial_reasoning', 'math']",multimodal,False,1.0,en,"MM-Vet is an evaluation benchmark that examines large multimodal models on complicated multimodal tasks requiring integrated capabilities. It assesses six core vision-language capabilities: recognition, knowledge, spatial awareness, language generation, OCR, and math through questions that require one or more of these capabilities.",0.7619,True,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,SR,,,mobileminiwob++-sr,MobileMiniWob++_SR,,,2025-07-19T19:56:14.820961+00:00,benchmark_result,,,,True,,,,,,1715.0,,qwen2.5-vl-72b,,,,0.68,,,qwen,,,,,,,,0.68,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.820961+00:00,,,,,,False,,False,0.0,False,0.0,0.68,0.68,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MobileMiniWob++_SR,"['multimodal', 'frontend_development']",multimodal,False,1.0,en,"MobileMiniWob++ SR (Success Rate) is an adaptation of the MiniWob++ web interaction benchmark for mobile Android environments within AndroidWorld. It comprises 92 web interaction tasks adapted for touch-based mobile interfaces, evaluating agents' ability to navigate and interact with web applications on mobile devices.",0.68,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mvbench,MVBench,,,2025-07-19T19:56:14.623550+00:00,benchmark_result,,,,True,,,,,,1644.0,,qwen2.5-vl-72b,,,,0.704,,,qwen,,,,,,,,0.704,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.623550+00:00,,,,,,False,,False,0.0,False,0.0,0.704,0.704,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",MVBench,"['vision', 'video', 'multimodal', 'spatial_reasoning', 'reasoning']",multimodal,False,1.0,en,"A comprehensive multi-modal video understanding benchmark covering 20 challenging video tasks that require temporal understanding beyond single-frame analysis. Tasks span from perception to cognition, including action recognition, temporal reasoning, spatial reasoning, object interaction, scene transition, and counterfactual inference. Uses a novel static-to-dynamic method to systematically generate video tasks from existing annotations.",0.704,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.318110+00:00,benchmark_result,,,,True,,,,,,1541.0,,qwen2.5-vl-72b,,,,0.885,,,qwen,,,,,,,,0.885,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.318110+00:00,,,,,,False,,False,0.0,False,0.0,0.885,0.885,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.885,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,ocrbench-v2-(en),OCRBench-V2 (en),,,2025-07-19T19:56:14.928710+00:00,benchmark_result,,,,True,,,,,,1744.0,,qwen2.5-vl-72b,,,,0.615,,,qwen,,,,,,,,0.615,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.928710+00:00,,,,,,False,,False,0.0,False,0.0,0.615,0.615,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",OCRBench-V2 (en),"['vision', 'image-to-text']",multimodal,False,1.0,en,OCRBench v2 English subset: Enhanced benchmark for evaluating Large Multimodal Models on visual text localization and reasoning with English text content,0.615,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,osworld,OSWorld,,,2025-07-19T19:56:14.937610+00:00,benchmark_result,,,,True,,,,,,1747.0,,qwen2.5-vl-72b,,,,0.0883,,,qwen,,,,,,,,0.0883,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.937610+00:00,,,,,,False,,False,0.0,False,0.0,0.0883,0.0883,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",OSWorld,"['multimodal', 'general', 'vision']",multimodal,False,1.0,en,"OSWorld: The first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across Ubuntu, Windows, and macOS with 369 computer tasks involving real web and desktop applications, OS file I/O, and multi-application workflows",0.0883,True,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,perceptiontest,PerceptionTest,,,2025-07-19T19:56:14.713944+00:00,benchmark_result,,,,True,,,,,,1680.0,,qwen2.5-vl-72b,,,,0.732,,,qwen,,,,,,,,0.732,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.713944+00:00,,,,,,False,,False,0.0,False,0.0,0.732,0.732,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",PerceptionTest,"['video', 'multimodal', 'reasoning', 'physics', 'spatial_reasoning']",multimodal,False,1.0,en,"A novel multimodal video benchmark designed to evaluate perception and reasoning skills of pre-trained models across video, audio, and text modalities. Contains 11.6k real-world videos (average 23 seconds) filmed by participants worldwide, densely annotated with six types of labels. Focuses on skills (Memory, Abstraction, Physics, Semantics) and reasoning types (descriptive, explanatory, predictive, counterfactual). Shows significant performance gap between human baseline (91.4%) and state-of-the-art video QA models (46.2%).",0.732,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,screenspot,ScreenSpot,,,2025-07-19T19:56:14.773284+00:00,benchmark_result,,,,True,,,,,,1697.0,,qwen2.5-vl-72b,,,,0.871,,,qwen,,,,,,,,0.871,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.773284+00:00,,,,,,False,,False,0.0,False,0.0,0.871,0.871,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",ScreenSpot,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot is the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. The dataset comprises over 1,200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element types (text and icon/widget), designed to evaluate visual GUI agents' ability to accurately locate screen elements based on natural language instructions.",0.871,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,screenspot-pro,ScreenSpot Pro,,,2025-07-19T19:56:14.780898+00:00,benchmark_result,,,,True,,,,,,1700.0,,qwen2.5-vl-72b,,,,0.436,,,qwen,,,,,,,,0.436,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.780898+00:00,,,,,,False,,False,0.0,False,0.0,0.436,0.436,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",ScreenSpot Pro,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot-Pro is a novel GUI grounding benchmark designed to rigorously evaluate the grounding capabilities of multimodal large language models (MLLMs) in professional high-resolution computing environments. The benchmark comprises 1,581 instructions across 23 applications spanning 5 industries and 3 operating systems, featuring authentic high-resolution images from professional domains with expert annotations. Unlike previous benchmarks that focus on cropped screenshots in consumer applications, ScreenSpot-Pro addresses the complexity and diversity of real-world professional software scenarios, revealing significant performance gaps in current MLLM GUI perception capabilities.",0.436,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,tempcompass,TempCompass,,,2025-07-19T19:56:14.754032+00:00,benchmark_result,,,,True,,,,,,1692.0,,qwen2.5-vl-72b,,,,0.748,,,qwen,,,,,,,,0.748,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.754032+00:00,,,,,,False,,False,0.0,False,0.0,0.748,0.748,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",TempCompass,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"TempCompass is a comprehensive benchmark for evaluating temporal perception capabilities of Video Large Language Models (Video LLMs). It constructs conflicting videos that share identical static content but differ in specific temporal aspects to prevent models from exploiting single-frame bias. The benchmark evaluates multiple temporal aspects including action, motion, speed, temporal order, and attribute changes across diverse task formats including multi-choice QA, yes/no QA, caption matching, and caption generation.",0.748,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,videomme-w-o-sub.,VideoMME w/o sub.,,,2025-07-19T19:56:14.720259+00:00,benchmark_result,,,,True,,,,,,1682.0,,qwen2.5-vl-72b,,,,0.733,,,qwen,,,,,,,,0.733,https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct,,,,,,,,2025-07-19T19:56:14.720259+00:00,,,,,,False,,False,0.0,False,0.0,0.733,0.733,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 72B Instruct,qwen,Alibaba Cloud / Qwen Team,72000000000.0,72000000000.0,True,0.0,False,True,tongyi_qianwen,Restricted/Community,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is the new flagship vision-language model of Qwen, significantly improved from Qwen2-VL. It excels at recognizing objects, analyzing text/charts/layouts in images, acting as a visual agent, understanding long videos (over 1 hour) with event pinpointing, performing visual localization (bounding boxes/points), and generating structured outputs from documents.",VideoMME w/o sub.,"['multimodal', 'video', 'vision']",multimodal,False,1.0,en,"Video-MME is a comprehensive evaluation benchmark for multi-modal large language models in video analysis. It features 900 videos across 6 primary visual domains with 30 subfields, ranging from 11 seconds to 1 hour in duration, with 2,700 question-answer pairs. The benchmark evaluates MLLMs' capabilities in processing sequential visual data and multi-modal content including video frames, subtitles, and audio.",0.733,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,EM,,,aitz-em,AITZ_EM,,,2025-07-19T19:56:14.787781+00:00,benchmark_result,,,,True,,,,,,1702.0,,qwen2.5-vl-7b,,,,0.819,,,qwen,,,,,,,,0.819,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.787781+00:00,,,,,,False,,False,0.0,False,0.0,0.819,0.819,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",AITZ_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,"Android-In-The-Zoo (AitZ) benchmark for evaluating autonomous GUI agents on smartphones. Contains 18,643 screen-action pairs with chain-of-action-thought annotations spanning over 70 Android apps. Designed to connect perception (screen layouts and UI elements) with cognition (action decision-making) for natural language-triggered smartphone task completion.",0.819,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen2.5
,EM,,,android-control-high-em,Android Control High_EM,,,2025-07-19T19:56:14.794879+00:00,benchmark_result,,,,True,,,,,,1705.0,,qwen2.5-vl-7b,,,,0.601,,,qwen,,,,,,,,0.601,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.794879+00:00,,,,,,False,,False,0.0,False,0.0,0.601,0.601,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",Android Control High_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android device control benchmark using high exact match evaluation metric for assessing agent performance on mobile interface tasks,0.601,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen2.5
,EM,,,android-control-low-em,Android Control Low_EM,,,2025-07-19T19:56:14.803305+00:00,benchmark_result,,,,True,,,,,,1708.0,,qwen2.5-vl-7b,,,,0.914,,,qwen,,,,,,,,0.914,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.803305+00:00,,,,,,False,,False,0.0,False,0.0,0.914,0.914,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",Android Control Low_EM,"['multimodal', 'reasoning']",multimodal,False,1.0,en,Android control benchmark evaluating autonomous agents on mobile device interaction tasks with low exact match scoring criteria,0.914,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,SR,,,androidworld-sr,AndroidWorld_SR,,,2025-07-19T19:56:14.811782+00:00,benchmark_result,,,,True,,,,,,1711.0,,qwen2.5-vl-7b,,,,0.255,,,qwen,,,,,,,,0.255,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.811782+00:00,,,,,,False,,False,0.0,False,0.0,0.255,0.255,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",AndroidWorld_SR,"['general', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"AndroidWorld Success Rate (SR) benchmark - A dynamic benchmarking environment for autonomous agents operating on Android devices. Evaluates agents on 116 programmatic tasks across 20 real-world Android apps using multimodal inputs (screen screenshots, accessibility trees, and natural language instructions). Measures success rate of agents completing tasks like sending messages, creating calendar events, and navigating mobile interfaces. Published at ICLR 2025. Best current performance: 30.6% success rate (M3A agent) vs 80.0% human performance.",0.255,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,cc-ocr,CC-OCR,,,2025-07-19T19:56:14.655251+00:00,benchmark_result,,,,True,,,,,,1656.0,,qwen2.5-vl-7b,,,,0.778,,,qwen,,,,,,,,0.778,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.655251+00:00,,,,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",CC-OCR,"['vision', 'multimodal', 'text-to-image']",multimodal,True,1.0,en,"A comprehensive OCR benchmark for evaluating Large Multimodal Models (LMMs) in literacy. Comprises four OCR-centric tracks: multi-scene text reading, multilingual text reading, document parsing, and key information extraction. Contains 39 subsets with 7,058 fully annotated images, 41% sourced from real applications. Tests capabilities including text grounding, multi-orientation text recognition, and detecting hallucination/repetition across diverse visual challenges.",0.778,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,mIoU,,,charadessta,CharadesSTA,,,2025-07-19T19:56:14.763802+00:00,benchmark_result,,,,True,,,,,,1694.0,,qwen2.5-vl-7b,,,,0.436,,,qwen,,,,,,,,0.436,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.763802+00:00,,,,,,False,,False,0.0,False,0.0,0.436,0.436,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",CharadesSTA,"['video', 'language', 'multimodal']",multimodal,False,1.0,en,"Charades-STA is a benchmark dataset for temporal activity localization via language queries, extending the Charades dataset with sentence temporal annotations. It contains 12,408 training and 3,720 testing segment-sentence pairs from videos with natural language descriptions and precise temporal boundaries for localizing activities based on language queries.",0.436,False,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,chartqa,ChartQA,,,2025-07-19T19:56:12.808329+00:00,benchmark_result,,,,True,,,,,,865.0,,qwen2.5-vl-7b,,,,0.873,,,qwen,,,,,,,,0.873,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:12.808329+00:00,,,,,,False,,False,0.0,False,0.0,0.873,0.873,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",ChartQA,"['reasoning', 'vision', 'multimodal']",multimodal,False,1.0,en,"ChartQA is a large-scale benchmark comprising 9.6K human-written questions and 23.1K questions generated from human-written chart summaries, designed to evaluate models' abilities in visual and logical reasoning over charts.",0.873,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,docvqa,DocVQA,,,2025-07-19T19:56:12.844347+00:00,benchmark_result,,,,True,,,,,,886.0,,qwen2.5-vl-7b,,,,0.957,,,qwen,,,,,,,,0.957,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:12.844347+00:00,,,,,,False,,False,0.0,False,0.0,0.957,0.957,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",DocVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"A dataset for Visual Question Answering on document images containing 50,000 questions defined on 12,000+ document images. The benchmark tests AI's ability to understand document structure and content, requiring models to comprehend document layout and perform information retrieval to answer questions about document images.",0.957,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,SOTA (95%+),qwen2.5
,Score,,,hallusion-bench,Hallusion Bench,,,2025-07-19T19:56:14.693096+00:00,benchmark_result,,,,True,,,,,,1672.0,,qwen2.5-vl-7b,,,,0.529,,,qwen,,,,,,,,0.529,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.693096+00:00,,,,,,False,,False,0.0,False,0.0,0.529,0.529,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",Hallusion Bench,"['vision', 'reasoning']",multimodal,False,1.0,en,"A comprehensive benchmark designed to evaluate image-context reasoning in large visual-language models (LVLMs) by challenging models with 346 images and 1,129 carefully crafted questions to assess language hallucination and visual illusion",0.529,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,infovqa,InfoVQA,,,2025-07-19T19:56:13.610945+00:00,benchmark_result,,,,True,,,,,,1242.0,,qwen2.5-vl-7b,,,,0.826,,,qwen,,,,,,,,0.826,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:13.610945+00:00,,,,,,False,,False,0.0,False,0.0,0.826,0.826,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",InfoVQA,"['vision', 'multimodal']",multimodal,False,1.0,en,"InfoVQA dataset with 30,000 questions and 5,000 infographic images requiring joint reasoning over document layout, textual content, graphical elements, and data visualizations with elementary reasoning and arithmetic skills",0.826,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,longvideobench,LongVideoBench,,,2025-07-19T19:56:14.737450+00:00,benchmark_result,,,,True,,,,,,1687.0,,qwen2.5-vl-7b,,,,0.547,,,qwen,,,,,,,,0.547,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.737450+00:00,,,,,,False,,False,0.0,False,0.0,0.547,0.547,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",LongVideoBench,"['vision', 'long_context', 'multimodal']",multimodal,False,1.0,en,"LongVideoBench is a question-answering benchmark featuring video-language interleaved inputs up to an hour long. It includes 3,763 varying-length web-collected videos with subtitles across diverse themes and 6,678 human-annotated multiple-choice questions in 17 fine-grained categories for comprehensive evaluation of long-term multimodal understanding.",0.547,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,lvbench,LVBench,,,2025-07-19T19:56:12.729778+00:00,benchmark_result,,,,True,,,,,,828.0,,qwen2.5-vl-7b,,,,0.453,,,qwen,,,,,,,,0.453,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:12.729778+00:00,,,,,,False,,False,0.0,False,0.0,0.453,0.453,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",LVBench,"['vision', 'multimodal', 'long_context']",multimodal,False,1.0,en,"LVBench is an extreme long video understanding benchmark designed to evaluate multimodal models on videos up to two hours in duration. It contains 6 major categories and 21 subcategories, with videos averaging five times longer than existing datasets. The benchmark addresses applications requiring comprehension of extremely long videos.",0.453,False,False,False,False,False,True,False,True,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvision,MathVision,,,2025-07-19T19:56:14.698748+00:00,benchmark_result,,,,True,,,,,,1674.0,,qwen2.5-vl-7b,,,,0.2507,,,qwen,,,,,,,,0.2507,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.698748+00:00,,,,,,False,,False,0.0,False,0.0,0.2507,0.2507,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MathVision,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MATH-Vision is a dataset designed to measure multimodal mathematical reasoning capabilities. It focuses on evaluating how well models can solve mathematical problems that require both visual understanding and mathematical reasoning, bridging the gap between visual and mathematical domains.",0.2507,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mathvista-mini,MathVista-Mini,,,2025-07-19T19:56:13.664381+00:00,benchmark_result,,,,True,,,,,,1270.0,,qwen2.5-vl-7b,,,,0.682,,,qwen,,,,,,,,0.682,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:13.664381+00:00,,,,,,False,,False,0.0,False,0.0,0.682,0.682,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MathVista-Mini,"['math', 'vision', 'multimodal']",multimodal,False,1.0,en,"MathVista-Mini is a smaller version of the MathVista benchmark that evaluates mathematical reasoning in visual contexts. It consists of examples derived from multimodal datasets involving mathematics, combining challenges from diverse mathematical and visual tasks to assess foundation models' ability to solve problems requiring both visual understanding and mathematical reasoning.",0.682,False,False,True,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mlvu,MLVU,,,2025-07-19T19:56:14.758833+00:00,benchmark_result,,,,True,,,,,,1693.0,,qwen2.5-vl-7b,,,,0.702,,,qwen,,,,,,,,0.702,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.758833+00:00,,,,,,False,,False,0.0,False,0.0,0.702,0.702,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MLVU,"['video', 'multimodal', 'long_context']",multimodal,False,1.0,en,"A comprehensive benchmark for multi-task long video understanding that evaluates multimodal large language models on videos ranging from 3 minutes to 2 hours across 9 distinct tasks including reasoning, captioning, recognition, and summarization.",0.702,False,False,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,mmbench,MMBench,,,2025-07-19T19:56:14.241869+00:00,benchmark_result,,,,True,,,,,,1511.0,,qwen2.5-vl-7b,,,,0.843,,,qwen,,,,,,,,0.843,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.241869+00:00,,,,,,False,,False,0.0,False,0.0,0.843,0.843,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMBench,"['vision', 'multimodal', 'reasoning']",multimodal,True,1.0,en,"A bilingual benchmark for assessing multi-modal capabilities of vision-language models through multiple-choice questions in both English and Chinese, providing systematic evaluation across diverse vision-language tasks with robust metrics.",0.843,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,mmbench-video,MMBench-Video,,,2025-07-19T19:56:14.742467+00:00,benchmark_result,,,,True,,,,,,1688.0,,qwen2.5-vl-7b,,,,0.0179,,,qwen,,,,,,,,0.0179,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.742467+00:00,,,,,,False,,False,0.0,False,0.0,0.0179,0.0179,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMBench-Video,"['video', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"A long-form multi-shot benchmark for holistic video understanding that incorporates approximately 600 web videos from YouTube spanning 16 major categories, with each video ranging from 30 seconds to 6 minutes. Includes roughly 2,000 original question-answer pairs covering 26 fine-grained capabilities.",0.0179,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmmu,MMMU,,,2025-07-19T19:56:12.170987+00:00,benchmark_result,,,,True,,,,,,569.0,,qwen2.5-vl-7b,,,,0.586,,,qwen,,,,,,,,0.586,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:12.170987+00:00,,,,,,False,,False,0.0,False,0.0,0.586,0.586,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMMU,"['multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMMU (Massive Multi-discipline Multimodal Understanding) is a benchmark designed to evaluate multimodal models on college-level subject knowledge and deliberate reasoning. Contains 11.5K meticulously collected multimodal questions from college exams, quizzes, and textbooks, covering six core disciplines: Art & Design, Business, Science, Health & Medicine, Humanities & Social Science, and Tech & Engineering across 30 subjects and 183 subfields.",0.586,True,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmmu-pro,MMMU-Pro,,,2025-07-19T19:56:14.294582+00:00,benchmark_result,,,,True,,,,,,1533.0,,qwen2.5-vl-7b,,,,0.383,,,qwen,,,,,,,,0.383,https://github.com/QwenLM/Qwen2.5-VL,,,,,,,,2025-07-19T19:56:14.294582+00:00,,,,,,False,,False,0.0,False,0.0,0.383,0.383,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMMU-Pro,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"A more robust multi-discipline multimodal understanding benchmark that enhances MMMU through a three-step process: filtering text-only answerable questions, augmenting candidate options, and introducing vision-only input settings. Achieves significantly lower model performance (16.8-26.9%) compared to original MMMU, providing more rigorous evaluation that closely mimics real-world scenarios.",0.383,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,mmstar,MMStar,,,2025-07-19T19:56:14.662888+00:00,benchmark_result,,,,True,,,,,,1659.0,,qwen2.5-vl-7b,,,,0.639,,,qwen,,,,,,,,0.639,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.662888+00:00,,,,,,False,,False,0.0,False,0.0,0.639,0.639,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMStar,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMStar is an elite vision-indispensable multimodal benchmark comprising 1,500 challenge samples meticulously selected by humans to evaluate 6 core capabilities and 18 detailed axes. The benchmark addresses issues of visual content unnecessity and unintentional data leakage in existing multimodal evaluations.",0.639,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmt-bench,MMT-Bench,,,2025-07-19T19:56:14.676869+00:00,benchmark_result,,,,True,,,,,,1666.0,,qwen2.5-vl-7b,,,,0.636,,,qwen,,,,,,,,0.636,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.676869+00:00,,,,,,False,,False,0.0,False,0.0,0.636,0.636,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMT-Bench,"['vision', 'multimodal', 'reasoning', 'general']",multimodal,False,1.0,en,"MMT-Bench is a comprehensive multimodal benchmark for evaluating Large Vision-Language Models towards multitask AGI. It comprises 31,325 meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering 32 core meta-tasks and 162 subtasks in multimodal understanding.",0.636,True,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,mmvet,MMVet,,,2025-07-19T19:56:14.687023+00:00,benchmark_result,,,,True,,,,,,1670.0,,qwen2.5-vl-7b,,,,0.671,,,qwen,,,,,,,,0.671,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.687023+00:00,,,,,,False,,False,0.0,False,0.0,0.671,0.671,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MMVet,"['vision', 'multimodal', 'reasoning', 'general', 'spatial_reasoning', 'math']",multimodal,False,1.0,en,"MM-Vet is an evaluation benchmark that examines large multimodal models on complicated multimodal tasks requiring integrated capabilities. It assesses six core vision-language capabilities: recognition, knowledge, spatial awareness, language generation, OCR, and math through questions that require one or more of these capabilities.",0.671,True,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,SR,,,mobileminiwob++-sr,MobileMiniWob++_SR,,,2025-07-19T19:56:14.819401+00:00,benchmark_result,,,,True,,,,,,1714.0,,qwen2.5-vl-7b,,,,0.914,,,qwen,,,,,,,,0.914,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.819401+00:00,,,,,,False,,False,0.0,False,0.0,0.914,0.914,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MobileMiniWob++_SR,"['multimodal', 'frontend_development']",multimodal,False,1.0,en,"MobileMiniWob++ SR (Success Rate) is an adaptation of the MiniWob++ web interaction benchmark for mobile Android environments within AndroidWorld. It comprises 92 web interaction tasks adapted for touch-based mobile interfaces, evaluating agents' ability to navigate and interact with web applications on mobile devices.",0.914,False,False,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen2.5
,Score,,,mvbench,MVBench,,,2025-07-19T19:56:14.620310+00:00,benchmark_result,,,,True,,,,,,1642.0,,qwen2.5-vl-7b,,,,0.696,,,qwen,,,,,,,,0.696,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.620310+00:00,,,,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",MVBench,"['vision', 'video', 'multimodal', 'spatial_reasoning', 'reasoning']",multimodal,False,1.0,en,"A comprehensive multi-modal video understanding benchmark covering 20 challenging video tasks that require temporal understanding beyond single-frame analysis. Tasks span from perception to cognition, including action recognition, temporal reasoning, spatial reasoning, object interaction, scene transition, and counterfactual inference. Uses a novel static-to-dynamic method to systematically generate video tasks from existing annotations.",0.696,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,ocrbench,OCRBench,,,2025-07-19T19:56:14.315649+00:00,benchmark_result,,,,True,,,,,,1540.0,,qwen2.5-vl-7b,,,,0.864,,,qwen,,,,,,,,0.864,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.315649+00:00,,,,,,False,,False,0.0,False,0.0,0.864,0.864,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",OCRBench,"['vision', 'image-to-text']",multimodal,False,1.0,en,"OCRBench: Comprehensive evaluation benchmark for assessing Optical Character Recognition (OCR) capabilities in Large Multimodal Models across text recognition, scene text VQA, and document understanding tasks",0.864,False,False,False,False,False,False,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,perceptiontest,PerceptionTest,,,2025-07-19T19:56:14.712010+00:00,benchmark_result,,,,True,,,,,,1679.0,,qwen2.5-vl-7b,,,,0.705,,,qwen,,,,,,,,0.705,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.712010+00:00,,,,,,False,,False,0.0,False,0.0,0.705,0.705,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",PerceptionTest,"['video', 'multimodal', 'reasoning', 'physics', 'spatial_reasoning']",multimodal,False,1.0,en,"A novel multimodal video benchmark designed to evaluate perception and reasoning skills of pre-trained models across video, audio, and text modalities. Contains 11.6k real-world videos (average 23 seconds) filmed by participants worldwide, densely annotated with six types of labels. Focuses on skills (Memory, Abstraction, Physics, Semantics) and reasoning types (descriptive, explanatory, predictive, counterfactual). Shows significant performance gap between human baseline (91.4%) and state-of-the-art video QA models (46.2%).",0.705,False,False,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,screenspot,ScreenSpot,,,2025-07-19T19:56:14.771516+00:00,benchmark_result,,,,True,,,,,,1696.0,,qwen2.5-vl-7b,,,,0.847,,,qwen,,,,,,,,0.847,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.771516+00:00,,,,,,False,,False,0.0,False,0.0,0.847,0.847,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",ScreenSpot,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot is the first realistic GUI grounding benchmark that encompasses mobile, desktop, and web environments. The dataset comprises over 1,200 instructions from iOS, Android, macOS, Windows and Web environments, along with annotated element types (text and icon/widget), designed to evaluate visual GUI agents' ability to accurately locate screen elements based on natural language instructions.",0.847,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,screenspot-pro,ScreenSpot Pro,,,2025-07-19T19:56:14.779312+00:00,benchmark_result,,,,True,,,,,,1699.0,,qwen2.5-vl-7b,,,,0.29,,,qwen,,,,,,,,0.29,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.779312+00:00,,,,,,False,,False,0.0,False,0.0,0.29,0.29,Unknown,,,,,Undisclosed,Poor (<60%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",ScreenSpot Pro,"['vision', 'multimodal', 'spatial_reasoning']",multimodal,False,1.0,en,"ScreenSpot-Pro is a novel GUI grounding benchmark designed to rigorously evaluate the grounding capabilities of multimodal large language models (MLLMs) in professional high-resolution computing environments. The benchmark comprises 1,581 instructions across 23 applications spanning 5 industries and 3 operating systems, featuring authentic high-resolution images from professional domains with expert annotations. Unlike previous benchmarks that focus on cropped screenshots in consumer applications, ScreenSpot-Pro addresses the complexity and diversity of real-world professional software scenarios, revealing significant performance gaps in current MLLM GUI perception capabilities.",0.29,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen2.5
,Score,,,tempcompass,TempCompass,,,2025-07-19T19:56:14.752008+00:00,benchmark_result,,,,True,,,,,,1691.0,,qwen2.5-vl-7b,,,,0.717,,,qwen,,,,,,,,0.717,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.752008+00:00,,,,,,False,,False,0.0,False,0.0,0.717,0.717,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",TempCompass,"['vision', 'multimodal', 'reasoning']",multimodal,False,1.0,en,"TempCompass is a comprehensive benchmark for evaluating temporal perception capabilities of Video Large Language Models (Video LLMs). It constructs conflicting videos that share identical static content but differ in specific temporal aspects to prevent models from exploiting single-frame bias. The benchmark evaluates multiple temporal aspects including action, motion, speed, temporal order, and attribute changes across diverse task formats including multi-choice QA, yes/no QA, caption matching, and caption generation.",0.717,False,False,False,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Score,,,textvqa,TextVQA,,,2025-07-19T19:56:12.896871+00:00,benchmark_result,,,,True,,,,,,910.0,,qwen2.5-vl-7b,,,,0.849,,,qwen,,,,,,,,0.849,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:12.896871+00:00,,,,,,False,,False,0.0,False,0.0,0.849,0.849,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",TextVQA,"['vision', 'multimodal', 'image-to-text']",multimodal,False,1.0,en,"TextVQA contains 45,336 questions on 28,408 images that require reasoning about text to answer. Introduced to benchmark VQA models' ability to read and reason about text within images, particularly for assistive technologies for visually impaired users. The dataset addresses the gap where existing VQA datasets had few text-based questions or were too small.",0.849,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Very Good (80-90%),qwen2.5
,Score,,,videomme-w-o-sub.,VideoMME w/o sub.,,,2025-07-19T19:56:14.718319+00:00,benchmark_result,,,,True,,,,,,1681.0,,qwen2.5-vl-7b,,,,0.651,,,qwen,,,,,,,,0.651,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.718319+00:00,,,,,,False,,False,0.0,False,0.0,0.651,0.651,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",VideoMME w/o sub.,"['multimodal', 'video', 'vision']",multimodal,False,1.0,en,"Video-MME is a comprehensive evaluation benchmark for multi-modal large language models in video analysis. It features 900 videos across 6 primary visual domains with 30 subfields, ranging from 11 seconds to 1 hour in duration, with 2,700 question-answer pairs. The benchmark evaluates MLLMs' capabilities in processing sequential visual data and multi-modal content including video frames, subtitles, and audio.",0.651,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen2.5
,Score,,,videomme-w-sub.,VideoMME w sub.,,,2025-07-19T19:56:14.726358+00:00,benchmark_result,,,,True,,,,,,1684.0,,qwen2.5-vl-7b,,,,0.716,,,qwen,,,,,,,,0.716,https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct,,,,,,,,2025-07-19T19:56:14.726358+00:00,,,,,,False,,False,0.0,False,0.0,0.716,0.716,Unknown,,,,,Undisclosed,Good (70-79%),Qwen2.5 VL 7B Instruct,qwen,Alibaba Cloud / Qwen Team,8290000000.0,8290000000.0,True,0.0,False,True,apache_2_0,Open & Permissive,2025-01-26,2025.0,1.0,2025-01,Very Large (>70B),"Qwen2.5-VL is a vision-language model from the Qwen family. Key enhancements include visual understanding (objects, text, charts, layouts), visual agent capabilities (tool use, computer/phone control), long video comprehension with event pinpointing, visual localization (bounding boxes/points), and structured output generation.",VideoMME w sub.,"['vision', 'multimodal', 'video']",multimodal,False,1.0,en,"The first-ever comprehensive evaluation benchmark of Multi-modal LLMs in Video analysis. Features 900 videos (254 hours) with 2,700 question-answer pairs covering 6 primary visual domains and 30 subfields. Evaluates temporal understanding across short (11 seconds) to long (1 hour) videos with multi-modal inputs including video frames, subtitles, and audio.",0.716,False,False,False,False,False,True,False,False,False,False,False,True,False,False,False,False,False,Good (70-80%),qwen2.5
,Pass@2,,,aider,Aider,,,2025-07-19T19:56:14.572970+00:00,benchmark_result,,,,True,,,,,,1626.0,,qwen3-235b-a22b,,,,0.618,,,qwen,,,,,,,,0.618,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.572970+00:00,,,,,,False,,False,0.0,False,0.0,0.618,0.618,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",Aider,"['reasoning', 'code']",text,False,1.0,en,"Aider is a comprehensive code editing benchmark based on 133 practice exercises from Exercism's Python repository, designed to evaluate AI models' ability to translate natural language coding requests into executable code that passes unit tests. The benchmark measures end-to-end code editing capabilities, including GPT's ability to edit existing code and format code changes for automated saving to local files. The Aider Polyglot variant extends this evaluation across 225 challenging exercises spanning C++, Go, Java, JavaScript, Python, and Rust, making it a standard benchmark for assessing multilingual code editing performance in AI research.",0.618,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Pass@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.963641+00:00,benchmark_result,,,,True,,,,,,454.0,,qwen3-235b-a22b,,,,0.857,,,qwen,,,,,,,,0.857,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.963641+00:00,,,,,,False,,False,0.0,False,0.0,0.857,0.857,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.857,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Pass@64,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.447678+00:00,benchmark_result,,,,True,,,,,,690.0,,qwen3-235b-a22b,,,,0.815,,,qwen,,,,,,,,0.815,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.447678+00:00,,,,,,False,,False,0.0,False,0.0,0.815,0.815,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.815,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.095282+00:00,benchmark_result,,,,True,,,,,,1452.0,,qwen3-235b-a22b,,,,0.956,,,qwen,,,,,,,,0.956,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.095282+00:00,,,,,,False,,False,0.0,False,0.0,0.956,0.956,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.956,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),qwen3
,Accuracy,,,bbh,BBH,,,2025-07-19T19:56:13.043683+00:00,benchmark_result,,,,True,,,,,,972.0,,qwen3-235b-a22b,,,,0.8887,,,qwen,,,,,,,,0.8887,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.043683+00:00,,,,,,False,,False,0.0,False,0.0,0.8887,0.8887,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",BBH,"['reasoning', 'math', 'language']",text,False,1.0,en,"Big-Bench Hard (BBH) is a suite of 23 challenging tasks selected from BIG-Bench for which prior language model evaluations did not outperform the average human-rater. These tasks require multi-step reasoning across diverse domains including arithmetic, logical reasoning, reading comprehension, and commonsense reasoning. The benchmark was designed to test capabilities believed to be beyond current language models and focuses on evaluating complex reasoning skills including temporal understanding, spatial reasoning, causal understanding, and deductive logical reasoning.",0.8887,False,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,v3,,,bfcl,BFCL,,,2025-07-19T19:56:12.780457+00:00,benchmark_result,,,,True,,,,,,851.0,,qwen3-235b-a22b,,,,0.708,,,qwen,,,,,,,,0.708,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.780457+00:00,,,,,,False,,False,0.0,False,0.0,0.708,0.708,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.708,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Score,,,crux-o,CRUX-O,,,2025-07-19T19:56:14.637715+00:00,benchmark_result,,,,True,,,,,,1648.0,,qwen3-235b-a22b,,,,0.79,,,qwen,,,,,,,,0.79,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.637715+00:00,,,,,,False,,False,0.0,False,0.0,0.79,0.79,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",CRUX-O,['reasoning'],text,False,100.0,en,"CRUXEval-O (output prediction) is part of the CRUXEval benchmark consisting of 800 Python functions (3-13 lines) designed to evaluate AI models' capabilities in code reasoning, understanding, and execution. The benchmark tests models' ability to predict correct function outputs given function code and inputs, focusing on short problems that a good human programmer should be able to solve in a minute.",0.0079,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Score,,,evalplus,EvalPlus,,,2025-07-19T19:56:11.801301+00:00,benchmark_result,,,,True,,,,,,371.0,,qwen3-235b-a22b,,,,0.776,,,qwen,,,,,,,,0.776,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.801301+00:00,,,,,,False,,False,0.0,False,0.0,0.776,0.776,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",EvalPlus,"['reasoning', 'code']",text,False,100.0,en,"A rigorous code synthesis evaluation framework that augments existing datasets with extensive test cases generated by LLM and mutation-based strategies to better assess functional correctness of generated code, including HumanEval+ with 80x more test cases",0.00776,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.679464+00:00,benchmark_result,,,,True,,,,,,302.0,,qwen3-235b-a22b,,,,0.4747,,,qwen,,,,,,,,0.4747,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.679464+00:00,,,,,,False,,False,0.0,False,0.0,0.4747,0.4747,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.4747,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,gsm8k,GSM8k,,,2025-07-19T19:56:13.083824+00:00,benchmark_result,,,,True,,,,,,995.0,,qwen3-235b-a22b,,,,0.9439,,,qwen,,,,,,,,0.9439,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.083824+00:00,,,,,,False,,False,0.0,False,0.0,0.9439,0.9439,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",GSM8k,"['math', 'reasoning']",text,False,1.0,en,"Grade School Math 8K, a dataset of 8.5K high-quality linguistically diverse grade school math word problems requiring multi-step reasoning and elementary arithmetic operations.",0.9439,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,Score,,,include,Include,,,2025-07-19T19:56:13.737543+00:00,benchmark_result,,,,True,,,,,,1308.0,,qwen3-235b-a22b,,,,0.7346,,,qwen,,,,,,,,0.7346,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.737543+00:00,,,,,,False,,False,0.0,False,0.0,0.7346,0.7346,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.7346,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,livebench,LiveBench,,,2025-07-19T19:56:12.575629+00:00,benchmark_result,,,,True,,,,,,749.0,,qwen3-235b-a22b,,,,0.771,,,qwen,,,,,,,,0.771,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.575629+00:00,,,,,,False,,False,0.0,False,0.0,0.771,0.771,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.771,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,v5,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.344206+00:00,benchmark_result,,,,True,,,,,,1123.0,,qwen3-235b-a22b,,,,0.707,,,qwen,,,,,,,,0.707,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.344206+00:00,,,,,,False,,False,0.0,False,0.0,0.707,0.707,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.707,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,math,MATH,,,2025-07-19T19:56:11.863985+00:00,benchmark_result,,,,True,,,,,,405.0,,qwen3-235b-a22b,,,,0.7184,,,qwen,,,,,,,,0.7184,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.863985+00:00,,,,,,False,,False,0.0,False,0.0,0.7184,0.7184,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MATH,"['math', 'reasoning']",text,False,1.0,en,"MATH dataset contains 12,500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels (1-5) across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.7184,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,mbpp,MBPP,,,2025-07-19T19:56:13.500617+00:00,benchmark_result,,,,True,,,,,,1186.0,,qwen3-235b-a22b,,,,0.814,,,qwen,,,,,,,,0.814,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.500617+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MBPP,"['reasoning', 'general']",text,False,100.0,en,"MBPP (Mostly Basic Python Problems) is a benchmark of 974 crowd-sourced Python programming problems designed to be solvable by entry-level programmers. Each problem consists of a task description, code solution, and 3 automated test cases covering programming fundamentals and standard library functionality.",0.00814,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,mgsm,MGSM,,,2025-07-19T19:56:13.700097+00:00,benchmark_result,,,,True,,,,,,1289.0,,qwen3-235b-a22b,,,,0.8353,,,qwen,,,,,,,,0.8353,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.700097+00:00,,,,,,False,,False,0.0,False,0.0,0.8353,0.8353,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MGSM,"['math', 'reasoning']",text,True,1.0,en,"MGSM (Multilingual Grade School Math) is a benchmark of grade-school math problems. Contains 250 grade-school math problems manually translated from the GSM8K dataset into ten typologically diverse languages: Spanish, French, German, Russian, Chinese, Japanese, Thai, Swahili, Bengali, and Telugu. Evaluates multilingual mathematical reasoning capabilities.",0.8353,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,mmlu,MMLU,,,2025-07-19T19:56:11.270963+00:00,benchmark_result,,,,True,,,,,,90.0,,qwen3-235b-a22b,,,,0.8781,,,qwen,,,,,,,,0.8781,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.270963+00:00,,,,,,False,,False,0.0,False,0.0,0.8781,0.8781,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MMLU,"['general', 'reasoning', 'language', 'math']",text,False,1.0,en,"Massive Multitask Language Understanding benchmark testing knowledge across 57 diverse subjects including STEM, humanities, social sciences, and professional domains",0.8781,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-07-19T19:56:11.472627+00:00,benchmark_result,,,,True,,,,,,195.0,,qwen3-235b-a22b,,,,0.6818,,,qwen,,,,,,,,0.6818,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.472627+00:00,,,,,,False,,False,0.0,False,0.0,0.6818,0.6818,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.6818,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,mmlu-redux,MMLU-Redux,,,2025-07-19T19:56:12.540685+00:00,benchmark_result,,,,True,,,,,,732.0,,qwen3-235b-a22b,,,,0.874,,,qwen,,,,,,,,0.874,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.540685+00:00,,,,,,False,,False,0.0,False,0.0,0.874,0.874,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.874,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,mmmlu,MMMLU,,,2025-07-19T19:56:14.150792+00:00,benchmark_result,,,,True,,,,,,1477.0,,qwen3-235b-a22b,,,,0.867,,,qwen,,,,,,,,0.867,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.150792+00:00,,,,,,False,,False,0.0,False,0.0,0.867,0.867,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MMMLU,"['language', 'reasoning', 'math', 'general']",text,True,1.0,en,"Multilingual Massive Multitask Language Understanding dataset released by OpenAI, featuring professionally translated MMLU test questions across 14 languages including Arabic, Bengali, German, Spanish, French, Hindi, Indonesian, Italian, Japanese, Korean, Portuguese, Swahili, Yoruba, and Chinese. Contains approximately 15,908 multiple-choice questions per language covering 57 subjects.",0.867,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,multilf,MultiLF,,,2025-07-19T19:56:14.633963+00:00,benchmark_result,,,,True,,,,,,1647.0,,qwen3-235b-a22b,,,,0.719,,,qwen,,,,,,,,0.719,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.633963+00:00,,,,,,False,,False,0.0,False,0.0,0.719,0.719,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MultiLF,['general'],text,False,1.0,en,MultiLF benchmark,0.719,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Score,,,multipl-e,MultiPL-E,,,2025-07-19T19:56:12.320821+00:00,benchmark_result,,,,True,,,,,,643.0,,qwen3-235b-a22b,,,,0.6594,,,qwen,,,,,,,,0.6594,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.320821+00:00,,,,,,False,,False,0.0,False,0.0,0.6594,0.6594,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.6594,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,supergpqa,SuperGPQA,,,2025-07-19T19:56:11.784624+00:00,benchmark_result,,,,True,,,,,,366.0,,qwen3-235b-a22b,,,,0.4406,,,qwen,,,,,,,,0.4406,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.784624+00:00,,,,,,False,,False,0.0,False,0.0,0.4406,0.4406,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3 235B A22B,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3 235B A22B is a large language model developed by Alibaba, featuring a Mixture-of-Experts (MoE) architecture with 235 billion total parameters and 22 billion activated parameters. It achieves competitive results in benchmark evaluations of coding, math, general capabilities, and more, compared to other top-tier models.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.4406,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,aider-polyglot,Aider-Polyglot,,,2025-08-03T22:06:13.609026+00:00,benchmark_result,,,,True,,,,,,15972.0,,qwen3-235b-a22b-instruct-2507,,,,0.573,,,qwen,,,,,,,,0.573,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.609026+00:00,,,,,,False,,False,0.0,False,0.0,0.573,0.573,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.573,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,aime-2025,AIME 2025,,,2025-08-03T22:06:13.611021+00:00,benchmark_result,,,,True,,,,,,15973.0,,qwen3-235b-a22b-instruct-2507,,,,0.703,,,qwen,,,,,,,,0.703,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.611021+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.703,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,arc-agi,ARC-AGI,,,2025-08-03T22:06:13.618116+00:00,benchmark_result,,,,True,,,,,,15974.0,,qwen3-235b-a22b-instruct-2507,,,,0.418,,,qwen,,,,,,,,0.418,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.618116+00:00,,,,,,False,,False,0.0,False,0.0,0.418,0.418,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",ARC-AGI,"['reasoning', 'vision', 'spatial_reasoning']",image,False,1.0,en,"The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a benchmark designed to test general intelligence and abstract reasoning capabilities through visual grid-based transformation tasks. Each task consists of 2-5 demonstration pairs showing input grids transformed into output grids according to underlying rules, with test-takers required to infer these rules and apply them to novel test inputs. The benchmark uses colored grids (up to 30x30) with 10 discrete colors/symbols, designed to measure human-like general fluid intelligence and skill-acquisition efficiency with minimal prior knowledge.",0.418,False,False,False,True,False,False,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen3
,Win Rate,,,arena-hard-v2,Arena-Hard v2,,,2025-08-03T22:06:13.620187+00:00,benchmark_result,,,,True,,,,,,15975.0,,qwen3-235b-a22b-instruct-2507,,,,0.792,,,qwen,,,,,,,,0.792,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.620187+00:00,,,,,,False,,False,0.0,False,0.0,0.792,0.792,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Arena-Hard v2,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",0.792,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,bfcl-v3,BFCL-v3,,,2025-08-03T22:06:13.622144+00:00,benchmark_result,,,,True,,,,,,15976.0,,qwen3-235b-a22b-instruct-2507,,,,0.709,,,qwen,,,,,,,,0.709,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.622144+00:00,,,,,,False,,False,0.0,False,0.0,0.709,0.709,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.709,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Score,,,creative-writing-v3,Creative Writing v3,,,2025-08-03T22:06:13.626065+00:00,benchmark_result,,,,True,,,,,,15977.0,,qwen3-235b-a22b-instruct-2507,,,,0.875,,,qwen,,,,,,,,0.875,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.626065+00:00,,,,,,False,,False,0.0,False,0.0,0.875,0.875,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Creative Writing v3,"['creativity', 'writing']",text,False,1.0,en,"EQ-Bench Creative Writing v3 is an LLM-judged creative writing benchmark that evaluates models across 32 writing prompts with 3 iterations per prompt. Uses a hybrid scoring system combining rubric assessment and Elo ratings through pairwise comparisons. Challenges models in areas like humor, romance, spatial awareness, and unique perspectives to assess emotional intelligence and creative writing capabilities.",0.875,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,csimpleqa,CSimpleQA,,,2025-08-03T22:06:13.629696+00:00,benchmark_result,,,,True,,,,,,15978.0,,qwen3-235b-a22b-instruct-2507,,,,0.843,,,qwen,,,,,,,,0.843,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.629696+00:00,,,,,,False,,False,0.0,False,0.0,0.843,0.843,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",CSimpleQA,"['general', 'language']",text,True,1.0,en,"Chinese SimpleQA is the first comprehensive Chinese benchmark to evaluate the factuality ability of language models to answer short questions. It contains 3,000 high-quality questions spanning 6 major topics with 99 diverse subtopics, designed to assess Chinese factual knowledge across humanities, science, engineering, culture, and society.",0.843,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,gpqa,GPQA,,,2025-08-03T22:06:13.631769+00:00,benchmark_result,,,,True,,,,,,15979.0,,qwen3-235b-a22b-instruct-2507,,,,0.775,,,qwen,,,,,,,,0.775,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.631769+00:00,,,,,,False,,False,0.0,False,0.0,0.775,0.775,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.775,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,hmmt25,HMMT25,,,2025-08-03T22:06:13.633387+00:00,benchmark_result,,,,True,,,,,,15980.0,,qwen3-235b-a22b-instruct-2507,,,,0.554,,,qwen,,,,,,,,0.554,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.633387+00:00,,,,,,False,,False,0.0,False,0.0,0.554,0.554,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.554,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,ifeval,IFEval,,,2025-08-03T22:06:13.635001+00:00,benchmark_result,,,,True,,,,,,15981.0,,qwen3-235b-a22b-instruct-2507,,,,0.887,,,qwen,,,,,,,,0.887,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.635001+00:00,,,,,,False,,False,0.0,False,0.0,0.887,0.887,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.887,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Score,,,include,INCLUDE,,,2025-08-03T22:06:13.636605+00:00,benchmark_result,,,,True,,,,,,15982.0,,qwen3-235b-a22b-instruct-2507,,,,0.795,,,qwen,,,,,,,,0.795,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.636605+00:00,,,,,,False,,False,0.0,False,0.0,0.795,0.795,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.795,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,livebench-20241125,LiveBench 20241125,,,2025-08-03T22:06:13.638166+00:00,benchmark_result,,,,True,,,,,,15983.0,,qwen3-235b-a22b-instruct-2507,,,,0.754,,,qwen,,,,,,,,0.754,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.638166+00:00,,,,,,False,,False,0.0,False,0.0,0.754,0.754,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",LiveBench 20241125,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.754,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,livecodebench-v6,LiveCodeBench v6,,,2025-08-03T22:06:13.639661+00:00,benchmark_result,,,,True,,,,,,15984.0,,qwen3-235b-a22b-instruct-2507,,,,0.518,,,qwen,,,,,,,,0.518,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.639661+00:00,,,,,,False,,False,0.0,False,0.0,0.518,0.518,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.518,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,mmlu-pro,MMLU-Pro,,,2025-08-03T22:06:13.641236+00:00,benchmark_result,,,,True,,,,,,15985.0,,qwen3-235b-a22b-instruct-2507,,,,0.83,,,qwen,,,,,,,,0.83,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.641236+00:00,,,,,,False,,False,0.0,False,0.0,0.83,0.83,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.83,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,mmlu-prox,MMLU-ProX,,,2025-08-03T22:06:13.642908+00:00,benchmark_result,,,,True,,,,,,15986.0,,qwen3-235b-a22b-instruct-2507,,,,0.794,,,qwen,,,,,,,,0.794,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.642908+00:00,,,,,,False,,False,0.0,False,0.0,0.794,0.794,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.794,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,mmlu-redux,MMLU-Redux,,,2025-08-03T22:06:13.644630+00:00,benchmark_result,,,,True,,,,,,15987.0,,qwen3-235b-a22b-instruct-2507,,,,0.931,,,qwen,,,,,,,,0.931,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.644630+00:00,,,,,,False,,False,0.0,False,0.0,0.931,0.931,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.931,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,Accuracy,,,multi-if,Multi-IF,,,2025-08-03T22:06:13.646355+00:00,benchmark_result,,,,True,,,,,,15988.0,,qwen3-235b-a22b-instruct-2507,,,,0.775,,,qwen,,,,,,,,0.775,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.646355+00:00,,,,,,False,,False,0.0,False,0.0,0.775,0.775,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.775,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Score,,,multipl-e,MultiPL-E,,,2025-08-03T22:06:13.648211+00:00,benchmark_result,,,,True,,,,,,15989.0,,qwen3-235b-a22b-instruct-2507,,,,0.879,,,qwen,,,,,,,,0.879,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.648211+00:00,,,,,,False,,False,0.0,False,0.0,0.879,0.879,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.879,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,polymath,PolyMATH,,,2025-08-03T22:06:13.649756+00:00,benchmark_result,,,,True,,,,,,15990.0,,qwen3-235b-a22b-instruct-2507,,,,0.502,,,qwen,,,,,,,,0.502,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.649756+00:00,,,,,,False,,False,0.0,False,0.0,0.502,0.502,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",PolyMATH,"['math', 'reasoning', 'spatial_reasoning', 'multimodal', 'vision']",multimodal,False,1.0,en,"Polymath is a challenging multi-modal mathematical reasoning benchmark designed to evaluate the general cognitive reasoning abilities of Multi-modal Large Language Models (MLLMs). The benchmark comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning.",0.502,False,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,simpleqa,SimpleQA,,,2025-08-03T22:06:13.651445+00:00,benchmark_result,,,,True,,,,,,15991.0,,qwen3-235b-a22b-instruct-2507,,,,0.543,,,qwen,,,,,,,,0.543,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.651445+00:00,,,,,,False,,False,0.0,False,0.0,0.543,0.543,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",SimpleQA,"['general', 'reasoning']",text,False,1.0,en,"SimpleQA is a factuality benchmark developed by OpenAI that measures the short-form factual accuracy of large language models. The benchmark contains 4,326 short, fact-seeking questions that are adversarially collected and designed to have single, indisputable answers. Questions cover diverse topics from science and technology to entertainment, and the benchmark also measures model calibration by evaluating whether models know what they know.",0.543,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,supergpqa,SuperGPQA,,,2025-08-03T22:06:13.652980+00:00,benchmark_result,,,,True,,,,,,15992.0,,qwen3-235b-a22b-instruct-2507,,,,0.626,,,qwen,,,,,,,,0.626,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.652980+00:00,,,,,,False,,False,0.0,False,0.0,0.626,0.626,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.626,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,tau2-airline,Tau2 airline,,,2025-08-03T22:06:13.654737+00:00,benchmark_result,,,,True,,,,,,15993.0,,qwen3-235b-a22b-instruct-2507,,,,0.44,,,qwen,,,,,,,,0.44,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.654737+00:00,,,,,,False,,False,0.0,False,0.0,0.44,0.44,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.44,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,tau2-retail,Tau2 retail,,,2025-08-03T22:06:13.656359+00:00,benchmark_result,,,,True,,,,,,15994.0,,qwen3-235b-a22b-instruct-2507,,,,0.713,,,qwen,,,,,,,,0.713,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.656359+00:00,,,,,,False,,False,0.0,False,0.0,0.713,0.713,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.713,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,writingbench,WritingBench,,,2025-08-03T22:06:13.657968+00:00,benchmark_result,,,,True,,,,,,15995.0,,qwen3-235b-a22b-instruct-2507,,,,0.852,,,qwen,,,,,,,,0.852,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.657968+00:00,,,,,,False,,False,0.0,False,0.0,0.852,0.852,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",WritingBench,"['writing', 'creativity', 'communication']",text,True,1.0,en,"A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",0.852,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,zebralogic,ZebraLogic,,,2025-08-03T22:06:13.659618+00:00,benchmark_result,,,,True,,,,,,15996.0,,qwen3-235b-a22b-instruct-2507,,,,0.95,,,qwen,,,,,,,,0.95,https://huggingface.co/Qwen/Qwen3-235B-A22B-Instruct-2507,,,,,,,,2025-08-03T22:06:13.659618+00:00,,,,,,False,,False,0.0,False,0.0,0.95,0.95,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-235B-A22B-Instruct-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-22,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Instruct-2507 is the updated instruct version of Qwen3-235B-A22B featuring significant improvements in general capabilities including instruction following, logical reasoning, text comprehension, mathematics, science, coding and tool usage. It provides substantial gains in long-tail knowledge coverage across multiple languages and markedly better alignment with user preferences in subjective and open-ended tasks.",ZebraLogic,['reasoning'],text,False,1.0,en,"ZebraLogic is an evaluation framework for assessing large language models' logical reasoning capabilities through logic grid puzzles derived from constraint satisfaction problems (CSPs). The benchmark consists of 1,000 programmatically generated puzzles with controllable and quantifiable complexity, revealing a 'curse of complexity' where model accuracy declines significantly as problem complexity grows.",0.95,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,SOTA (95%+),qwen3
,,,,aime-2025,AIME 2025,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9105.0,,qwen3-235b-a22b-thinking-2507,,,,0.923,,,qwen,,,,,,,,0.923,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.923,0.923,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.923,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,GPT-4 evaluated win rates,,,arena-hard-v2,Arena-Hard v2,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9113.0,,qwen3-235b-a22b-thinking-2507,,,,0.797,,,qwen,,,,,,,,0.797,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.797,0.797,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Arena-Hard v2,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",0.797,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,bfcl-v3,BFCL-v3,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9116.0,,qwen3-235b-a22b-thinking-2507,,,,0.719,,,qwen,,,,,,,,0.719,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.719,0.719,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.719,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,cfeval,CFEval,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9110.0,,qwen3-235b-a22b-thinking-2507,,,,0.2134,,,qwen,,,,,,,,2134.0,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Raw score: 2134,,,False,,False,0.0,False,0.0,2134.0,0.2134,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",CFEval,['code'],text,False,10000.0,en,CFEval benchmark for evaluating code generation and problem-solving capabilities,0.2134,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,creative-writing-v3,Creative Writing v3,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9114.0,,qwen3-235b-a22b-thinking-2507,,,,0.861,,,qwen,,,,,,,,0.861,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.861,0.861,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Creative Writing v3,"['creativity', 'writing']",text,False,1.0,en,"EQ-Bench Creative Writing v3 is an LLM-judged creative writing benchmark that evaluates models across 32 writing prompts with 3 iterations per prompt. Uses a hybrid scoring system combining rubric assessment and Elo ratings through pairwise comparisons. Challenges models in areas like humor, romance, spatial awareness, and unique perspectives to assess emotional intelligence and creative writing capabilities.",0.861,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,gpqa,GPQA,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9103.0,,qwen3-235b-a22b-thinking-2507,,,,0.811,,,qwen,,,,,,,,0.811,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.811,0.811,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.811,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,hmmt25,HMMT25,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9106.0,,qwen3-235b-a22b-thinking-2507,,,,0.839,,,qwen,,,,,,,,0.839,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.839,0.839,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.839,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,text-only subset,,,humanity's-last-exam,HLE,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9108.0,,qwen3-235b-a22b-thinking-2507,,,,0.182,,,qwen,,,,,,,,0.182,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Score refers to text-only subset as model is not multi-modal,,,False,,False,0.0,False,0.0,0.182,0.182,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Humanity's Last Exam,['general'],multimodal,False,1.0,en,"A multi-modal benchmark at the frontier of human knowledge with 2,500 questions across dozens of subjects including mathematics, humanities, and natural sciences, created by nearly 1000 subject expert contributors from over 500 institutions",0.182,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,ifeval,IFEval,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9112.0,,qwen3-235b-a22b-thinking-2507,,,,0.878,,,qwen,,,,,,,,0.878,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.878,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,include,INCLUDE,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9124.0,,qwen3-235b-a22b-thinking-2507,,,,0.81,,,qwen,,,,,,,,0.81,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.81,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,livebench-20241125,LiveBench 20241125,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9107.0,,qwen3-235b-a22b-thinking-2507,,,,0.784,,,qwen,,,,,,,,0.784,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.784,0.784,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",LiveBench 20241125,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.784,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,25.02-25.05,,,livecodebench-v6,LiveCodeBench v6,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9109.0,,qwen3-235b-a22b-thinking-2507,,,,0.741,,,qwen,,,,,,,,0.741,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.741,0.741,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.741,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,mmlu-pro,MMLU-Pro,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9101.0,,qwen3-235b-a22b-thinking-2507,,,,0.844,,,qwen,,,,,,,,0.844,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.844,0.844,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.844,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,mmlu-prox,MMLU-ProX,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9123.0,,qwen3-235b-a22b-thinking-2507,,,,0.81,,,qwen,,,,,,,,0.81,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MMLU-ProX,,,False,,False,0.0,False,0.0,0.81,0.81,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.81,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,mmlu-redux,MMLU-Redux,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9102.0,,qwen3-235b-a22b-thinking-2507,,,,0.938,,,qwen,,,,,,,,0.938,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.938,0.938,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.938,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,,,,multi-if,MultiIF,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9122.0,,qwen3-235b-a22b-thinking-2507,,,,0.806,,,qwen,,,,,,,,0.806,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MultiIF,,,False,,False,0.0,False,0.0,0.806,0.806,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.806,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,ojbench,OJBench,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9111.0,,qwen3-235b-a22b-thinking-2507,,,,0.325,,,qwen,,,,,,,,0.325,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.325,0.325,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",OJBench,['reasoning'],text,False,1.0,en,"OJBench is a competition-level code benchmark designed to assess the competitive-level code reasoning abilities of large language models. It comprises 232 programming competition problems from NOI and ICPC, categorized into Easy, Medium, and Hard difficulty levels. The benchmark evaluates models' ability to solve complex competitive programming challenges using Python and C++.",0.325,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,polymath,PolyMATH,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9125.0,,qwen3-235b-a22b-thinking-2507,,,,0.601,,,qwen,,,,,,,,0.601,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.601,0.601,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",PolyMATH,"['math', 'reasoning', 'spatial_reasoning', 'multimodal', 'vision']",multimodal,False,1.0,en,"Polymath is a challenging multi-modal mathematical reasoning benchmark designed to evaluate the general cognitive reasoning abilities of Multi-modal Large Language Models (MLLMs). The benchmark comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning.",0.601,False,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Fair (60-70%),qwen3
,,,,supergpqa,SuperGPQA,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9104.0,,qwen3-235b-a22b-thinking-2507,,,,0.649,,,qwen,,,,,,,,0.649,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.649,0.649,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.649,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau-bench-airline,TAU1-Airline,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9118.0,,qwen3-235b-a22b-thinking-2507,,,,0.46,,,qwen,,,,,,,,0.46,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Airline,,,False,,False,0.0,False,0.0,0.46,0.46,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.46,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau-bench-retail,TAU1-Retail,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9117.0,,qwen3-235b-a22b-thinking-2507,,,,0.678,,,qwen,,,,,,,,0.678,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Retail,,,False,,False,0.0,False,0.0,0.678,0.678,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.678,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau2-airline,TAU2-Airline,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9120.0,,qwen3-235b-a22b-thinking-2507,,,,0.58,,,qwen,,,,,,,,0.58,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.58,0.58,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.58,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau2-retail,TAU2-Retail,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9119.0,,qwen3-235b-a22b-thinking-2507,,,,0.719,,,qwen,,,,,,,,0.719,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.719,0.719,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.719,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,tau2-telecom,TAU2-Telecom,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9121.0,,qwen3-235b-a22b-thinking-2507,,,,0.456,,,qwen,,,,,,,,0.456,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.456,0.456,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.456,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,writingbench,WritingBench,,,2025-07-25T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9115.0,,qwen3-235b-a22b-thinking-2507,,,,0.883,,,qwen,,,,,,,,0.883,https://qwenlm.github.io/blog/qwen3-thinking/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.883,0.883,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-235B-A22B-Thinking-2507,qwen,Alibaba Cloud / Qwen Team,235000000000.0,235000000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-07-25,2025.0,7.0,2025-07,Very Large (>70B),"Qwen3-235B-A22B-Thinking-2507 is a state-of-the-art thinking-enabled Mixture-of-Experts (MoE) model with 235B total parameters (22B activated). It features 94 layers, 128 experts (8 activated), and supports 262K native context length. This version delivers significantly improved reasoning performance, achieving state-of-the-art results among open-source thinking models on logical reasoning, mathematics, science, coding, and academic benchmarks. Key enhancements include markedly better general capabilities (instruction following, tool usage, text generation), enhanced 256K long-context understanding, and increased thinking depth. The model supports only thinking mode with automatic <think> tag inclusion.",WritingBench,"['writing', 'creativity', 'communication']",text,True,1.0,en,"A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",0.883,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.965575+00:00,benchmark_result,,,,True,,,,,,455.0,,qwen3-30b-a3b,,,,0.804,,,qwen,,,,,,,,0.804,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.965575+00:00,,,,,,False,,False,0.0,False,0.0,0.804,0.804,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.804,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Accuracy,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.449947+00:00,benchmark_result,,,,True,,,,,,691.0,,qwen3-30b-a3b,,,,0.709,,,qwen,,,,,,,,0.709,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.449947+00:00,,,,,,False,,False,0.0,False,0.0,0.709,0.709,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.709,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.098594+00:00,benchmark_result,,,,True,,,,,,1454.0,,qwen3-30b-a3b,,,,0.91,,,qwen,,,,,,,,0.91,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.098594+00:00,,,,,,False,,False,0.0,False,0.0,0.91,0.91,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.91,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,v3,,,bfcl,BFCL,,,2025-07-19T19:56:12.782049+00:00,benchmark_result,,,,True,,,,,,852.0,,qwen3-30b-a3b,,,,0.691,,,qwen,,,,,,,,0.691,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.782049+00:00,,,,,,False,,False,0.0,False,0.0,0.691,0.691,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.691,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.682771+00:00,benchmark_result,,,,True,,,,,,304.0,,qwen3-30b-a3b,,,,0.658,,,qwen,,,,,,,,0.658,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.682771+00:00,,,,,,False,,False,0.0,False,0.0,0.658,0.658,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.658,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,livebench,LiveBench,,,2025-07-19T19:56:12.579527+00:00,benchmark_result,,,,True,,,,,,751.0,,qwen3-30b-a3b,,,,0.743,,,qwen,,,,,,,,0.743,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.579527+00:00,,,,,,False,,False,0.0,False,0.0,0.743,0.743,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.743,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,v5,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.349221+00:00,benchmark_result,,,,True,,,,,,1125.0,,qwen3-30b-a3b,,,,0.626,,,qwen,,,,,,,,0.626,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.349221+00:00,,,,,,False,,False,0.0,False,0.0,0.626,0.626,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.626,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,multi-if,Multi-IF,,,2025-07-19T19:56:14.641584+00:00,benchmark_result,,,,True,,,,,,1649.0,,qwen3-30b-a3b,,,,0.722,,,qwen,,,,,,,,0.722,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.641584+00:00,,,,,,False,,False,0.0,False,0.0,0.722,0.722,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 30B A3B,qwen,Alibaba Cloud / Qwen Team,30500000000.0,30500000000.0,True,36000000000000.0,True,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-30B-A3B is a smaller Mixture-of-Experts (MoE) model from the Qwen3 series by Alibaba, with 30.5 billion total parameters and 3.3 billion activated parameters. Features hybrid thinking/non-thinking modes, support for 119 languages, and enhanced agent capabilities. It aims to outperform previous models like QwQ-32B while using significantly fewer activated parameters.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.722,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Pass@2,,,aider,Aider,,,2025-07-19T19:56:14.571165+00:00,benchmark_result,,,,True,,,,,,1625.0,,qwen3-32b,,,,0.502,,,qwen,,,,,,,,0.502,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.571165+00:00,,,,,,False,,False,0.0,False,0.0,0.502,0.502,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",Aider,"['reasoning', 'code']",text,False,1.0,en,"Aider is a comprehensive code editing benchmark based on 133 practice exercises from Exercism's Python repository, designed to evaluate AI models' ability to translate natural language coding requests into executable code that passes unit tests. The benchmark measures end-to-end code editing capabilities, including GPT's ability to edit existing code and format code changes for automated saving to local files. The Aider Polyglot variant extends this evaluation across 225 challenging exercises spanning C++, Go, Java, JavaScript, Python, and Rust, making it a standard benchmark for assessing multilingual code editing performance in AI research.",0.502,False,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Pass@64,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.961658+00:00,benchmark_result,,,,True,,,,,,453.0,,qwen3-32b,,,,0.814,,,qwen,,,,,,,,0.814,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:11.961658+00:00,,,,,,False,,False,0.0,False,0.0,0.814,0.814,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.814,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,Pass@64,,,aime-2025,AIME 2025,,,2025-07-19T19:56:12.446075+00:00,benchmark_result,,,,True,,,,,,689.0,,qwen3-32b,,,,0.729,,,qwen,,,,,,,,0.729,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.446075+00:00,,,,,,False,,False,0.0,False,0.0,0.729,0.729,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.729,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Accuracy,,,arena-hard,Arena Hard,,,2025-07-19T19:56:14.093495+00:00,benchmark_result,,,,True,,,,,,1451.0,,qwen3-32b,,,,0.938,,,qwen,,,,,,,,0.938,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.093495+00:00,,,,,,False,,False,0.0,False,0.0,0.938,0.938,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",Arena Hard,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto is an automatic evaluation benchmark for instruction-tuned LLMs consisting of 500 challenging real-world prompts curated by BenchBuilder. It includes open-ended software engineering problems, mathematical questions, and creative writing tasks. The benchmark uses LLM-as-a-Judge methodology with GPT-4.1 and Gemini-2.5 as automatic judges to approximate human preference. Arena-Hard achieves 98.6% correlation with human preference rankings and provides 3x higher separation of model performances compared to MT-Bench, making it highly effective for distinguishing between models of similar quality.",0.938,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,v3,,,bfcl,BFCL,,,2025-07-19T19:56:12.778924+00:00,benchmark_result,,,,True,,,,,,850.0,,qwen3-32b,,,,0.703,,,qwen,,,,,,,,0.703,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.778924+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.703,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,Elo Rating,,,codeforces,CodeForces,,,2025-07-19T19:56:14.627279+00:00,benchmark_result,,,,True,,,,,,1645.0,,qwen3-32b,,,,0.659,,,qwen,,,,,,,,0.659,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.627279+00:00,,,,,,False,,False,0.0,False,0.0,0.659,0.659,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",CodeForces,"['math', 'reasoning']",text,False,3000.0,en,"A competitive programming benchmark using problems from the CodeForces platform. The benchmark evaluates code generation capabilities of LLMs on algorithmic problems with difficulty ratings ranging from 800 to 2400. Problems cover diverse algorithmic categories including dynamic programming, graph algorithms, data structures, and mathematical problems with standardized evaluation through direct platform submission.",0.00021966666666666668,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,Accuracy,,,livebench,LiveBench,,,2025-07-19T19:56:12.573432+00:00,benchmark_result,,,,True,,,,,,748.0,,qwen3-32b,,,,0.749,,,qwen,,,,,,,,0.749,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:12.573432+00:00,,,,,,False,,False,0.0,False,0.0,0.749,0.749,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.749,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,v5,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.342304+00:00,benchmark_result,,,,True,,,,,,1122.0,,qwen3-32b,,,,0.657,,,qwen,,,,,,,,0.657,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:13.342304+00:00,,,,,,False,,False,0.0,False,0.0,0.657,0.657,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.657,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,Accuracy,,,multilf,MultiLF,,,2025-07-19T19:56:14.630716+00:00,benchmark_result,,,,True,,,,,,1646.0,,qwen3-32b,,,,0.73,,,qwen,,,,,,,,0.73,https://qwenlm.github.io/blog/qwen3/,,,,,,,,2025-07-19T19:56:14.630716+00:00,,,,,,False,,False,0.0,False,0.0,0.73,0.73,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3 32B,qwen,Alibaba Cloud / Qwen Team,32800000000.0,32800000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-04-29,2025.0,4.0,2025-04,Very Large (>70B),"Qwen3-32B is a large language model from Alibaba's Qwen3 series. It features 32.8 billion parameters, a 128k token context window, support for 119 languages, and hybrid thinking modes allowing switching between deep reasoning and fast responses. It demonstrates strong performance in reasoning, instruction-following, and agent capabilities.",MultiLF,['general'],text,False,1.0,en,MultiLF benchmark,0.73,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,aider-polyglot,Aider-Polyglot,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9310.0,,qwen3-next-80b-a3b-instruct,,,,0.498,,,qwen,,,,,,,,0.498,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.498,0.498,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Aider-Polyglot,"['general', 'code']",text,False,1.0,en,"A coding benchmark that evaluates LLMs on 225 challenging Exercism programming exercises across C++, Go, Java, JavaScript, Python, and Rust. Models receive two attempts to solve each problem, with test error feedback provided after the first attempt if it fails. The benchmark measures both initial problem-solving ability and capacity to edit code based on error feedback, providing an end-to-end evaluation of code generation and editing capabilities across multiple programming languages.",0.498,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,aime-2025,AIME 2025,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9305.0,,qwen3-next-80b-a3b-instruct,,,,0.695,,,qwen,,,,,,,,0.695,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.695,0.695,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.695,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,GPT-4.1 evaluated win rates,,,arena-hard-v2,Arena-Hard v2,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9312.0,,qwen3-next-80b-a3b-instruct,,,,0.827,,,qwen,,,,,,,,0.827,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.827,0.827,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Arena-Hard v2,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",0.827,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,bfcl-v3,BFCL-v3,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9315.0,,qwen3-next-80b-a3b-instruct,,,,0.703,,,qwen,,,,,,,,0.703,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.703,0.703,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.703,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,creative-writing-v3,Creative Writing v3,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9313.0,,qwen3-next-80b-a3b-instruct,,,,0.853,,,qwen,,,,,,,,0.853,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.853,0.853,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Creative Writing v3,"['creativity', 'writing']",text,False,1.0,en,"EQ-Bench Creative Writing v3 is an LLM-judged creative writing benchmark that evaluates models across 32 writing prompts with 3 iterations per prompt. Uses a hybrid scoring system combining rubric assessment and Elo ratings through pairwise comparisons. Challenges models in areas like humor, romance, spatial awareness, and unique perspectives to assess emotional intelligence and creative writing capabilities.",0.853,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,gpqa,GPQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9303.0,,qwen3-next-80b-a3b-instruct,,,,0.729,,,qwen,,,,,,,,0.729,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.729,0.729,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.729,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,hmmt25,HMMT25,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9306.0,,qwen3-next-80b-a3b-instruct,,,,0.541,,,qwen,,,,,,,,0.541,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.541,0.541,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.541,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,ifeval,IFEval,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9311.0,,qwen3-next-80b-a3b-instruct,,,,0.876,,,qwen,,,,,,,,0.876,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.876,0.876,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.876,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,include,INCLUDE,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9323.0,,qwen3-next-80b-a3b-instruct,,,,0.789,,,qwen,,,,,,,,0.789,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.789,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,livebench-20241125,LiveBench 20241125,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9307.0,,qwen3-next-80b-a3b-instruct,,,,0.758,,,qwen,,,,,,,,0.758,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",LiveBench 20241125,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.758,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,25.02-25.05,,,livecodebench-v6,LiveCodeBench v6,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9308.0,,qwen3-next-80b-a3b-instruct,,,,0.566,,,qwen,,,,,,,,0.566,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.566,0.566,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.566,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,mmlu-pro,MMLU-Pro,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9301.0,,qwen3-next-80b-a3b-instruct,,,,0.806,,,qwen,,,,,,,,0.806,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.806,0.806,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.806,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,mmlu-prox,MMLU-ProX,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9322.0,,qwen3-next-80b-a3b-instruct,,,,0.767,,,qwen,,,,,,,,0.767,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MMLU-ProX,,,False,,False,0.0,False,0.0,0.767,0.767,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.767,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,mmlu-redux,MMLU-Redux,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9302.0,,qwen3-next-80b-a3b-instruct,,,,0.909,,,qwen,,,,,,,,0.909,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.909,0.909,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.909,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,,,,multi-if,MultiIF,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9321.0,,qwen3-next-80b-a3b-instruct,,,,0.758,,,qwen,,,,,,,,0.758,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MultiIF,,,False,,False,0.0,False,0.0,0.758,0.758,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.758,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,multipl-e,MultiPL-E,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9309.0,,qwen3-next-80b-a3b-instruct,,,,0.878,,,qwen,,,,,,,,0.878,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",MultiPL-E,"['general', 'language']",text,True,1.0,en,"MultiPL-E is a scalable and extensible system for translating unit test-driven code generation benchmarks to multiple programming languages. It extends HumanEval and MBPP Python benchmarks to 18 additional programming languages, enabling evaluation of neural code generation models across diverse programming paradigms and language features.",0.878,True,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,polymath,PolyMATH,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9324.0,,qwen3-next-80b-a3b-instruct,,,,0.459,,,qwen,,,,,,,,0.459,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.459,0.459,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",PolyMATH,"['math', 'reasoning', 'spatial_reasoning', 'multimodal', 'vision']",multimodal,False,1.0,en,"Polymath is a challenging multi-modal mathematical reasoning benchmark designed to evaluate the general cognitive reasoning abilities of Multi-modal Large Language Models (MLLMs). The benchmark comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning.",0.459,False,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen3
,,,,supergpqa,SuperGPQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9304.0,,qwen3-next-80b-a3b-instruct,,,,0.588,,,qwen,,,,,,,,0.588,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.588,0.588,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.588,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau-bench-airline,TAU1-Airline,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9317.0,,qwen3-next-80b-a3b-instruct,,,,0.44,,,qwen,,,,,,,,0.44,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Airline,,,False,,False,0.0,False,0.0,0.44,0.44,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.44,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau-bench-retail,TAU1-Retail,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9316.0,,qwen3-next-80b-a3b-instruct,,,,0.609,,,qwen,,,,,,,,0.609,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Retail,,,False,,False,0.0,False,0.0,0.609,0.609,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.609,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau2-airline,TAU2-Airline,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9319.0,,qwen3-next-80b-a3b-instruct,,,,0.455,,,qwen,,,,,,,,0.455,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.455,0.455,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.455,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau2-retail,TAU2-Retail,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9318.0,,qwen3-next-80b-a3b-instruct,,,,0.573,,,qwen,,,,,,,,0.573,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.573,0.573,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.573,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau2-telecom,TAU2-Telecom,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9320.0,,qwen3-next-80b-a3b-instruct,,,,0.132,,,qwen,,,,,,,,0.132,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.132,0.132,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.132,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,writingbench,WritingBench,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9314.0,,qwen3-next-80b-a3b-instruct,,,,0.873,,,qwen,,,,,,,,0.873,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.873,0.873,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Instruct,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Instruct is the first in the Qwen3-Next series, featuring groundbreaking architectural innovations. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared) achieving extreme low activation ratio, and Multi-Token Prediction for improved performance and faster inference. With 80B total parameters and only 3B activated, it outperforms Qwen3-32B-Base with 10% training cost and 10x throughput for 32K+ contexts. The model performs on par with Qwen3-235B-A22B-Instruct-2507 while excelling at ultra-long-context tasks up to 256K tokens (extensible to 1M with YaRN). Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)).",WritingBench,"['writing', 'creativity', 'communication']",text,True,1.0,en,"A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",0.873,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,aime-2025,AIME 2025,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9205.0,,qwen3-next-80b-a3b-thinking,,,,0.878,,,qwen,,,,,,,,0.878,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.878,0.878,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",AIME 2025,"['math', 'reasoning']",text,False,1.0,en,"All 30 problems from the 2025 American Invitational Mathematics Examination (AIME I and AIME II), testing olympiad-level mathematical reasoning with integer answers from 000-999. Used as an AI benchmark to evaluate large language models' ability to solve complex mathematical problems requiring multi-step logical deductions and structured symbolic reasoning.",0.878,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,GPT-4.1 evaluated win rates,,,arena-hard-v2,Arena-Hard v2,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9212.0,,qwen3-next-80b-a3b-thinking,,,,0.623,,,qwen,,,,,,,,0.623,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.623,0.623,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Arena-Hard v2,"['general', 'reasoning', 'creativity']",text,False,1.0,en,"Arena-Hard-Auto v2 is a challenging benchmark consisting of 500 carefully curated prompts sourced from Chatbot Arena and WildChat-1M, designed to evaluate large language models on real-world user queries. The benchmark covers diverse domains including open-ended software engineering problems, mathematics, creative writing, and technical problem-solving. It uses LLM-as-a-Judge for automatic evaluation, achieving 98.6% correlation with human preference rankings while providing 3x higher separation of model performances compared to MT-Bench. The benchmark emphasizes prompt specificity, complexity, and domain knowledge to better distinguish between model capabilities.",0.623,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,bfcl-v3,BFCL-v3,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9214.0,,qwen3-next-80b-a3b-thinking,,,,0.72,,,qwen,,,,,,,,0.72,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.72,0.72,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",BFCL-v3,"['general', 'reasoning']",text,False,1.0,en,"Berkeley Function Calling Leaderboard v3 (BFCL-v3) is an advanced benchmark that evaluates large language models' function calling capabilities through multi-turn and multi-step interactions. It introduces extended conversational exchanges where models must retain contextual information across turns and execute multiple internal function calls for complex user requests. The benchmark includes 1000 test cases across domains like vehicle control, trading bots, travel booking, and file system management, using state-based evaluation to verify both system state changes and execution path correctness.",0.72,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,cfeval,CFEval,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9209.0,,qwen3-next-80b-a3b-thinking,,,,0.2071,,,qwen,,,,,,,,2071.0,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,Raw score: 2071,,,False,,False,0.0,False,0.0,2071.0,0.2071,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",CFEval,['code'],text,False,10000.0,en,CFEval benchmark for evaluating code generation and problem-solving capabilities,0.2071,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,gpqa,GPQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9203.0,,qwen3-next-80b-a3b-thinking,,,,0.772,,,qwen,,,,,,,,0.772,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.772,0.772,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.772,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,hmmt25,HMMT25,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9206.0,,qwen3-next-80b-a3b-thinking,,,,0.739,,,qwen,,,,,,,,0.739,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.739,0.739,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",HMMT25,['math'],text,False,1.0,en,"Harvard-MIT Mathematics Tournament 2025 - A prestigious student-organized mathematics competition for high school students featuring two tournaments (November 2025 at MIT and February 2026 at Harvard) with individual tests, team rounds, and guts rounds",0.739,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,ifeval,IFEval,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9211.0,,qwen3-next-80b-a3b-thinking,,,,0.889,,,qwen,,,,,,,,0.889,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.889,0.889,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.889,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,include,INCLUDE,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9222.0,,qwen3-next-80b-a3b-thinking,,,,0.789,,,qwen,,,,,,,,0.789,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.789,0.789,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Include,['general'],text,False,1.0,en,Include benchmark - specific documentation not found in official sources,0.789,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,livebench-20241125,LiveBench 241125,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9207.0,,qwen3-next-80b-a3b-thinking,,,,0.766,,,qwen,,,,,,,,0.766,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.766,0.766,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",LiveBench 20241125,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.766,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,25.02-25.05,,,livecodebench-v6,LiveCodeBench v6,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9208.0,,qwen3-next-80b-a3b-thinking,,,,0.687,,,qwen,,,,,,,,0.687,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.687,0.687,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",LiveCodeBench v6,"['reasoning', 'general']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.687,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,mmlu-pro,MMLU-Pro,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9201.0,,qwen3-next-80b-a3b-thinking,,,,0.827,,,qwen,,,,,,,,0.827,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.827,0.827,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",MMLU-Pro,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,"A more robust and challenging multi-task language understanding benchmark that extends MMLU by expanding multiple-choice options from 4 to 10, eliminating trivial questions, and focusing on reasoning-intensive tasks. Features over 12,000 curated questions across 14 domains and causes a 16-33% accuracy drop compared to original MMLU.",0.827,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,,,,mmlu-prox,MMLU-ProX,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9221.0,,qwen3-next-80b-a3b-thinking,,,,0.787,,,qwen,,,,,,,,0.787,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MMLU-ProX,,,False,,False,0.0,False,0.0,0.787,0.787,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",MMLU-ProX,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,Extended version of MMLU-Pro providing additional challenging multiple-choice questions for evaluating language models across diverse academic and professional domains. Built on the foundation of the Massive Multitask Language Understanding benchmark framework.,0.787,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,mmlu-redux,MMLU-Redux,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9202.0,,qwen3-next-80b-a3b-thinking,,,,0.925,,,qwen,,,,,,,,0.925,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.925,0.925,Unknown,,,,,Undisclosed,Excellent (90%+),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",MMLU-Redux,"['language', 'reasoning', 'math', 'general']",text,False,1.0,en,An improved version of the MMLU benchmark featuring manually re-annotated questions to identify and correct errors in the original dataset. Provides more reliable evaluation metrics for language models by addressing dataset quality issues found in the original MMLU.,0.925,True,False,True,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwen3
,,,,multi-if,MultiIF,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9220.0,,qwen3-next-80b-a3b-thinking,,,,0.778,,,qwen,,,,,,,,0.778,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,MultiIF,,,False,,False,0.0,False,0.0,0.778,0.778,Unknown,,,,,Undisclosed,Good (70-79%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Multi-IF,"['reasoning', 'communication', 'language']",text,True,1.0,en,"Multi-IF benchmarks LLMs on multi-turn and multilingual instruction following. It expands upon IFEval by incorporating multi-turn sequences and translating English prompts into 7 other languages, resulting in 4,501 multilingual conversations with three turns each. The benchmark reveals that current leading LLMs struggle with maintaining accuracy in multi-turn instructions and shows higher error rates for non-Latin script languages.",0.778,False,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwen3
,,,,ojbench,OJBench,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9210.0,,qwen3-next-80b-a3b-thinking,,,,0.297,,,qwen,,,,,,,,0.297,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.297,0.297,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",OJBench,['reasoning'],text,False,1.0,en,"OJBench is a competition-level code benchmark designed to assess the competitive-level code reasoning abilities of large language models. It comprises 232 programming competition problems from NOI and ICPC, categorized into Easy, Medium, and Hard difficulty levels. The benchmark evaluates models' ability to solve complex competitive programming challenges using Python and C++.",0.297,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,polymath,PolyMATH,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9223.0,,qwen3-next-80b-a3b-thinking,,,,0.563,,,qwen,,,,,,,,0.563,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.563,0.563,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",PolyMATH,"['math', 'reasoning', 'spatial_reasoning', 'multimodal', 'vision']",multimodal,False,1.0,en,"Polymath is a challenging multi-modal mathematical reasoning benchmark designed to evaluate the general cognitive reasoning abilities of Multi-modal Large Language Models (MLLMs). The benchmark comprises 5,000 manually collected high-quality images of cognitive textual and visual challenges across 10 distinct categories, including pattern recognition, spatial reasoning, and relative reasoning.",0.563,False,False,True,True,False,True,False,False,False,False,False,True,False,False,False,False,False,Poor (<60%),qwen3
,,,,supergpqa,SuperGPQA,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9204.0,,qwen3-next-80b-a3b-thinking,,,,0.608,,,qwen,,,,,,,,0.608,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.608,0.608,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",SuperGPQA,"['reasoning', 'general', 'math', 'legal', 'healthcare', 'finance', 'chemistry', 'economics', 'physics']",text,False,1.0,en,"SuperGPQA is a comprehensive benchmark that evaluates large language models across 285 graduate-level academic disciplines. The benchmark contains 25,957 questions covering 13 broad disciplinary areas including Engineering, Medicine, Science, and Law, with specialized fields in light industry, agriculture, and service-oriented domains. It employs a Human-LLM collaborative filtering mechanism with over 80 expert annotators to create challenging questions that assess graduate-level knowledge and reasoning capabilities.",0.608,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau-bench-airline,TAU1-Airline,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9216.0,,qwen3-next-80b-a3b-thinking,,,,0.49,,,qwen,,,,,,,,0.49,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Airline,,,False,,False,0.0,False,0.0,0.49,0.49,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",TAU-bench Airline,"['reasoning', 'communication']",text,False,1.0,en,"Part of τ-bench (TAU-bench), a benchmark for Tool-Agent-User interaction in real-world domains. The airline domain evaluates language agents' ability to interact with users through dynamic conversations while following domain-specific rules and using API tools. Agents must handle airline-related tasks and policies reliably.",0.49,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,tau-bench-retail,TAU1-Retail,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9215.0,,qwen3-next-80b-a3b-thinking,,,,0.696,,,qwen,,,,,,,,0.696,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,TAU1-Retail,,,False,,False,0.0,False,0.0,0.696,0.696,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",TAU-bench Retail,"['reasoning', 'communication']",text,False,1.0,en,"A benchmark for evaluating tool-agent-user interaction in retail environments. Tests language agents' ability to handle dynamic conversations with users while using domain-specific API tools and following policy guidelines. Evaluates agents on tasks like order cancellations, address changes, and order status checks through multi-turn conversations.",0.696,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau2-airline,TAU2-Airline,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9218.0,,qwen3-next-80b-a3b-thinking,,,,0.605,,,qwen,,,,,,,,0.605,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.605,0.605,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Tau2 Airline,"['reasoning', 'communication']",text,False,1.0,en,"TAU2 airline domain benchmark for evaluating conversational agents in dual-control environments where both AI agents and users interact with tools in airline customer service scenarios. Tests agent coordination, communication, and ability to guide user actions in tasks like flight booking, modifications, cancellations, and refunds.",0.605,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau2-retail,TAU2-Retail,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9217.0,,qwen3-next-80b-a3b-thinking,,,,0.678,,,qwen,,,,,,,,0.678,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.678,0.678,Unknown,,,,,Undisclosed,Fair (60-69%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Tau2 Retail,"['communication', 'reasoning']",text,False,1.0,en,"τ²-bench retail domain evaluates conversational AI agents in customer service scenarios within a dual-control environment where both agent and user can interact with tools. Tests tool-agent-user interaction, rule adherence, and task consistency in retail customer support contexts.",0.678,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwen3
,,,,tau2-telecom,TAU2-Telecom,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9219.0,,qwen3-next-80b-a3b-thinking,,,,0.439,,,qwen,,,,,,,,0.439,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.439,0.439,Unknown,,,,,Undisclosed,Poor (<60%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",Tau2 Telecom,"['communication', 'reasoning']",text,False,1.0,en,"τ²-Bench telecom domain evaluates conversational agents in a dual-control environment modeled as a Dec-POMDP, where both agent and user use tools in shared telecommunications troubleshooting scenarios that test coordination and communication capabilities.",0.439,False,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwen3
,,,,writingbench,WritingBench,,,2025-01-10T00:00:00.000000+00:00,benchmark_result,,,,True,,,,,,9213.0,,qwen3-next-80b-a3b-thinking,,,,0.846,,,qwen,,,,,,,,0.846,https://qwenlm.github.io/blog/qwen3-next/,,,,,,,,2025-09-15T00:00:00.000000+00:00,,,,,,False,,False,0.0,False,0.0,0.846,0.846,Unknown,,,,,Undisclosed,Very Good (80-89%),Qwen3-Next-80B-A3B-Thinking,qwen,Alibaba Cloud / Qwen Team,80000000000.0,80000000000.0,True,15000000000000.0,True,False,apache_2_0,Open & Permissive,2025-09-10,2025.0,9.0,2025-09,Very Large (>70B),"Qwen3-Next-80B-A3B-Thinking is the thinking variant of the Qwen3-Next series, featuring the same groundbreaking architecture as the instruct model. Leveraging GSPO, it addresses stability and efficiency challenges of hybrid attention + high-sparsity MoE in RL training. It uses Hybrid Attention combining Gated DeltaNet and Gated Attention for efficient ultra-long context modeling, High-Sparsity MoE with 512 experts (10 activated + 1 shared), and Multi-Token Prediction. With 80B total parameters and only 3B activated, it demonstrates outstanding performance on complex reasoning tasks — outperforming Qwen3-30B-A3B-Thinking-2507, Qwen3-32B-Thinking, and even the proprietary Gemini-2.5-Flash-Thinking across multiple benchmarks. Architecture: 48 layers, 15T training tokens, hybrid layout of 12*(3*(Gated DeltaNet->MoE)->(Gated Attention->MoE)). Supports only thinking mode with automatic <think> tag inclusion, may generate longer thinking content.",WritingBench,"['writing', 'creativity', 'communication']",text,True,1.0,en,"A comprehensive benchmark for evaluating large language models' generative writing capabilities across 6 core writing domains (Academic & Engineering, Finance & Business, Politics & Law, Literature & Art, Education, Advertising & Marketing) and 100 subdomains. Contains 1,239 queries with a query-dependent evaluation framework that dynamically generates 5 instance-specific assessment criteria for each writing task, using a fine-tuned critic model to score responses on style, format, and length dimensions.",0.846,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwen3
,accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.957773+00:00,benchmark_result,,,,True,,,,,,451.0,,qwq-32b,,,,0.795,,,qwen,,,,,,,,0.795,https://qwenlm.github.io/blog/qwq-32b/,,,,,,,,2025-07-19T19:56:11.957773+00:00,,,,,,False,,False,0.0,False,0.0,0.795,0.795,Unknown,,,,,Undisclosed,Good (70-79%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.795,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwq
,accuracy,,,bfcl,BFCL,,,2025-07-19T19:56:12.777209+00:00,benchmark_result,,,,True,,,,,,849.0,,qwq-32b,,,,0.664,,,qwen,,,,,,,,0.664,https://qwenlm.github.io/blog/qwq-32b/,,,,,,,,2025-07-19T19:56:12.777209+00:00,,,,,,False,,False,0.0,False,0.0,0.664,0.664,Unknown,,,,,Undisclosed,Fair (60-69%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",BFCL,"['general', 'reasoning']",text,False,1.0,en,"The Berkeley Function Calling Leaderboard (BFCL) is the first comprehensive and executable function call evaluation dedicated to assessing Large Language Models' ability to invoke functions. It evaluates serial and parallel function calls across multiple programming languages (Python, Java, JavaScript, REST API) using a novel Abstract Syntax Tree (AST) evaluation method. The benchmark consists of over 2,000 question-function-answer pairs covering diverse application domains and complex use cases including multiple function calls, parallel function calls, and multi-turn interactions.",0.664,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwq
,Pass@1,,,gpqa,GPQA,,,2025-07-19T19:56:11.672880+00:00,benchmark_result,,,,True,,,,,,298.0,,qwq-32b,,,,0.652,,,qwen,,,,,,,,0.652,https://qwen-ai.com/qwq-32b/,,,,,,,,2025-07-19T19:56:11.672880+00:00,,,,,,False,,False,0.0,False,0.0,0.652,0.652,Unknown,,,,,Undisclosed,Fair (60-69%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.652,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwq
,accuracy,,,ifeval,IFEval,,,2025-07-19T19:56:12.275723+00:00,benchmark_result,,,,True,,,,,,619.0,,qwq-32b,,,,0.839,,,qwen,,,,,,,,0.839,https://qwenlm.github.io/blog/qwq-32b/,,,,,,,,2025-07-19T19:56:12.275723+00:00,,,,,,False,,False,0.0,False,0.0,0.839,0.839,Unknown,,,,,Undisclosed,Very Good (80-89%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",IFEval,['general'],text,False,1.0,en,"Instruction-Following Evaluation (IFEval) benchmark for large language models, focusing on verifiable instructions with 25 types of instructions and around 500 prompts containing one or more verifiable constraints",0.839,True,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,False,Very Good (80-90%),qwq
,accuracy,,,livebench,LiveBench,,,2025-07-19T19:56:12.570952+00:00,benchmark_result,,,,True,,,,,,747.0,,qwq-32b,,,,0.731,,,qwen,,,,,,,,0.731,https://qwenlm.github.io/blog/qwq-32b/,,,,,,,,2025-07-19T19:56:12.570952+00:00,,,,,,False,,False,0.0,False,0.0,0.731,0.731,Unknown,,,,,Undisclosed,Good (70-79%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",LiveBench,"['math', 'reasoning', 'general']",text,False,1.0,en,"LiveBench is a challenging, contamination-limited LLM benchmark that addresses test set contamination by releasing new questions monthly based on recently-released datasets, arXiv papers, news articles, and IMDb movie synopses. It comprises tasks across math, coding, reasoning, language, instruction following, and data analysis with verifiable, objective ground-truth answers.",0.731,True,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Good (70-80%),qwq
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.332752+00:00,benchmark_result,,,,True,,,,,,1118.0,,qwq-32b,,,,0.634,,,qwen,,,,,,,,0.634,https://qwenlm.github.io/blog/qwq-32b/,,,,,,,,2025-07-19T19:56:13.332752+00:00,,,,,,False,,False,0.0,False,0.0,0.634,0.634,Unknown,,,,,Undisclosed,Fair (60-69%),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.634,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwq
,accuracy,,,math-500,MATH-500,,,2025-07-19T19:56:12.034467+00:00,benchmark_result,,,,True,,,,,,495.0,,qwq-32b,,,,0.906,,,qwen,,,,,,,,0.906,https://qwen-ai.com/qwq-32b/,,,,,,,,2025-07-19T19:56:12.034467+00:00,,,,,,False,,False,0.0,False,0.0,0.906,0.906,Unknown,,,,,Undisclosed,Excellent (90%+),QwQ-32B,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2025-03-05,2025.0,3.0,2025-03,Very Large (>70B),"A model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive/endless reasoning patterns.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.906,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwq
,accuracy,,,aime-2024,AIME 2024,,,2025-07-19T19:56:11.959852+00:00,benchmark_result,,,,True,,,,,,452.0,,qwq-32b-preview,,,,0.5,,,qwen,,,,,,,,0.5,https://qwenlm.github.io/blog/qwq-32b-preview/,,,,,,,,2025-07-19T19:56:11.959852+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),QwQ-32B-Preview,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-11-28,2024.0,11.0,2024-11,Very Large (>70B),"An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns.",AIME 2024,"['math', 'reasoning']",text,False,1.0,en,"American Invitational Mathematics Examination 2024, consisting of 30 challenging mathematical reasoning problems from AIME I and AIME II competitions. Each problem requires an integer answer between 0-999 and tests advanced mathematical reasoning across algebra, geometry, combinatorics, and number theory. Used as a benchmark for evaluating mathematical reasoning capabilities in large language models at Olympiad-level difficulty.",0.5,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwq
,accuracy,,,gpqa,GPQA,,,2025-07-19T19:56:11.675997+00:00,benchmark_result,,,,True,,,,,,300.0,,qwq-32b-preview,,,,0.652,,,qwen,,,,,,,,0.652,https://qwenlm.github.io/blog/qwq-32b-preview/,,,,,,,,2025-07-19T19:56:11.675997+00:00,,,,,,False,,False,0.0,False,0.0,0.652,0.652,Unknown,,,,,Undisclosed,Fair (60-69%),QwQ-32B-Preview,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-11-28,2024.0,11.0,2024-11,Very Large (>70B),"An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns.",GPQA,"['reasoning', 'general']",text,False,1.0,en,"A challenging dataset of 448 multiple-choice questions written by domain experts in biology, physics, and chemistry. Questions are Google-proof and extremely difficult, with PhD experts reaching 65% accuracy.",0.652,True,False,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Fair (60-70%),qwq
,accuracy,,,livecodebench,LiveCodeBench,,,2025-07-19T19:56:13.337401+00:00,benchmark_result,,,,True,,,,,,1120.0,,qwq-32b-preview,,,,0.5,,,qwen,,,,,,,,0.5,https://qwenlm.github.io/blog/qwq-32b-preview/,,,,,,,,2025-07-19T19:56:13.337401+00:00,,,,,,False,,False,0.0,False,0.0,0.5,0.5,Unknown,,,,,Undisclosed,Poor (<60%),QwQ-32B-Preview,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-11-28,2024.0,11.0,2024-11,Very Large (>70B),"An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns.",LiveCodeBench,"['reasoning', 'general', 'code']",text,False,1.0,en,"LiveCodeBench is a holistic and contamination-free evaluation benchmark for large language models for code. It continuously collects new problems from programming contests (LeetCode, AtCoder, CodeForces) and evaluates four different scenarios: code generation, self-repair, code execution, and test output prediction. Problems are annotated with release dates to enable evaluation on unseen problems released after a model's training cutoff.",0.5,True,True,False,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Poor (<60%),qwq
,accuracy,,,math-500,MATH-500,,,2025-07-19T19:56:12.036449+00:00,benchmark_result,,,,True,,,,,,496.0,,qwq-32b-preview,,,,0.906,,,qwen,,,,,,,,0.906,https://qwenlm.github.io/blog/qwq-32b-preview/,,,,,,,,2025-07-19T19:56:12.036449+00:00,,,,,,False,,False,0.0,False,0.0,0.906,0.906,Unknown,,,,,Undisclosed,Excellent (90%+),QwQ-32B-Preview,qwen,Alibaba Cloud / Qwen Team,32500000000.0,32500000000.0,True,0.0,False,False,apache_2_0,Open & Permissive,2024-11-28,2024.0,11.0,2024-11,Very Large (>70B),"An experimental research model focused on advancing AI reasoning capabilities, particularly excelling in mathematics and programming. Features deep introspection and self-questioning abilities while having some limitations in language mixing and recursive reasoning patterns.",MATH-500,"['math', 'reasoning']",text,False,1.0,en,"MATH-500 is a subset of the MATH dataset containing 500 challenging competition mathematics problems from AMC 10, AMC 12, AIME, and other mathematics competitions. Each problem includes full step-by-step solutions and spans multiple difficulty levels across seven mathematical subjects including Prealgebra, Algebra, Number Theory, Counting and Probability, Geometry, Intermediate Algebra, and Precalculus.",0.906,False,False,True,True,False,False,False,False,False,False,False,False,False,False,False,False,False,Excellent (90-95%),qwq
